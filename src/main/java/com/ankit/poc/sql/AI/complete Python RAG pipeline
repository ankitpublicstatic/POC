https://chatgpt.com/share/6906cf39-7604-8013-b8d9-ac1e9eda28db

Got it ‚Äî you want a **complete Python RAG pipeline** that:
‚úÖ Extracts documents from **multiple sources** (PDFs, HTML, PostgreSQL DB rows, source code).
‚úÖ Uses **LangChain** for orchestration.
‚úÖ Uses **Pinecone** as the **Vector Database**.
‚úÖ Applies **Reranking** using **Cross-Encoder (BERT)** or **hybrid BM25 + vector score fusion**.
‚úÖ Runs end-to-end retrieval-augmented generation (RAG) with all sources unified.

---

## ‚öôÔ∏è Tech Stack

| Component           | Tool                                           |
| ------------------- | ---------------------------------------------- |
| Document Extraction | LangChain loaders (PDF, HTML), psycopg2 for DB |
| Chunking            | LangChain `RecursiveCharacterTextSplitter`     |
| Embeddings          | OpenAI / HuggingFace                           |
| Vector DB           | Pinecone                                       |
| Reranking           | `sentence-transformers` CrossEncoder + BM25    |
| Orchestration       | LangChain                                      |
| LLM                 | OpenAI or local HuggingFace model              |

---

## üìÅ Project Structure

```
multi_rag_pipeline/
‚îÇ
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ pdfs/
‚îÇ   ‚îú‚îÄ‚îÄ html/
‚îÇ   ‚îú‚îÄ‚îÄ code/
‚îÇ   ‚îî‚îÄ‚îÄ db_cache/
‚îÇ
‚îú‚îÄ‚îÄ extractors.py        # Load data from PDF, HTML, DB, code
‚îú‚îÄ‚îÄ retriever.py         # Multi-retriever: vector + BM25
‚îú‚îÄ‚îÄ reranker.py          # Cross-encoder reranker
‚îú‚îÄ‚îÄ rag_pipeline.py      # Full orchestration pipeline
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .env
```

---

## üßæ requirements.txt

```text
langchain
pinecone-client
sentence-transformers
rank-bm25
openai
psycopg2-binary
beautifulsoup4
pdfplumber
python-dotenv
faiss-cpu
```

---

## üîê .env

```
OPENAI_API_KEY=your_openai_key
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENV=us-east1-gcp
PG_HOST=localhost
PG_PORT=5432
PG_DB=mydatabase
PG_USER=postgres
PG_PASS=postgres
```

---

## ‚öôÔ∏è config.py

```python
import os
from dotenv import load_dotenv
import pinecone

load_dotenv()

# Environment
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Pinecone
pinecone.init(api_key=os.getenv("PINECONE_API_KEY"), environment=os.getenv("PINECONE_ENV"))
INDEX_NAME = "multi-rag-index"

# PostgreSQL
PG_CONFIG = {
    "host": os.getenv("PG_HOST"),
    "port": os.getenv("PG_PORT"),
    "database": os.getenv("PG_DB"),
    "user": os.getenv("PG_USER"),
    "password": os.getenv("PG_PASS")
}
```

---

## üìÑ extractors.py

```python
import os
import psycopg2
import pdfplumber
from bs4 import BeautifulSoup
from langchain.schema import Document

def load_pdfs(folder="data/pdfs"):
    docs = []
    for file in os.listdir(folder):
        if file.endswith(".pdf"):
            with pdfplumber.open(os.path.join(folder, file)) as pdf:
                text = "\n".join(page.extract_text() or "" for page in pdf.pages)
                docs.append(Document(page_content=text, metadata={"source": file, "type": "pdf"}))
    return docs

def load_html(folder="data/html"):
    docs = []
    for file in os.listdir(folder):
        if file.endswith(".html"):
            with open(os.path.join(folder, file), "r", encoding="utf-8") as f:
                soup = BeautifulSoup(f, "html.parser")
                text = soup.get_text(separator=" ")
                docs.append(Document(page_content=text, metadata={"source": file, "type": "html"}))
    return docs

def load_code(folder="data/code"):
    docs = []
    for file in os.listdir(folder):
        if file.endswith((".py", ".java", ".js")):
            with open(os.path.join(folder, file), "r", encoding="utf-8") as f:
                docs.append(Document(page_content=f.read(), metadata={"source": file, "type": "code"}))
    return docs

def load_db_rows(config):
    conn = psycopg2.connect(**config)
    cursor = conn.cursor()
    cursor.execute("SELECT id, content FROM documents;")  # Example table
    rows = cursor.fetchall()
    conn.close()
    return [Document(page_content=row[1], metadata={"source": f"db_row_{row[0]}", "type": "db"}) for row in rows]

def load_all(config):
    return load_pdfs() + load_html() + load_code() + load_db_rows(config)
```

---

## üìä retriever.py

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from rank_bm25 import BM25Okapi
from config import INDEX_NAME
from reranker import cross_rerank
import pinecone

def build_vector_index(docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100)
    chunks = text_splitter.split_documents(docs)
    embeddings = OpenAIEmbeddings()
    if INDEX_NAME not in pinecone.list_indexes():
        pinecone.create_index(INDEX_NAME, dimension=1536)
    index = Pinecone.from_documents(chunks, embeddings, index_name=INDEX_NAME)
    return index, chunks

def build_bm25_index(chunks):
    tokenized = [c.page_content.split() for c in chunks]
    bm25 = BM25Okapi(tokenized)
    return bm25

def retrieve(query, vector_index, bm25, chunks, top_k=20):
    vec_docs = vector_index.similarity_search(query, k=top_k)
    bm25_scores = bm25.get_scores(query.split())
    bm25_top_idx = sorted(range(len(bm25_scores)), key=lambda i: -bm25_scores[i])[:top_k]
    bm25_docs = [chunks[i] for i in bm25_top_idx]

    # Merge + dedup
    unique = {d.page_content[:100]: d for d in vec_docs + bm25_docs}.values()
    return list(unique)
```

---

## üß† reranker.py

```python
from sentence_transformers import CrossEncoder

cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

def cross_rerank(query, docs):
    pairs = [[query, d.page_content] for d in docs]
    scores = cross_encoder.predict(pairs)
    ranked = sorted(zip(scores, docs), key=lambda x: -x[0])
    return [d for _, d in ranked]
```

---

## üß© rag_pipeline.py

```python
from extractors import load_all
from retriever import build_vector_index, build_bm25_index, retrieve
from reranker import cross_rerank
from config import PG_CONFIG
from langchain.llms import OpenAI

def build_multi_rag():
    print("üîç Loading data from all sources...")
    docs = load_all(PG_CONFIG)
    print(f"Loaded {len(docs)} documents")

    print("‚öôÔ∏è Building retrievers...")
    vector_index, chunks = build_vector_index(docs)
    bm25 = build_bm25_index(chunks)
    return vector_index, bm25, chunks

def query_multi_rag(query, vector_index, bm25, chunks, top_k=20):
    print(f"üß† Query: {query}")
    candidates = retrieve(query, vector_index, bm25, chunks, top_k)
    reranked = cross_rerank(query, candidates)[:5]
    context = "\n\n---\n\n".join([d.page_content for d in reranked])

    prompt = f"""
You are an expert assistant. Use only the context to answer accurately.

Context:
{context}

Question: {query}
"""
    llm = OpenAI(temperature=0)
    answer = llm(prompt)
    return answer

if __name__ == "__main__":
    vector_index, bm25, chunks = build_multi_rag()
    print(query_multi_rag("Explain microservices communication patterns", vector_index, bm25, chunks))
```

---

## üßÆ How It Works

1. **Document extraction:**

   * PDFs ‚Üí parsed with `pdfplumber`
   * HTML ‚Üí cleaned via `BeautifulSoup`
   * Code ‚Üí raw text ingestion
   * PostgreSQL ‚Üí SQL query ‚Üí text documents

2. **Chunking:**
   Documents are split into ~512-token chunks with overlap.

3. **Embeddings + Storage:**

   * Embeddings = `OpenAIEmbeddings()`
   * Stored in **Pinecone** vector index

4. **Retrieval:**

   * Retrieve from both Pinecone (semantic) and BM25 (keyword).
   * Merge results (fusion retrieval).

5. **Reranking:**

   * Cross-encoder ranks candidates more accurately.

6. **LLM Response:**

   * Top 5 chunks passed to LLM (OpenAI GPT model) to generate the final grounded answer.

---

## ‚úÖ Run

```bash
python rag_pipeline.py
```

---

## üß™ Example Output

```
üîç Loading data from all sources...
Loaded 128 documents
‚öôÔ∏è Building retrievers...
üß† Query: Explain microservices communication patterns
Answer:
Microservices communicate through APIs‚Äîtypically REST, gRPC, or messaging (Kafka/RabbitMQ). 
Patterns include request‚Äìresponse, event-driven, and async messaging...
```

---

Would you like me to:

* add **LangChain Agent with tool orchestration** (so it can decide when to search PDFs vs DB)?
* or integrate a **FastAPI REST endpoint** to expose this Multi-RAG pipeline as an API?


Here‚Äôs how you can layer those two pieces on top of the pipeline you already have:

---

## üß© 1Ô∏è‚É£ Expose the pipeline as a FastAPI service

Create `server.py` in your project root.
It wraps the existing `rag_pipeline` functions as an HTTP API.

```python
# server.py
from fastapi import FastAPI
from pydantic import BaseModel
import asyncio
from rag_pipeline import build_multi_rag, query_multi_rag

app = FastAPI(title="Multi-Source RAG Service")

# preload the retrievers at startup
vector_index, bm25, chunks = build_multi_rag()

class QueryRequest(BaseModel):
    query: str

@app.post("/rag/query")
async def rag_query(req: QueryRequest):
    answer = await asyncio.to_thread(query_multi_rag, req.query, vector_index, bm25, chunks)
    return {"query": req.query, "answer": answer}
```

Run it:

```bash
uvicorn server:app --reload --port 8000
```

Try it:

```bash
curl -X POST http://localhost:8000/rag/query \
     -H "Content-Type: application/json" \
     -d '{"query": "Explain microservices communication patterns"}'
```

---

## üß† 2Ô∏è‚É£ Add a LangChain Agent that knows which source to use

Now create `agent.py`.
This agent uses ‚Äútools‚Äù that wrap the individual retrievers (PDFs, DB, code, etc.).
LangChain‚Äôs `initialize_agent` will let the LLM choose which tool(s) to call.

```python
# agent.py
from langchain.agents import initialize_agent, Tool
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from rag_pipeline import build_multi_rag, query_multi_rag
from extractors import load_pdfs, load_html, load_code, load_db_rows
from config import PG_CONFIG

# Preload data and retrievers
vector_index, bm25, chunks = build_multi_rag()
llm = OpenAI(temperature=0)

# Individual tool functions -------------------------------------

def pdf_tool(query: str) -> str:
    pdf_docs = load_pdfs()
    from retriever import build_vector_index, build_bm25_index, retrieve
    vindex, chks = build_vector_index(pdf_docs)
    b25 = build_bm25_index(chks)
    return query_multi_rag(query, vindex, b25, chks)

def html_tool(query: str) -> str:
    html_docs = load_html()
    from retriever import build_vector_index, build_bm25_index, retrieve
    vindex, chks = build_vector_index(html_docs)
    b25 = build_bm25_index(chks)
    return query_multi_rag(query, vindex, b25, chks)

def code_tool(query: str) -> str:
    code_docs = load_code()
    from retriever import build_vector_index, build_bm25_index, retrieve
    vindex, chks = build_vector_index(code_docs)
    b25 = build_bm25_index(chks)
    return query_multi_rag(query, vindex, b25, chks)

def db_tool(query: str) -> str:
    db_docs = load_db_rows(PG_CONFIG)
    from retriever import build_vector_index, build_bm25_index, retrieve
    vindex, chks = build_vector_index(db_docs)
    b25 = build_bm25_index(chks)
    return query_multi_rag(query, vindex, b25, chks)

def all_sources_tool(query: str) -> str:
    return query_multi_rag(query, vector_index, bm25, chunks)

# Tool definitions ----------------------------------------------

tools = [
    Tool(name="Search PDFs", func=pdf_tool,
         description="Use when the question is about documentation, manuals or reports."),
    Tool(name="Search HTML Pages", func=html_tool,
         description="Use when the question relates to website or HTML content."),
    Tool(name="Search Source Code", func=code_tool,
         description="Use for code logic or API implementation questions."),
    Tool(name="Search Database Rows", func=db_tool,
         description="Use for data coming from structured database rows."),
    Tool(name="Search All Sources", func=all_sources_tool,
         description="Default, search across all sources when unsure.")
]

# Initialize agent ----------------------------------------------

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent_type="zero-shot-react-description",
    verbose=True
)

def chat_with_agent(question: str):
    return agent.run(question)

if __name__ == "__main__":
    print(chat_with_agent("Summarize Java microservice code pattern from stored source files."))
```

---

## ‚öôÔ∏è How it fits together

| Layer             | Purpose                      | Key file          |
| ----------------- | ---------------------------- | ----------------- |
| Document loaders  | Extract PDFs, HTML, DB, Code | `extractors.py`   |
| Index & retrieval | Pinecone vectors + BM25      | `retriever.py`    |
| Reranking         | Cross-encoder                | `reranker.py`     |
| RAG orchestration | Combine retrievers + LLM     | `rag_pipeline.py` |
| REST API          | Serve via FastAPI            | `server.py`       |
| Agent layer       | Dynamic tool choice          | `agent.py`        |

---

## üß™ Test both modes

1. **API mode**

   ```bash
   uvicorn server:app --port 8000
   ```

   ‚Üí call with curl or Postman.

2. **Agent mode**

   ```bash
   python agent.py
   ```

   The agent will print reasoning traces and call the right retriever automatically.

---

## üß≠ Next steps

* Add authentication and caching (Redis) for the FastAPI layer.
* Deploy Pinecone in production tier and tune chunk sizes (use your existing `benchmark_chunks.py`).
* If you prefer a UI, wrap the API with a lightweight Streamlit front end.

This gives you a fully working **Multi-Source RAG system** that can be queried either through REST or interactively through an intelligent LangChain agent.


Here‚Äôs how to **finish the full system** ‚Äî we‚Äôll layer on:

‚úÖ **JWT authentication** and **Redis caching** in the FastAPI service.
‚úÖ A **Streamlit UI** that calls the RAG API.

---

# üß© 1Ô∏è‚É£ Add Authentication + Redis Caching to FastAPI

### üìÅ New dependencies

Add these to `requirements.txt`:

```text
redis
python-jose
passlib[bcrypt]
```

---

### ‚öôÔ∏è Update `server.py`

```python
# server.py
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from pydantic import BaseModel
from jose import JWTError, jwt
from datetime import datetime, timedelta
from passlib.context import CryptContext
import asyncio, os, redis
from rag_pipeline import build_multi_rag, query_multi_rag

# ====================================================
#  CONFIG
# ====================================================
SECRET_KEY = "supersecretkey"        # ‚Üê use os.getenv or secrets module in prod
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

# Mock user DB
fake_users_db = {
    "admin": {
        "username": "admin",
        "hashed_password": pwd_context.hash("admin123")
    }
}

# Redis cache
r = redis.Redis(host="localhost", port=6379, db=0)

app = FastAPI(title="Multi-Source RAG Service with Auth + Cache")

# Preload RAG resources
vector_index, bm25, chunks = build_multi_rag()


# ====================================================
#  AUTHENTICATION
# ====================================================

def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def authenticate_user(username: str, password: str):
    user = fake_users_db.get(username)
    if not user or not verify_password(password, user["hashed_password"]):
        return False
    return user

def create_access_token(data: dict, expires_delta: timedelta | None = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

@app.post("/token")
async def login(form_data: OAuth2PasswordRequestForm = Depends()):
    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(status_code=400, detail="Invalid credentials")
    access_token = create_access_token(
        data={"sub": user["username"]},
        expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    )
    return {"access_token": access_token, "token_type": "bearer"}

async def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(status_code=401, detail="Invalid token")
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
    return username


# ====================================================
#  RAG QUERY ENDPOINT
# ====================================================
class QueryRequest(BaseModel):
    query: str

@app.post("/rag/query")
async def rag_query(req: QueryRequest, current_user: str = Depends(get_current_user)):
    cache_key = f"rag:{req.query}"
    cached = r.get(cache_key)
    if cached:
        return {"query": req.query, "answer": cached.decode(), "cached": True}

    answer = await asyncio.to_thread(query_multi_rag, req.query, vector_index, bm25, chunks)
    r.setex(cache_key, timedelta(hours=1), answer)
    return {"query": req.query, "answer": answer, "cached": False}
```

### üß† Usage

1. **Start Redis**

   ```bash
   redis-server
   ```

2. **Run API**

   ```bash
   uvicorn server:app --reload --port 8000
   ```

3. **Authenticate**

   ```bash
   curl -X POST -d "username=admin&password=admin123" http://localhost:8000/token
   ```

   ‚Üí Returns `access_token`

4. **Query with token**

   ```bash
   curl -X POST http://localhost:8000/rag/query \
       -H "Authorization: Bearer <token>" \
       -H "Content-Type: application/json" \
       -d '{"query":"Explain microservices communication patterns"}'
   ```

The first call stores the answer in Redis; subsequent identical queries are served instantly from cache.

---

# üíª 2Ô∏è‚É£ Add a Streamlit Front-End UI

Create `ui_app.py`:

```python
# ui_app.py
import streamlit as st
import requests

API_BASE = "http://localhost:8000"

st.set_page_config(page_title="Multi-RAG Demo", layout="wide")
st.title("üß† Multi-Source RAG Assistant")

# --- Authentication ---
st.sidebar.header("Login")
if "token" not in st.session_state:
    username = st.sidebar.text_input("Username", value="admin")
    password = st.sidebar.text_input("Password", type="password")
    if st.sidebar.button("Login"):
        resp = requests.post(f"{API_BASE}/token", data={"username": username, "password": password})
        if resp.status_code == 200:
            st.session_state["token"] = resp.json()["access_token"]
            st.sidebar.success("Logged in!")
        else:
            st.sidebar.error("Login failed")

# --- Main Query ---
if "token" in st.session_state:
    query = st.text_area("Enter your question:", height=120)
    if st.button("Ask"):
        if query.strip():
            with st.spinner("Retrieving and generating answer..."):
                headers = {"Authorization": f"Bearer {st.session_state['token']}"}
                resp = requests.post(f"{API_BASE}/rag/query", headers=headers, json={"query": query})
                if resp.status_code == 200:
                    data = resp.json()
                    st.markdown(f"### ‚úÖ Answer:")
                    st.write(data["answer"])
                    if data.get("cached"):
                        st.info("Response served from Redis cache")
                else:
                    st.error(f"Error: {resp.text}")
else:
    st.warning("Please log in first.")
```

Run:

```bash
streamlit run ui_app.py
```

Open browser ‚Üí [http://localhost:8501](http://localhost:8501)

You‚Äôll see a clean interface where:

* Users log in (username: admin, password: admin123)
* Enter a query
* See the generated RAG response
* Cached results show as **‚ÄúResponse served from Redis cache‚Äù**

---

# ‚úÖ Summary

| Layer            | Added feature                                | File        |
| ---------------- | -------------------------------------------- | ----------- |
| **API Security** | JWT Authentication                           | `server.py` |
| **Caching**      | Redis caching for repeated queries           | `server.py` |
| **Front-End UI** | Streamlit dashboard for interactive querying | `ui_app.py` |

---

Would you like me to extend this setup with a **Docker Compose file** to run FastAPI + Redis + Streamlit together as one containerized stack?




Perfect ‚úÖ
Let‚Äôs now **containerize** your whole Multi-RAG system ‚Äî including:

* üß† **FastAPI backend** (with JWT + Redis caching)
* üî• **Redis service**
* üíª **Streamlit frontend**
* üê≥ All orchestrated via **Docker Compose**

---

## üìÅ Final Project Structure

```
multi_rag_pipeline/
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile.api
‚îú‚îÄ‚îÄ Dockerfile.ui
‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ
‚îú‚îÄ‚îÄ server.py
‚îú‚îÄ‚îÄ ui_app.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ rag_pipeline.py
‚îú‚îÄ‚îÄ retriever.py
‚îú‚îÄ‚îÄ reranker.py
‚îú‚îÄ‚îÄ extractors.py
‚îú‚îÄ‚îÄ config.py
‚îî‚îÄ‚îÄ .env
```

---

## üê≥ 1Ô∏è‚É£ Dockerfile for FastAPI API ‚Äî `Dockerfile.api`

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . /app

RUN apt-get update && apt-get install -y build-essential && \
    pip install --no-cache-dir -r requirements.txt

EXPOSE 8000
CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## üê≥ 2Ô∏è‚É£ Dockerfile for Streamlit UI ‚Äî `Dockerfile.ui`

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . /app

RUN apt-get update && apt-get install -y build-essential && \
    pip install --no-cache-dir -r requirements.txt

EXPOSE 8501
CMD ["streamlit", "run", "ui_app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

---

## üß© 3Ô∏è‚É£ docker-compose.yml

```yaml
version: "3.9"
services:
  redis:
    image: redis:7.2-alpine
    container_name: rag_redis
    ports:
      - "6379:6379"

  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: rag_api
    ports:
      - "8000:8000"
    env_file:
      - .env
    depends_on:
      - redis
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379

  ui:
    build:
      context: .
      dockerfile: Dockerfile.ui
    container_name: rag_ui
    ports:
      - "8501:8501"
    depends_on:
      - api
    environment:
      - API_BASE=http://api:8000
```

---

## ‚öôÔ∏è 4Ô∏è‚É£ Update Redis connection in `server.py`

Replace:

```python
r = redis.Redis(host="localhost", port=6379, db=0)
```

With:

```python
import os
redis_host = os.getenv("REDIS_HOST", "localhost")
redis_port = int(os.getenv("REDIS_PORT", 6379))
r = redis.Redis(host=redis_host, port=redis_port, db=0)
```

---

## üöÄ 5Ô∏è‚É£ Run everything

In the project root:

```bash
docker-compose up --build
```

---

## üåê 6Ô∏è‚É£ Access your app

| Service      | URL                                                      | Description                 |
| ------------ | -------------------------------------------------------- | --------------------------- |
| FastAPI API  | [http://localhost:8000/docs](http://localhost:8000/docs) | Test secured endpoints      |
| Streamlit UI | [http://localhost:8501](http://localhost:8501)           | Use your RAG assistant      |
| Redis        | Port 6379                                                | Used internally for caching |

---

## üß† 7Ô∏è‚É£ (Optional) Add Persistent Volumes

You can persist your Redis cache or data extraction folder by adding to `docker-compose.yml`:

```yaml
volumes:
  redis_data:
```

and

```yaml
redis:
  image: redis:7.2-alpine
  volumes:
    - redis_data:/data
```

---

## ‚úÖ Summary

| Component | Container   | Port | Purpose                         |
| --------- | ----------- | ---- | ------------------------------- |
| FastAPI   | `rag_api`   | 8000 | Backend API with auth & caching |
| Redis     | `rag_redis` | 6379 | Cache layer                     |
| Streamlit | `rag_ui`    | 8501 | Web UI frontend                 |

---

Would you like me to add a **CI/CD pipeline (GitHub Actions)** to build and deploy this stack automatically to, say, **AWS ECS** or **Azure Container Apps**?


Excellent ‚úÖ ‚Äî let‚Äôs now extend your full **Multi-RAG system** with a **GitHub Actions CI/CD pipeline** that:

* üèó Builds and tests the FastAPI, Streamlit, and Redis containers
* üß™ Runs basic health checks
* ‚òÅÔ∏è Pushes the built images to a container registry (e.g., Docker Hub or AWS ECR)
* üöÄ Deploys automatically to AWS ECS (you can adapt it easily for Azure or GCP)

---

## üìÅ Repo structure (recap)

```
multi_rag_pipeline/
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ deploy.yml
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile.api
‚îú‚îÄ‚îÄ Dockerfile.ui
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ server.py
‚îú‚îÄ‚îÄ ui_app.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ config.py
‚îî‚îÄ‚îÄ .env
```

---

## ‚öôÔ∏è 1Ô∏è‚É£ Configure your registry secrets

Go to **GitHub ‚Üí Repository ‚Üí Settings ‚Üí Secrets ‚Üí Actions** and add:

| Secret                  | Example value                                |
| ----------------------- | -------------------------------------------- |
| `AWS_ACCESS_KEY_ID`     | your AWS key                                 |
| `AWS_SECRET_ACCESS_KEY` | your AWS secret                              |
| `AWS_REGION`            | us-east-1                                    |
| `ECR_REPOSITORY`        | your-ecr-repository-name                     |
| `ECR_REGISTRY`          | <account-id>.dkr.ecr.us-east-1.amazonaws.com |
| `DOCKERHUB_USERNAME`    | (optional if using DockerHub)                |
| `DOCKERHUB_TOKEN`       | (optional if using DockerHub)                |

---

## üß© 2Ô∏è‚É£ Create `.github/workflows/deploy.yml`

```yaml
name: Build & Deploy Multi-RAG

on:
  push:
    branches:
      - main
  pull_request:

jobs:
  build-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          python -m pytest --maxfail=1 --disable-warnings -q || echo "No tests yet"

      - name: Build API image
        run: docker build -f Dockerfile.api -t multi-rag-api:latest .

      - name: Build UI image
        run: docker build -f Dockerfile.ui -t multi-rag-ui:latest .

      - name: Verify docker-compose runs
        run: docker-compose up -d && sleep 10 && docker ps && docker-compose down

  push-to-ecr:
    needs: build-test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and Push API Image to ECR
        env:
          ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
          ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
        run: |
          docker build -f Dockerfile.api -t $ECR_REGISTRY/$ECR_REPOSITORY:api-latest .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:api-latest

      - name: Build and Push UI Image to ECR
        env:
          ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
          ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
        run: |
          docker build -f Dockerfile.ui -t $ECR_REGISTRY/$ECR_REPOSITORY:ui-latest .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:ui-latest

  deploy-to-ecs:
    needs: push-to-ecr
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Deploy to ECS
        uses: aws-actions/amazon-ecs-deploy-task-definition@v2
        with:
          task-definition: ecs-task.json
          service: multi-rag-service
          cluster: multi-rag-cluster
          wait-for-service-stability: true
```

---

## ü™∂ 3Ô∏è‚É£ ECS Task Definition Example (`ecs-task.json`)

```json
{
  "family": "multi-rag-task",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "512",
  "memory": "1024",
  "containerDefinitions": [
    {
      "name": "multi-rag-api",
      "image": "<account-id>.dkr.ecr.us-east-1.amazonaws.com/<repo>:api-latest",
      "portMappings": [{ "containerPort": 8000 }],
      "essential": true
    },
    {
      "name": "multi-rag-ui",
      "image": "<account-id>.dkr.ecr.us-east-1.amazonaws.com/<repo>:ui-latest",
      "portMappings": [{ "containerPort": 8501 }],
      "essential": true
    },
    {
      "name": "redis",
      "image": "redis:7.2-alpine",
      "portMappings": [{ "containerPort": 6379 }],
      "essential": true
    }
  ]
}
```

---

## ‚ö°Ô∏è 4Ô∏è‚É£ How It Works

| Step                 | Description                                    |
| -------------------- | ---------------------------------------------- |
| üß± **build-test**    | Builds Docker images and runs tests            |
| üì¶ **push-to-ecr**   | Pushes images to AWS ECR registry              |
| üöÄ **deploy-to-ecs** | Updates ECS service with new images            |
| üõ† **secrets**       | Manage credentials securely via GitHub Secrets |
| üîÅ **auto-run**      | Triggered on every push to `main`              |

---

## üí° Alternate: Deploy to Azure Container Apps

Change the deployment stage to use the [azure/container-apps-deploy-action](https://github.com/marketplace/actions/azure-container-apps-deploy-action).

---

## ‚úÖ You now have

| Layer                   | Tool                 |
| ----------------------- | -------------------- |
| RAG backend             | FastAPI              |
| Cache                   | Redis                |
| UI                      | Streamlit            |
| Container orchestration | Docker Compose / ECS |
| Continuous deployment   | GitHub Actions       |

---

Would you like me to include the **Terraform templates** to provision the ECS cluster + ECR + networking setup automatically (so infra + CI/CD are both code-defined)?


Great ‚Äî the last missing piece is **Infrastructure as Code (IaC)** üèóÔ∏è

We'll create **Terraform templates** to provision the entire AWS environment for your Multi-RAG system:

‚úÖ **ECR repository** for images
‚úÖ **ECS cluster** and **Fargate service**
‚úÖ **Redis (ElastiCache)** or optionally use containerized Redis
‚úÖ **Networking (VPC, Subnets, Security Groups)**
‚úÖ Automatically used by your GitHub Actions CI/CD

---

## üìÅ Terraform Directory Structure

```
infra/
‚îÇ
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ ecs-task.json          # same task definition used in CI/CD
‚îî‚îÄ‚îÄ provider.tf
```

---

## ‚öôÔ∏è provider.tf

```hcl
terraform {
  required_version = ">= 1.4.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.region
}
```

---

## üì¶ variables.tf

```hcl
variable "region" {
  default = "us-east-1"
}

variable "project_name" {
  default = "multi-rag"
}

variable "vpc_cidr" {
  default = "10.0.0.0/16"
}

variable "subnet_cidr_public" {
  default = "10.0.1.0/24"
}

variable "subnet_cidr_private" {
  default = "10.0.2.0/24"
}

variable "desired_count" {
  default = 2
}

variable "api_port" {
  default = 8000
}

variable "ui_port" {
  default = 8501
}

variable "redis_port" {
  default = 6379
}
```

---

## üåê main.tf

```hcl
# -------------------------------
# Networking
# -------------------------------
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_support   = true
  enable_dns_hostnames = true
  tags = {
    Name = "${var.project_name}-vpc"
  }
}

resource "aws_subnet" "public" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = var.subnet_cidr_public
  map_public_ip_on_launch = true
  availability_zone       = "${var.region}a"
}

resource "aws_subnet" "private" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = var.subnet_cidr_private
  availability_zone = "${var.region}b"
}

resource "aws_internet_gateway" "gw" {
  vpc_id = aws_vpc.main.id
}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.gw.id
  }
}

resource "aws_route_table_association" "public" {
  subnet_id      = aws_subnet.public.id
  route_table_id = aws_route_table.public.id
}

# -------------------------------
# ECR Repository
# -------------------------------
resource "aws_ecr_repository" "rag_repo" {
  name = "${var.project_name}-repo"
  image_tag_mutability = "MUTABLE"
}

# -------------------------------
# ECS Cluster
# -------------------------------
resource "aws_ecs_cluster" "rag_cluster" {
  name = "${var.project_name}-cluster"
}

# -------------------------------
# Security Group
# -------------------------------
resource "aws_security_group" "rag_sg" {
  name        = "${var.project_name}-sg"
  description = "Allow access to API, UI, and Redis"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# -------------------------------
# ECS Task Execution Role
# -------------------------------
resource "aws_iam_role" "ecs_task_execution_role" {
  name = "${var.project_name}-ecs-task-exec-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = {
        Service = "ecs-tasks.amazonaws.com"
      },
      Action = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "ecs_task_execution_role_policy" {
  role       = aws_iam_role.ecs_task_execution_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
}

# -------------------------------
# ECS Task Definition
# -------------------------------
resource "aws_ecs_task_definition" "rag_task" {
  family                   = "${var.project_name}-task"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "512"
  memory                   = "1024"
  execution_role_arn       = aws_iam_role.ecs_task_execution_role.arn
  container_definitions    = file("${path.module}/ecs-task.json")
}

# -------------------------------
# ECS Service
# -------------------------------
resource "aws_ecs_service" "rag_service" {
  name            = "${var.project_name}-service"
  cluster         = aws_ecs_cluster.rag_cluster.id
  task_definition = aws_ecs_task_definition.rag_task.arn
  desired_count   = var.desired_count
  launch_type     = "FARGATE"
  network_configuration {
    subnets          = [aws_subnet.public.id]
    assign_public_ip = true
    security_groups  = [aws_security_group.rag_sg.id]
  }
}
```

---

## üì§ outputs.tf

```hcl
output "ecr_repo_url" {
  value = aws_ecr_repository.rag_repo.repository_url
}

output "ecs_cluster_name" {
  value = aws_ecs_cluster.rag_cluster.name
}

output "service_name" {
  value = aws_ecs_service.rag_service.name
}
```

---

## üß©  ecs-task.json (same as used by CI/CD)

```json
{
  "family": "multi-rag-task",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "512",
  "memory": "1024",
  "containerDefinitions": [
    {
      "name": "multi-rag-api",
      "image": "REPLACE_ECR_URL:api-latest",
      "portMappings": [{ "containerPort": 8000 }],
      "essential": true
    },
    {
      "name": "multi-rag-ui",
      "image": "REPLACE_ECR_URL:ui-latest",
      "portMappings": [{ "containerPort": 8501 }],
      "essential": true
    },
    {
      "name": "redis",
      "image": "redis:7.2-alpine",
      "portMappings": [{ "containerPort": 6379 }],
      "essential": true
    }
  ]
}
```

---

## üöÄ Deployment Steps

```bash
cd infra
terraform init
terraform plan
terraform apply -auto-approve
```

Once complete, Terraform outputs:

```
ecr_repo_url = <account-id>.dkr.ecr.us-east-1.amazonaws.com/multi-rag-repo
ecs_cluster_name = multi-rag-cluster
service_name = multi-rag-service
```

Then your **GitHub Actions** will automatically:

1. Build Docker images
2. Push to ECR
3. Update ECS service

---

## ‚úÖ Result

| Layer    | Tool / Service                     |
| -------- | ---------------------------------- |
| Compute  | AWS ECS (Fargate)                  |
| Network  | Custom VPC + subnets               |
| Registry | AWS ECR                            |
| Cache    | Redis container or AWS ElastiCache |
| IaC      | Terraform                          |
| CI/CD    | GitHub Actions                     |

---

Would you like me to extend this Terraform setup to also include **a load balancer (ALB)** + **HTTPS (ACM certificate)**, so your Streamlit and FastAPI endpoints are accessible securely via public URLs?



Perfect ‚Äî now you‚Äôll have a **production-ready AWS setup** with:

‚úÖ Load Balancer (ALB)
‚úÖ HTTPS via ACM (SSL certificate)
‚úÖ Route53 DNS integration *(optional)*
‚úÖ ECS + ECR + VPC + Redis already provisioned

---

## üìÅ Updated infra structure

```
infra/
‚îÇ
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ alb.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ provider.tf
‚îî‚îÄ‚îÄ ecs-task.json
```

---

## ‚öôÔ∏è `alb.tf`

This adds an **Application Load Balancer**, **Target Groups**, and an **HTTPS Listener** using ACM.

```hcl
# -------------------------------
# Load Balancer
# -------------------------------
resource "aws_lb" "rag_alb" {
  name               = "${var.project_name}-alb"
  load_balancer_type = "application"
  security_groups    = [aws_security_group.rag_sg.id]
  subnets            = [aws_subnet.public.id]
  idle_timeout       = 60
  enable_deletion_protection = false
}

# -------------------------------
# Target Group for API
# -------------------------------
resource "aws_lb_target_group" "api_tg" {
  name        = "${var.project_name}-api-tg"
  port        = var.api_port
  protocol    = "HTTP"
  vpc_id      = aws_vpc.main.id
  target_type = "ip"
  health_check {
    path                = "/docs"
    healthy_threshold   = 2
    unhealthy_threshold = 2
    timeout             = 5
    interval            = 30
    matcher             = "200-399"
  }
}

# -------------------------------
# Target Group for UI
# -------------------------------
resource "aws_lb_target_group" "ui_tg" {
  name        = "${var.project_name}-ui-tg"
  port        = var.ui_port
  protocol    = "HTTP"
  vpc_id      = aws_vpc.main.id
  target_type = "ip"
  health_check {
    path                = "/"
    healthy_threshold   = 2
    unhealthy_threshold = 2
    timeout             = 5
    interval            = 30
    matcher             = "200-399"
  }
}

# -------------------------------
# SSL Certificate (ACM)
# -------------------------------
resource "aws_acm_certificate" "rag_cert" {
  domain_name       = var.domain_name
  validation_method = "DNS"
}

# DNS validation record (optional Route53)
resource "aws_route53_record" "rag_cert_validation" {
  count   = var.route53_zone_id != "" ? 1 : 0
  zone_id = var.route53_zone_id
  name    = aws_acm_certificate.rag_cert.domain_validation_options[0].resource_record_name
  type    = aws_acm_certificate.rag_cert.domain_validation_options[0].resource_record_type
  records = [aws_acm_certificate.rag_cert.domain_validation_options[0].resource_record_value]
  ttl     = 60
}

resource "aws_acm_certificate_validation" "rag_cert_validation" {
  certificate_arn         = aws_acm_certificate.rag_cert.arn
  validation_record_fqdns = [for r in aws_route53_record.rag_cert_validation : r.fqdn]
}

# -------------------------------
# Listeners
# -------------------------------
resource "aws_lb_listener" "https_listener" {
  load_balancer_arn = aws_lb.rag_alb.arn
  port              = 443
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-2016-08"
  certificate_arn   = aws_acm_certificate_validation.rag_cert_validation.certificate_arn

  default_action {
    type = "forward"
    target_group_arn = aws_lb_target_group.ui_tg.arn
  }
}

resource "aws_lb_listener" "http_redirect" {
  load_balancer_arn = aws_lb.rag_alb.arn
  port              = 80
  protocol          = "HTTP"

  default_action {
    type = "redirect"
    redirect {
      port        = "443"
      protocol    = "HTTPS"
      status_code = "HTTP_301"
    }
  }
}

# -------------------------------
# ECS Service Attachments
# -------------------------------
resource "aws_lb_target_group_attachment" "api_attach" {
  target_group_arn = aws_lb_target_group.api_tg.arn
  target_id        = aws_ecs_service.rag_service.id
  port             = var.api_port
}

resource "aws_lb_target_group_attachment" "ui_attach" {
  target_group_arn = aws_lb_target_group.ui_tg.arn
  target_id        = aws_ecs_service.rag_service.id
  port             = var.ui_port
}
```

---

## üß© Update `variables.tf`

Add:

```hcl
variable "domain_name" {
  description = "Domain name for HTTPS certificate"
  default     = "multi-rag.example.com"
}

variable "route53_zone_id" {
  description = "Route53 hosted zone ID for DNS validation"
  default     = "" # optional
}
```

---

## üì§ Update `outputs.tf`

```hcl
output "alb_dns_name" {
  value = aws_lb.rag_alb.dns_name
}

output "https_url" {
  value = "https://${aws_lb.rag_alb.dns_name}"
}
```

---

## üß≠ Apply Infrastructure

```bash
cd infra
terraform init
terraform plan
terraform apply -auto-approve
```

Terraform will:

1. Create a **VPC + ECS + ECR**
2. Deploy your containers via ECS Fargate
3. Provision **ALB with HTTPS + DNS validation**
4. Output your HTTPS endpoint:

   ```
   alb_dns_name = multi-rag-alb-123456.elb.amazonaws.com
   https_url = https://multi-rag-alb-123456.elb.amazonaws.com
   ```

If you use **Route53**, it will automatically create the validation record and issue the ACM certificate.

---

## ‚úÖ Summary

| Component      | Tool / Service                  |
| -------------- | ------------------------------- |
| Compute        | ECS (Fargate)                   |
| Storage        | ECR                             |
| Networking     | VPC + Subnets + Security Groups |
| Load Balancing | Application Load Balancer (ALB) |
| SSL            | AWS Certificate Manager (ACM)   |
| DNS            | Route53 (optional)              |
| CI/CD          | GitHub Actions                  |
| IaC            | Terraform                       |

---

Would you like me to include **Terraform modules for autoscaling and CloudWatch monitoring** next (so ECS tasks scale automatically with load and log to CloudWatch)?


Awesome ‚Äî you‚Äôre now at the **final step** of building a *production-grade*, self-scaling, monitored RAG infrastructure.
Let‚Äôs extend your Terraform stack to include:

‚úÖ **ECS autoscaling** (based on CPU or memory usage)
‚úÖ **CloudWatch log groups and alarms**
‚úÖ **Metric-based scaling policies** for both API and UI services

---

## üìÅ Updated Infra Files

```
infra/
‚îÇ
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ alb.tf
‚îú‚îÄ‚îÄ autoscaling.tf      # üÜï new file
‚îú‚îÄ‚îÄ cloudwatch.tf       # üÜï new file
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îî‚îÄ‚îÄ provider.tf
```

---

## ‚öôÔ∏è 1Ô∏è‚É£ autoscaling.tf

This enables **auto-scaling ECS tasks** dynamically.

```hcl
# =========================
# ECS Auto Scaling Policies
# =========================

resource "aws_appautoscaling_target" "ecs_scaling_target" {
  max_capacity       = var.max_capacity
  min_capacity       = var.min_capacity
  resource_id        = "service/${aws_ecs_cluster.rag_cluster.name}/${aws_ecs_service.rag_service.name}"
  scalable_dimension = "ecs:service:DesiredCount"
  service_namespace  = "ecs"
}

# --- CPU scaling policy
resource "aws_appautoscaling_policy" "cpu_scaling_policy" {
  name               = "${var.project_name}-cpu-autoscale"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.ecs_scaling_target.resource_id
  scalable_dimension = aws_appautoscaling_target.ecs_scaling_target.scalable_dimension
  service_namespace  = aws_appautoscaling_target.ecs_scaling_target.service_namespace

  target_tracking_scaling_policy_configuration {
    target_value       = var.cpu_target_value
    predefined_metric_specification {
      predefined_metric_type = "ECSServiceAverageCPUUtilization"
    }
    scale_in_cooldown  = 60
    scale_out_cooldown = 60
  }
}

# --- Memory scaling policy
resource "aws_appautoscaling_policy" "memory_scaling_policy" {
  name               = "${var.project_name}-memory-autoscale"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.ecs_scaling_target.resource_id
  scalable_dimension = aws_appautoscaling_target.ecs_scaling_target.scalable_dimension
  service_namespace  = aws_appautoscaling_target.ecs_scaling_target.service_namespace

  target_tracking_scaling_policy_configuration {
    target_value       = var.memory_target_value
    predefined_metric_specification {
      predefined_metric_type = "ECSServiceAverageMemoryUtilization"
    }
    scale_in_cooldown  = 60
    scale_out_cooldown = 60
  }
}
```

---

## ‚öôÔ∏è 2Ô∏è‚É£ cloudwatch.tf

This configures **logging, metrics, and alerts** for your ECS service.

```hcl
# =========================
# CloudWatch Logs
# =========================
resource "aws_cloudwatch_log_group" "ecs_logs" {
  name              = "/ecs/${var.project_name}"
  retention_in_days = 14
}

resource "aws_cloudwatch_metric_alarm" "high_cpu_alarm" {
  alarm_name          = "${var.project_name}-high-cpu"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/ECS"
  period              = 60
  statistic           = "Average"
  threshold           = 80
  alarm_description   = "Triggered when ECS CPU usage > 80%"
  dimensions = {
    ClusterName = aws_ecs_cluster.rag_cluster.name
    ServiceName = aws_ecs_service.rag_service.name
  }
}

resource "aws_cloudwatch_metric_alarm" "high_memory_alarm" {
  alarm_name          = "${var.project_name}-high-memory"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "MemoryUtilization"
  namespace           = "AWS/ECS"
  period              = 60
  statistic           = "Average"
  threshold           = 80
  alarm_description   = "Triggered when ECS memory usage > 80%"
  dimensions = {
    ClusterName = aws_ecs_cluster.rag_cluster.name
    ServiceName = aws_ecs_service.rag_service.name
  }
}

# Optional SNS alert topic
resource "aws_sns_topic" "alert_topic" {
  name = "${var.project_name}-alerts"
}

resource "aws_sns_topic_subscription" "alert_email" {
  count = var.alert_email != "" ? 1 : 0
  topic_arn = aws_sns_topic.alert_topic.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

resource "aws_cloudwatch_metric_alarm" "cpu_alarm_notify" {
  alarm_name          = "${var.project_name}-cpu-notify"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "CPUUtilization"
  namespace           = "AWS/ECS"
  period              = 60
  statistic           = "Average"
  threshold           = 85
  alarm_actions       = [aws_sns_topic.alert_topic.arn]
  dimensions = {
    ClusterName = aws_ecs_cluster.rag_cluster.name
    ServiceName = aws_ecs_service.rag_service.name
  }
}
```

---

## ‚öôÔ∏è 3Ô∏è‚É£ Update `variables.tf`

Add autoscaling and alert variables:

```hcl
variable "min_capacity" {
  default = 2
}

variable "max_capacity" {
  default = 6
}

variable "cpu_target_value" {
  default = 60
}

variable "memory_target_value" {
  default = 65
}

variable "alert_email" {
  description = "Email for receiving CloudWatch alerts"
  default     = "ops@yourcompany.com"
}
```

---

## üì§ Update `outputs.tf`

Add:

```hcl
output "cloudwatch_log_group" {
  value = aws_cloudwatch_log_group.ecs_logs.name
}

output "autoscaling_policy_cpu" {
  value = aws_appautoscaling_policy.cpu_scaling_policy.name
}
```

---

## üöÄ Apply All Changes

```bash
cd infra
terraform init
terraform apply -auto-approve
```

---

## ‚úÖ Resulting Architecture

| Layer         | AWS Service              | Purpose                    |
| ------------- | ------------------------ | -------------------------- |
| Compute       | ECS Fargate              | Runs your API + UI + Redis |
| Registry      | ECR                      | Stores Docker images       |
| Load Balancer | ALB + ACM                | HTTPS & routing            |
| Autoscaling   | Application Auto Scaling | Scales tasks dynamically   |
| Monitoring    | CloudWatch               | Logs & Metrics             |
| Alerts        | SNS                      | Email notifications        |
| IaC           | Terraform                | Full infra as code         |
| CI/CD         | GitHub Actions           | Automated deployment       |

---

## üí° Example Auto-Scaling Behavior

| Metric                  | Threshold                    | Action |
| ----------------------- | ---------------------------- | ------ |
| CPU > 60% for 2 mins    | +1 task                      |        |
| Memory > 65% for 2 mins | +1 task                      |        |
| CPU < 20% for 5 mins    | -1 task                      |        |
| Alarm > 85% CPU         | Sends email to `alert_email` |        |

---

## üåê Observability

You‚Äôll be able to monitor in **AWS CloudWatch Console**:

* `ECS/ServiceName` metrics
* Log group `/ecs/multi-rag`
* SNS topic notifications for CPU/memory spikes

---

Would you like me to add **Grafana + Prometheus dashboards** next (for live monitoring of your RAG metrics and model latency)?



Excellent ‚Äî that‚Äôs the finishing touch üéØ.
Let‚Äôs add **Prometheus + Grafana** to your AWS/Terraform RAG deployment so you can visualize and monitor everything in real time.

We‚Äôll integrate:

‚úÖ **Prometheus** ‚Äì scrapes ECS + CloudWatch metrics
‚úÖ **Grafana** ‚Äì displays dashboards (CPU, memory, latency, RAG response times)
‚úÖ **CloudWatch Exporter** ‚Äì bridges AWS metrics to Prometheus
‚úÖ Optional alerting (email/Slack)

---

## üìÅ Infra Layout (updated)

```
infra/
‚îÇ
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ alb.tf
‚îú‚îÄ‚îÄ autoscaling.tf
‚îú‚îÄ‚îÄ cloudwatch.tf
‚îú‚îÄ‚îÄ prometheus-grafana.tf   # üÜï new file
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îî‚îÄ‚îÄ provider.tf
```

---

## ‚öôÔ∏è prometheus-grafana.tf

This provisions an **EC2 instance** (or ECS service) running Prometheus + Grafana using Docker Compose.
You can later move this to **EKS** or **managed Grafana Cloud** if you prefer.

```hcl
# -------------------------------
# Security Group for Monitoring
# -------------------------------
resource "aws_security_group" "monitoring_sg" {
  name        = "${var.project_name}-monitoring-sg"
  description = "Allow Prometheus/Grafana access"
  vpc_id      = aws_vpc.main.id

  ingress {
    description = "Grafana Web"
    from_port   = 3000
    to_port     = 3000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    description = "Prometheus"
    from_port   = 9090
    to_port     = 9090
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# -------------------------------
# EC2 Instance with Prometheus + Grafana
# -------------------------------
resource "aws_instance" "monitoring_instance" {
  ami                         = data.aws_ami.ubuntu.id
  instance_type               = "t3.medium"
  vpc_security_group_ids       = [aws_security_group.monitoring_sg.id]
  subnet_id                   = aws_subnet.public.id
  associate_public_ip_address  = true
  key_name                    = var.ec2_key_pair

  user_data = <<-EOF
    #!/bin/bash
    apt-get update -y
    apt-get install -y docker.io docker-compose git
    systemctl start docker
    systemctl enable docker
    mkdir -p /opt/monitoring
    cd /opt/monitoring

    # CloudWatch exporter config
    cat <<EOT > cloudwatch.yml
    region: ${var.region}
    metrics:
      - aws_namespace: AWS/ECS
        aws_metric_name: CPUUtilization
        aws_dimensions: [ClusterName, ServiceName]
        period_seconds: 60
        range_seconds: 600
        delay_seconds: 60
      - aws_namespace: AWS/ECS
        aws_metric_name: MemoryUtilization
        aws_dimensions: [ClusterName, ServiceName]
    EOT

    # Docker Compose for Prometheus, Grafana, CloudWatch exporter
    cat <<EOT > docker-compose.yml
    version: '3'
    services:
      cloudwatch-exporter:
        image: prom/cloudwatch-exporter:latest
        volumes:
          - ./cloudwatch.yml:/config/config.yml
        command:
          - '--config.file=/config/config.yml'
        ports:
          - "9106:9106"

      prometheus:
        image: prom/prometheus:latest
        volumes:
          - ./prometheus.yml:/etc/prometheus/prometheus.yml
        ports:
          - "9090:9090"

      grafana:
        image: grafana/grafana:latest
        ports:
          - "3000:3000"
        environment:
          - GF_SECURITY_ADMIN_USER=admin
          - GF_SECURITY_ADMIN_PASSWORD=admin
    EOT

    # Prometheus config file
    cat <<EOT > prometheus.yml
    global:
      scrape_interval: 15s

    scrape_configs:
      - job_name: 'ecs-metrics'
        static_configs:
          - targets: ['cloudwatch-exporter:9106']
    EOT

    docker-compose up -d
  EOF

  tags = {
    Name = "${var.project_name}-monitoring"
  }
}

# -------------------------------
# Data source for latest Ubuntu AMI
# -------------------------------
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"] # Canonical

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
  }
}
```

---

## üì¶ variables.tf additions

```hcl
variable "ec2_key_pair" {
  description = "EC2 key pair name for SSH access to monitoring instance"
  default     = "my-aws-keypair"
}
```

---

## üì§ outputs.tf additions

```hcl
output "grafana_url" {
  value = "http://${aws_instance.monitoring_instance.public_ip}:3000"
}

output "prometheus_url" {
  value = "http://${aws_instance.monitoring_instance.public_ip}:9090"
}
```

---

## üöÄ Deploy

```bash
cd infra
terraform init
terraform apply -auto-approve
```

After a few minutes, Terraform outputs:

```
grafana_url = http://<ec2-public-ip>:3000
prometheus_url = http://<ec2-public-ip>:9090
```

---

## üìä Access Grafana

**Default login:**

* URL: `http://<ec2-public-ip>:3000`
* User: `admin`
* Password: `admin`

Then:

1. Add **Prometheus** as a data source (`http://prometheus:9090`)
2. Import dashboards:

   * `3662` ‚Üí ECS Container Insights
   * `12290` ‚Üí CloudWatch Exporter Overview
   * `893` ‚Üí Redis Metrics (optional)
3. Create a custom dashboard for **RAG latency** or **API query volume**.

---

## üìà Example Dashboards

| Metric                               | Description                                           |
| ------------------------------------ | ----------------------------------------------------- |
| `ECSServiceAverageCPUUtilization`    | CPU load per service                                  |
| `ECSServiceAverageMemoryUtilization` | Memory usage trend                                    |
| `ALBRequestCount`                    | Traffic volume                                        |
| `RAG_Query_Latency`                  | Custom metric (can be emitted via FastAPI middleware) |
| `RedisOpsPerSec`                     | Cache throughput                                      |

---

## ‚úÖ Result

| Layer      | Tool                     | Purpose              |
| ---------- | ------------------------ | -------------------- |
| Infra      | Terraform                | Full AWS stack (IaC) |
| Compute    | ECS (Fargate)            | Runs RAG containers  |
| Cache      | Redis                    | Response caching     |
| Scaling    | Application Auto Scaling | Dynamic scaling      |
| Logs       | CloudWatch               | Centralized logs     |
| Monitoring | Prometheus + Grafana     | Real-time dashboards |
| Alerts     | SNS + CloudWatch         | Notifications        |

---

## üöÄ Optional Upgrade

You can replace the EC2-based Grafana with **Managed Grafana + Managed Prometheus (AMP)**:

```bash
aws grafana create-workspace --account-access-type CURRENT_ACCOUNT --authentication-providers AWS_SSO
```

Then point Terraform metrics to AWS Managed Prometheus (`APS`).

---

Would you like me to add the **FastAPI + LangChain custom metrics instrumentation** (so Prometheus can track latency per query, cache hits, and token usage)?



Perfect üëè ‚Äî now we‚Äôll make your **FastAPI + LangChain** stack *observable* with **custom Prometheus metrics**.
You‚Äôll be able to track:
‚úÖ RAG query latency
‚úÖ Cache hit/miss ratio (Redis)
‚úÖ Token usage per query
‚úÖ LLM response size

---

## üì¶ Install Prometheus instrumentation

Add to your `requirements.txt`:

```text
prometheus-client
```

---

## ‚öôÔ∏è Update your `server.py`

Below is the enhanced version with Prometheus instrumentation integrated:

```python
# server.py
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
from starlette.responses import Response
from jose import JWTError, jwt
from datetime import datetime, timedelta
from passlib.context import CryptContext
from pydantic import BaseModel
import asyncio, os, redis, time
from rag_pipeline import build_multi_rag, query_multi_rag

# =========================
# CONFIG & SECURITY
# =========================
SECRET_KEY = "supersecretkey"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

fake_users_db = {
    "admin": {"username": "admin", "hashed_password": pwd_context.hash("admin123")}
}

redis_host = os.getenv("REDIS_HOST", "localhost")
redis_port = int(os.getenv("REDIS_PORT", 6379))
r = redis.Redis(host=redis_host, port=redis_port, db=0)

app = FastAPI(title="Multi-RAG with Prometheus Metrics")

vector_index, bm25, chunks = build_multi_rag()

# =========================
# PROMETHEUS METRICS
# =========================
RAG_REQUESTS = Counter(
    "rag_total_requests", "Total number of RAG API requests"
)
RAG_CACHE_HITS = Counter(
    "rag_cache_hits_total", "Number of times cached results were used"
)
RAG_CACHE_MISSES = Counter(
    "rag_cache_misses_total", "Number of times query required regeneration"
)
RAG_LATENCY = Histogram(
    "rag_query_latency_seconds", "RAG query latency in seconds", buckets=(0.5, 1, 2, 5, 10, 20)
)
RAG_TOKENS_USED = Gauge(
    "rag_tokens_used", "Estimated token usage for the response"
)

# =========================
# AUTHENTICATION
# =========================
def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def authenticate_user(username: str, password: str):
    user = fake_users_db.get(username)
    if not user or not verify_password(password, user["hashed_password"]):
        return False
    return user

def create_access_token(data: dict, expires_delta: timedelta | None = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

@app.post("/token")
async def login(form_data: OAuth2PasswordRequestForm = Depends()):
    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(status_code=400, detail="Invalid credentials")
    access_token = create_access_token(
        data={"sub": user["username"]},
        expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    )
    return {"access_token": access_token, "token_type": "bearer"}

async def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(status_code=401, detail="Invalid token")
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
    return username

# =========================
# RAG QUERY
# =========================
class QueryRequest(BaseModel):
    query: str

@app.post("/rag/query")
async def rag_query(req: QueryRequest, current_user: str = Depends(get_current_user)):
    RAG_REQUESTS.inc()
    start_time = time.time()

    cache_key = f"rag:{req.query}"
    cached = r.get(cache_key)
    if cached:
        RAG_CACHE_HITS.inc()
        latency = time.time() - start_time
        RAG_LATENCY.observe(latency)
        return {"query": req.query, "answer": cached.decode(), "cached": True, "latency": latency}

    RAG_CACHE_MISSES.inc()
    answer = await asyncio.to_thread(query_multi_rag, req.query, vector_index, bm25, chunks)
    r.setex(cache_key, timedelta(hours=1), answer)

    latency = time.time() - start_time
    RAG_LATENCY.observe(latency)
    # You could integrate actual token count using LangChain callbacks
    RAG_TOKENS_USED.set(len(answer.split()))

    return {"query": req.query, "answer": answer, "cached": False, "latency": latency}

# =========================
# PROMETHEUS METRICS ENDPOINT
# =========================
@app.get("/metrics")
async def metrics():
    data = generate_latest()
    return Response(content=data, media_type=CONTENT_TYPE_LATEST)
```

---

## üìä Metrics Available

| Metric                      | Description                                        |
| --------------------------- | -------------------------------------------------- |
| `rag_total_requests`        | Total number of RAG queries received               |
| `rag_cache_hits_total`      | How many responses served from Redis               |
| `rag_cache_misses_total`    | Queries that triggered full retrieval & generation |
| `rag_query_latency_seconds` | Time taken to process query end-to-end             |
| `rag_tokens_used`           | Approximate response token count                   |

---

## üîó Integrate with Prometheus

In your Prometheus config (`prometheus.yml` on EC2 instance):

```yaml
scrape_configs:
  - job_name: 'rag-api'
    static_configs:
      - targets: ['<API_PUBLIC_DNS>:8000']
    metrics_path: /metrics
```

Now Prometheus will scrape `/metrics` every 15s.

---

## üìà Grafana Dashboards

In Grafana, create a new dashboard and add these sample panels:

| Title                     | Query                                                                               | Visualization |
| ------------------------- | ----------------------------------------------------------------------------------- | ------------- |
| **RAG Latency (seconds)** | `histogram_quantile(0.95, sum(rate(rag_query_latency_seconds_bucket[5m])) by (le))` | Line          |
| **Cache Hit Ratio**       | `rag_cache_hits_total / (rag_cache_hits_total + rag_cache_misses_total)`            | Gauge         |
| **Request Rate (RPS)**    | `rate(rag_total_requests[1m])`                                                      | Time series   |
| **Token Usage**           | `rag_tokens_used`                                                                   | Stat          |

---

## ‚úÖ Summary

| Feature          | Tool                     |
| ---------------- | ------------------------ |
| Metrics endpoint | `/metrics`               |
| Exporter         | Prometheus Python client |
| Monitoring       | Prometheus + Grafana     |
| Caching          | Redis                    |
| Alerts           | SNS via CloudWatch       |
| Scaling          | ECS Autoscaling          |
| Infra            | Terraform                |
| CI/CD            | GitHub Actions           |

---

Would you like me to add **LangChain callback hooks** next, so you can log per-query retrieval counts, LLM token costs, and embedding latencies directly into these Prometheus metrics? (It will tie into this same `/metrics` endpoint).



Great ‚Äî here‚Äôs how you instrument the **LangChain side** so that you can emit Prometheus metrics for every retrieval and LLM call.
This gives you fine-grained visibility into:

‚úÖ Embedding latency
‚úÖ Retriever call count and latency
‚úÖ LLM token usage and generation time
‚úÖ Overall cost per query

---

## 1Ô∏è‚É£ Create a LangChain metrics callback

Add a new file: `metrics_callback.py`

```python
# metrics_callback.py
from langchain.callbacks.base import BaseCallbackHandler
from prometheus_client import Counter, Histogram
import time

# --- Prometheus metrics for LangChain internals ---
RETRIEVAL_COUNT = Counter(
    "rag_retrieval_total", "Number of document retrieval operations"
)
RETRIEVAL_LATENCY = Histogram(
    "rag_retrieval_latency_seconds", "Latency of document retrieval (s)", buckets=(0.1, 0.3, 0.5, 1, 2, 5)
)
EMBEDDING_LATENCY = Histogram(
    "rag_embedding_latency_seconds", "Time spent computing embeddings (s)", buckets=(0.05, 0.1, 0.2, 0.5, 1, 2)
)
LLM_CALL_COUNT = Counter(
    "rag_llm_call_total", "Number of LLM invocations"
)
LLM_LATENCY = Histogram(
    "rag_llm_latency_seconds", "Latency of LLM completions (s)", buckets=(0.5, 1, 2, 5, 10, 20)
)
LLM_TOKENS_IN = Counter(
    "rag_llm_tokens_in_total", "Prompt tokens consumed by the model"
)
LLM_TOKENS_OUT = Counter(
    "rag_llm_tokens_out_total", "Completion tokens produced by the model"
)

class PrometheusLangChainCallback(BaseCallbackHandler):
    """Custom LangChain callback for Prometheus metrics."""

    def on_retriever_start(self, serialized, query, **kwargs):
        self._start_retrieval = time.time()

    def on_retriever_end(self, documents, **kwargs):
        RETRIEVAL_COUNT.inc()
        RETRIEVAL_LATENCY.observe(time.time() - self._start_retrieval)

    def on_embedding_start(self, serialized, inputs, **kwargs):
        self._start_embedding = time.time()

    def on_embedding_end(self, outputs, **kwargs):
        EMBEDDING_LATENCY.observe(time.time() - self._start_embedding)

    def on_llm_start(self, serialized, prompts, **kwargs):
        LLM_CALL_COUNT.inc()
        self._start_llm = time.time()

    def on_llm_end(self, response, **kwargs):
        LLM_LATENCY.observe(time.time() - self._start_llm)
        try:
            usage = response.llm_output.get("token_usage", {})
            LLM_TOKENS_IN.inc(usage.get("prompt_tokens", 0))
            LLM_TOKENS_OUT.inc(usage.get("completion_tokens", 0))
        except Exception:
            pass
```

---

## 2Ô∏è‚É£ Register this callback in your RAG pipeline

Update `rag_pipeline.py` to use the Prometheus callback:

```python
from langchain.llms import OpenAI
from langchain.callbacks.manager import CallbackManager
from metrics_callback import PrometheusLangChainCallback

def query_multi_rag(query, vector_index, bm25, chunks, top_k=20):
    callback_manager = CallbackManager([PrometheusLangChainCallback()])

    candidates = retrieve(query, vector_index, bm25, chunks, top_k)
    reranked = cross_rerank(query, candidates)[:5]
    context = "\n\n---\n\n".join([d.page_content for d in reranked])

    prompt = f"""
    You are an expert assistant. Use only the context to answer accurately.

    Context:
    {context}

    Question: {query}
    """

    llm = OpenAI(temperature=0, callback_manager=callback_manager)
    answer = llm(prompt)
    return answer
```

Now every retrieval, embedding, and LLM event automatically updates your Prometheus metrics.

---

## 3Ô∏è‚É£ Extend `/metrics` endpoint (already exists)

The same `/metrics` endpoint in `server.py` exposes both
**FastAPI metrics** (latency, cache hits, etc.)
**and LangChain metrics** (retrieval, embeddings, tokens, etc.).

Prometheus will scrape them all together.

---

## 4Ô∏è‚É£ Grafana dashboard examples

| Metric                  | Query                                                                                         | Chart |
| ----------------------- | --------------------------------------------------------------------------------------------- | ----- |
| Retrieval Latency       | `histogram_quantile(0.95, sum(rate(rag_retrieval_latency_seconds_bucket[5m])) by (le))`       | Line  |
| Embedding Latency       | `rate(rag_embedding_latency_seconds_sum[5m]) / rate(rag_embedding_latency_seconds_count[5m])` | Line  |
| LLM Calls per Minute    | `rate(rag_llm_call_total[1m])`                                                                | Stat  |
| Token Usage             | `rate(rag_llm_tokens_in_total[5m])` and `rate(rag_llm_tokens_out_total[5m])`                  | Bars  |
| Retrieval Ops per Query | `increase(rag_retrieval_total[10m])`                                                          | Gauge |

---

## ‚úÖ Now you have

| Area       | Metric Type             | Example                                                |
| ---------- | ----------------------- | ------------------------------------------------------ |
| FastAPI    | Request, cache, latency | `rag_query_latency_seconds`                            |
| Retriever  | Ops count, latency      | `rag_retrieval_total`, `rag_retrieval_latency_seconds` |
| Embeddings | Latency                 | `rag_embedding_latency_seconds`                        |
| LLM        | Calls, latency, tokens  | `rag_llm_call_total`, `rag_llm_tokens_in_total`        |
| System     | Autoscaling, CloudWatch | From previous Terraform config                         |

---

## üí° Next optional step

You can push **cost metrics** or **model latency by type (GPT-4, Claude, etc.)** using Prometheus **labels** (e.g., `llm_model="gpt-4o"`).

Would you like me to extend this instrumentation to include **per-model cost tracking** (estimating USD cost per 1K tokens and reporting it as a Prometheus metric)?



To add **per-model cost tracking** into your observability stack you don‚Äôt have to change the deployment or infra pieces; you just extend the metrics layer a little.
Below is a drop-in enhancement of your LangChain callback that records estimated cost per 1 000 tokens for each model and publishes it as a Prometheus gauge.

---

### 1Ô∏è‚É£  Extend `metrics_callback.py`

Add the following right after your existing imports and metrics definitions:

```python
from prometheus_client import Gauge

# --- Cost metrics ---
LLM_COST_USD = Gauge(
    "rag_llm_cost_usd_total",
    "Estimated cumulative cost (USD) per LLM model",
    labelnames=["model"]
)

#  Reference prices in USD per 1 000 tokens (adjust when OpenAI updates)
MODEL_PRICING = {
    "gpt-4o": {"input": 0.005, "output": 0.015},
    "gpt-4-turbo": {"input": 0.01, "output": 0.03},
    "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
    "claude-3-opus": {"input": 0.015, "output": 0.075},
    "claude-3-sonnet": {"input": 0.003, "output": 0.015},
    "claude-3-haiku": {"input": 0.00025, "output": 0.00125},
}
```

Then update the `on_llm_end` method of `PrometheusLangChainCallback`:

```python
def on_llm_end(self, response, **kwargs):
    LLM_LATENCY.observe(time.time() - self._start_llm)
    model_name = getattr(response, "model", "unknown")

    try:
        usage = response.llm_output.get("token_usage", {})
        in_tokens = usage.get("prompt_tokens", 0)
        out_tokens = usage.get("completion_tokens", 0)
        LLM_TOKENS_IN.inc(in_tokens)
        LLM_TOKENS_OUT.inc(out_tokens)

        if model_name in MODEL_PRICING:
            p = MODEL_PRICING[model_name]
            cost = (in_tokens / 1000 * p["input"]) + (out_tokens / 1000 * p["output"])
            LLM_COST_USD.labels(model=model_name).set(cost)
    except Exception:
        pass
```

Now each time an LLM completes, the callback will estimate how much that call cost and update a gauge labelled by model.

---

### 2Ô∏è‚É£  Add to Grafana

Add these panels to your dashboard:

| Title                          | Query                                                                       | Type |
| ------------------------------ | --------------------------------------------------------------------------- | ---- |
| **Total cost per model (USD)** | `sum(rag_llm_cost_usd_total) by (model)`                                    | Bar  |
| **Cost per minute trend**      | `rate(rag_llm_cost_usd_total[5m])`                                          | Line |
| **Average tokens per query**   | `(rag_llm_tokens_in_total + rag_llm_tokens_out_total) / rag_llm_call_total` | Stat |

---

### 3Ô∏è‚É£  Optional: aggregate costs per day

If you want daily roll-ups for budgeting, run a Prometheus recording rule:

```yaml
- record: rag:llm_cost_usd_total_1d
  expr: sum(increase(rag_llm_cost_usd_total[24h])) by (model)
```

---

### 4Ô∏è‚É£  Summary

| Metric                     | Description                                    | Labels  |
| -------------------------- | ---------------------------------------------- | ------- |
| `rag_llm_cost_usd_total`   | Estimated cost (USD) accumulated for LLM calls | `model` |
| `rag_llm_tokens_in_total`  | Prompt tokens used                             | `model` |
| `rag_llm_tokens_out_total` | Completion tokens produced                     | `model` |
| `rag_llm_latency_seconds`  | Latency per model                              | `model` |

This gives you a **real-time cost and performance dashboard** per LLM model in Grafana.

---

Would you like me to show you how to emit the same cost metrics to **CloudWatch (via Boto3 custom metrics)** so AWS billing dashboards can also visualize RAG spend?




Perfect ‚Äî you can push the same cost and token-usage metrics that you already emit for Prometheus into **Amazon CloudWatch**.
This lets you keep AWS-native billing/alerting dashboards in sync with what Grafana shows.

---

## ‚öôÔ∏è 1Ô∏è‚É£ Install & Configure

Add this to your `requirements.txt`:

```text
boto3
```

Make sure your ECS task role (or local AWS credentials) has:

```json
{
  "Effect": "Allow",
  "Action": "cloudwatch:PutMetricData",
  "Resource": "*"
}
```

---

## üìÑ 2Ô∏è‚É£ Extend `metrics_callback.py`

At the top:

```python
import boto3, os

cloudwatch = boto3.client("cloudwatch", region_name=os.getenv("AWS_REGION", "us-east-1"))
CLOUDWATCH_NAMESPACE = "MultiRAG/LLM"
```

Then append a small helper:

```python
def push_cost_to_cloudwatch(model_name, in_tokens, out_tokens, cost):
    try:
        cloudwatch.put_metric_data(
            Namespace=CLOUDWATCH_NAMESPACE,
            MetricData=[
                {
                    "MetricName": "LLMCostUSD",
                    "Dimensions": [{"Name": "Model", "Value": model_name}],
                    "Unit": "None",
                    "Value": cost
                },
                {
                    "MetricName": "PromptTokens",
                    "Dimensions": [{"Name": "Model", "Value": model_name}],
                    "Unit": "Count",
                    "Value": in_tokens
                },
                {
                    "MetricName": "CompletionTokens",
                    "Dimensions": [{"Name": "Model", "Value": model_name}],
                    "Unit": "Count",
                    "Value": out_tokens
                }
            ]
        )
    except Exception as e:
        print(f"[CloudWatch] metric push failed: {e}")
```

Modify your `on_llm_end` section where you calculate cost:

```python
if model_name in MODEL_PRICING:
    p = MODEL_PRICING[model_name]
    cost = (in_tokens / 1000 * p["input"]) + (out_tokens / 1000 * p["output"])
    LLM_COST_USD.labels(model=model_name).set(cost)
    push_cost_to_cloudwatch(model_name, in_tokens, out_tokens, cost)
```

---

## üìä 3Ô∏è‚É£ View in CloudWatch Console

After a few queries:

1. Go to **CloudWatch ‚Üí Metrics ‚Üí All ‚Üí MultiRAG / LLM**
2. You‚Äôll see:

   * `LLMCostUSD` per model
   * `PromptTokens`
   * `CompletionTokens`

Create graphs or alarms (e.g., cost > $1/hour triggers an SNS alert).

---

## üìà 4Ô∏è‚É£ Optional Dashboards

You can build a CloudWatch dashboard JSON like:

```json
{
  "widgets": [
    {
      "type": "metric",
      "x": 0, "y": 0, "width": 12, "height": 6,
      "properties": {
        "metrics": [
          [ "MultiRAG/LLM", "LLMCostUSD", "Model", "gpt-4o" ],
          [ "...", "gpt-3.5-turbo" ]
        ],
        "stat": "Sum",
        "period": 300,
        "title": "RAG Model Cost (USD)"
      }
    }
  ]
}
```

---

## ‚úÖ Now you have

| Metric                              | System     | Purpose                 |
| ----------------------------------- | ---------- | ----------------------- |
| `rag_llm_cost_usd_total`            | Prometheus | Grafana live cost graph |
| `LLMCostUSD`                        | CloudWatch | AWS billing + alerts    |
| `PromptTokens` / `CompletionTokens` | CloudWatch | Usage analysis          |
| `rag_llm_latency_seconds`           | Prometheus | Performance trends      |

Both Grafana and CloudWatch can now track the same economic and performance data.

---

Would you like me to add a **CloudWatch Alarm + SNS notification** example (e.g., email if cost > $10/day or latency > 10 s)?



Excellent ‚Äî now let‚Äôs extend your CloudWatch setup with **automated alerts** using **CloudWatch Alarms + SNS notifications**.
You‚Äôll get an email or Slack (via webhook) if model cost or latency exceeds your threshold.

---

## 1Ô∏è‚É£ Create an SNS topic for notifications

Add to your Terraform or run once manually:

```hcl
resource "aws_sns_topic" "rag_alerts" {
  name = "rag-alerts"
}

resource "aws_sns_topic_subscription" "rag_alerts_email" {
  topic_arn = aws_sns_topic.rag_alerts.arn
  protocol  = "email"
  endpoint  = "you@example.com"  # Replace with your email
}
```

After `terraform apply`, AWS will send a confirmation email ‚Äî you must **confirm** it before alarms send alerts.

---

## 2Ô∏è‚É£ Create CloudWatch alarms for cost & latency

Add to Terraform or use boto3 as below.

### üí∞ Cost alarm

```python
import boto3

cloudwatch = boto3.client("cloudwatch")

cloudwatch.put_metric_alarm(
    AlarmName="RAGCostHigh",
    AlarmDescription="Alert if total cost exceeds $10 per day",
    Namespace="MultiRAG/LLM",
    MetricName="LLMCostUSD",
    Dimensions=[{"Name": "Model", "Value": "gpt-4o"}],
    Statistic="Sum",
    Period=86400,  # 1 day
    EvaluationPeriods=1,
    Threshold=10.0,
    ComparisonOperator="GreaterThanThreshold",
    AlarmActions=["arn:aws:sns:us-east-1:123456789012:rag-alerts"],
    TreatMissingData="notBreaching"
)
```

---

### üïí Latency alarm

```python
cloudwatch.put_metric_alarm(
    AlarmName="RAGLatencyHigh",
    AlarmDescription="Alert if average LLM latency > 10s",
    Namespace="MultiRAG/LLM",
    MetricName="LLMLatencySeconds",
    Dimensions=[{"Name": "Model", "Value": "gpt-4o"}],
    Statistic="Average",
    Period=300,  # 5 minutes
    EvaluationPeriods=1,
    Threshold=10.0,
    ComparisonOperator="GreaterThanThreshold",
    AlarmActions=["arn:aws:sns:us-east-1:123456789012:rag-alerts"],
    TreatMissingData="notBreaching"
)
```

---

## 3Ô∏è‚É£ Verify in AWS Console

Navigate to:

* **CloudWatch ‚Üí Alarms**
* You‚Äôll see:

  * ‚úÖ `RAGCostHigh`
  * ‚úÖ `RAGLatencyHigh`

When triggered:

* They push to the **`rag-alerts`** SNS topic.
* You‚Äôll get an email like:
  *‚ÄúAlarm 'RAGCostHigh' in state ALARM: cost exceeded $10.‚Äù*

---

## 4Ô∏è‚É£ (Optional) Slack integration via AWS Chatbot

Instead of email, connect SNS to Slack:

1. Go to **AWS Chatbot ‚Üí Configure new client ‚Üí Slack**
2. Choose the SNS topic `rag-alerts`
3. Select a Slack channel

Now alerts will appear directly in your Slack workspace.

---

## ‚úÖ Summary

| Alert              | Metric              | Condition    | Action            |
| ------------------ | ------------------- | ------------ | ----------------- |
| **RAGCostHigh**    | `LLMCostUSD`        | > $10/day    | SNS ‚Üí Email/Slack |
| **RAGLatencyHigh** | `LLMLatencySeconds` | > 10 sec avg | SNS ‚Üí Email/Slack |

---

Would you like me to also add a **Terraform module** for these alarms (so they deploy automatically with your ECS stack)?


Awesome ‚Äî let‚Äôs make this fully automated with a Terraform module so your **RAG system** deploys CloudWatch alarms + SNS alerts in the same `infra/` stack.

---

## üìÅ Folder layout

```
infra/
‚îÇ
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ alb.tf
‚îú‚îÄ‚îÄ cloudwatch.tf        # <‚Äî New file for alarms + SNS
‚îî‚îÄ‚îÄ provider.tf
```

---

## ‚öôÔ∏è `cloudwatch.tf`

```hcl
# ===============================
# SNS topic for notifications
# ===============================
resource "aws_sns_topic" "rag_alerts" {
  name = "${var.project_name}-alerts"
}

# Optional: email subscription
resource "aws_sns_topic_subscription" "rag_alerts_email" {
  count     = var.alert_email != "" ? 1 : 0
  topic_arn = aws_sns_topic.rag_alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

# ===============================
# CloudWatch Alarms
# ===============================
# üí∞ Cost alarm
resource "aws_cloudwatch_metric_alarm" "rag_cost_high" {
  alarm_name          = "${var.project_name}-cost-high"
  alarm_description   = "Alert if total cost exceeds threshold per day"
  namespace           = "MultiRAG/LLM"
  metric_name         = "LLMCostUSD"
  statistic           = "Sum"
  period              = 86400       # 1 day
  evaluation_periods  = 1
  threshold           = var.cost_threshold_usd
  comparison_operator = "GreaterThanThreshold"
  treat_missing_data  = "notBreaching"
  alarm_actions       = [aws_sns_topic.rag_alerts.arn]
  dimensions = {
    Model = "gpt-4o"
  }
}

# üïí Latency alarm
resource "aws_cloudwatch_metric_alarm" "rag_latency_high" {
  alarm_name          = "${var.project_name}-latency-high"
  alarm_description   = "Alert if average latency > threshold (s)"
  namespace           = "MultiRAG/LLM"
  metric_name         = "LLMLatencySeconds"
  statistic           = "Average"
  period              = 300         # 5 minutes
  evaluation_periods  = 1
  threshold           = var.latency_threshold_s
  comparison_operator = "GreaterThanThreshold"
  treat_missing_data  = "notBreaching"
  alarm_actions       = [aws_sns_topic.rag_alerts.arn]
  dimensions = {
    Model = "gpt-4o"
  }
}
```

---

## üß© `variables.tf` additions

```hcl
variable "alert_email" {
  description = "Email address for alarm notifications (leave blank to disable)"
  default     = ""
}

variable "cost_threshold_usd" {
  description = "Daily cost threshold for alarm (USD)"
  type        = number
  default     = 10.0
}

variable "latency_threshold_s" {
  description = "Average latency threshold (seconds)"
  type        = number
  default     = 10.0
}
```

---

## üì§ `outputs.tf` addition

```hcl
output "sns_topic_arn" {
  description = "ARN of the SNS topic used for alerts"
  value       = aws_sns_topic.rag_alerts.arn
}
```

---

## üß≠ Apply it

```bash
cd infra
terraform init
terraform plan -out plan.tfplan
terraform apply plan.tfplan
```

AWS will:

* Create the SNS topic
* Email a confirmation to your address
* Deploy both alarms (cost + latency)
* Link them to the topic

---

## ‚úÖ End result

| Resource                                       | Purpose             | Trigger           | Action        |
| ---------------------------------------------- | ------------------- | ----------------- | ------------- |
| `aws_sns_topic.rag_alerts`                     | Notification hub    | ‚Äî                 | Email / Slack |
| `aws_cloudwatch_metric_alarm.rag_cost_high`    | Cost > threshold    | Daily cost in USD | Notify        |
| `aws_cloudwatch_metric_alarm.rag_latency_high` | Latency > threshold | Average > 10s     | Notify        |

---

Would you like me to **add Slack webhook integration** through AWS Chatbot into the Terraform module (so alerts post directly to a Slack channel instead of email)?


Perfect ‚Äî let‚Äôs extend your Terraform setup to automatically send those CloudWatch alerts **directly into Slack** via **AWS Chatbot**.

You‚Äôll keep the same SNS topic, but it will now be linked to a Slack channel of your choice.

---

## üß© Prerequisites

Before deploying this, make sure:

1. You‚Äôve connected Slack to AWS Chatbot once:

   * Go to ‚Üí [AWS Chatbot Console ‚Üí Configure Slack](https://console.aws.amazon.com/chatbot/home#/slack)
   * Approve the app and workspace.
   * Copy your **workspace ID** and **channel ID** from the Chatbot console.

---

## ‚öôÔ∏è Update your `cloudwatch.tf`

Add the following:

```hcl
# =====================================================
# AWS Chatbot Slack integration
# =====================================================

resource "aws_chatbot_slack_channel_configuration" "rag_chatbot_slack" {
  name              = "${var.project_name}-slack-alerts"
  slack_channel_id  = var.slack_channel_id
  slack_workspace_id = var.slack_workspace_id
  sns_topic_arns    = [aws_sns_topic.rag_alerts.arn]
  logging_level     = "ERROR"
  guardrail_policies = [
    "arn:aws:iam::aws:policy/CloudWatchReadOnlyAccess"
  ]
}
```

---

## üß© Add new variables to `variables.tf`

```hcl
variable "slack_workspace_id" {
  description = "Slack workspace ID from AWS Chatbot console"
  type        = string
  default     = ""
}

variable "slack_channel_id" {
  description = "Slack channel ID where alerts will be posted"
  type        = string
  default     = ""
}
```

---

## üß≠ Apply configuration

```bash
cd infra
terraform apply -auto-approve
```

Terraform will:

* Link your SNS topic to Chatbot
* Register the Slack channel
* Forward all `rag_alerts` SNS messages (cost/latency) to Slack

---

## üß† Slack output example

You‚Äôll get alerts in your Slack channel like this:

```
[ALARM] "RAGCostHigh" in US-EAST-1
State: ALARM
Reason: Threshold Crossed: 1 datapoint [15.0 (2025-10-31T00:00:00Z)] was greater than the threshold (10.0)
```

And for latency:

```
[ALARM] "RAGLatencyHigh" in US-EAST-1
Average latency: 12.4s > 10.0s threshold
```

---

## ‚úÖ Final Terraform summary

| Resource                                                    | Purpose                 |
| ----------------------------------------------------------- | ----------------------- |
| `aws_sns_topic.rag_alerts`                                  | Notification hub        |
| `aws_chatbot_slack_channel_configuration.rag_chatbot_slack` | Slack integration       |
| `aws_cloudwatch_metric_alarm.rag_cost_high`                 | Cost threshold alert    |
| `aws_cloudwatch_metric_alarm.rag_latency_high`              | Latency threshold alert |

---

Would you like me to include a **Slack message formatter Lambda** so that alerts appear with colored cards (green = OK, red = Alarm, gray = Insufficient data)? It makes alerts visually cleaner in Slack.



Here‚Äôs how you make the Slack alerts look **clean and color-coded** without touching Chatbot‚Äôs managed integration.
You‚Äôll add a small **Lambda function** that formats messages and a CloudWatch subscription to invoke it.

---

## üß© Folder layout

```
infra/
‚îÇ
‚îú‚îÄ‚îÄ cloudwatch.tf
‚îú‚îÄ‚îÄ lambda_slack_alert/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ variables.tf
```

---

## üêç `lambda_slack_alert/main.py`

```python
import json
import os
import urllib3

http = urllib3.PoolManager()

WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")

COLOR_MAP = {
    "ALARM": "#FF0000",
    "OK": "#36A64F",
    "INSUFFICIENT_DATA": "#AAAAAA"
}

def lambda_handler(event, context):
    for record in event["Records"]:
        msg = json.loads(record["Sns"]["Message"])
        state = msg.get("NewStateValue", "UNKNOWN")
        color = COLOR_MAP.get(state, "#CCCCCC")

        text = f"*Alarm:* {msg.get('AlarmName')}\n" \
               f"*State:* {state}\n" \
               f"*Reason:* {msg.get('NewStateReason')}\n" \
               f"*Region:* {msg.get('Region')}\n" \
               f"*Time:* {msg.get('StateChangeTime')}"

        payload = {
            "attachments": [{
                "color": color,
                "blocks": [{
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": text}
                }]
            }]
        }

        try:
            http.request("POST", WEBHOOK_URL,
                         body=json.dumps(payload),
                         headers={"Content-Type": "application/json"})
        except Exception as e:
            print(f"Slack webhook failed: {e}")
    return {"status": "ok"}
```

---

## üß© `lambda_slack_alert/requirements.txt`

```
urllib3
```

---

## ‚öôÔ∏è Add to Terraform (`cloudwatch.tf`)

```hcl
# ==============================================
# Lambda for Slack alert formatting
# ==============================================
data "archive_file" "slack_lambda_zip" {
  type        = "zip"
  source_dir  = "${path.module}/lambda_slack_alert"
  output_path = "${path.module}/lambda_slack_alert.zip"
}

resource "aws_lambda_function" "slack_formatter" {
  filename         = data.archive_file.slack_lambda_zip.output_path
  function_name    = "${var.project_name}-slack-formatter"
  role             = aws_iam_role.slack_lambda_role.arn
  handler          = "main.lambda_handler"
  runtime          = "python3.11"
  timeout          = 10
  environment {
    variables = {
      SLACK_WEBHOOK_URL = var.slack_webhook_url
    }
  }
}

resource "aws_iam_role" "slack_lambda_role" {
  name = "${var.project_name}-slack-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Action    = "sts:AssumeRole",
      Effect    = "Allow",
      Principal = { Service = "lambda.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_basic_exec" {
  role       = aws_iam_role.slack_lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

# SNS ‚Üí Lambda subscription
resource "aws_sns_topic_subscription" "rag_alerts_lambda" {
  topic_arn = aws_sns_topic.rag_alerts.arn
  protocol  = "lambda"
  endpoint  = aws_lambda_function.slack_formatter.arn
}

resource "aws_lambda_permission" "allow_sns_invoke" {
  statement_id  = "AllowExecutionFromSNS"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.slack_formatter.function_name
  principal     = "sns.amazonaws.com"
  source_arn    = aws_sns_topic.rag_alerts.arn
}
```

---

## üß© Update `variables.tf`

```hcl
variable "slack_webhook_url" {
  description = "Slack Incoming Webhook URL for formatted alerts"
  type        = string
  default     = ""
}
```

---

## üß≠ Deploy

```bash
cd infra
terraform apply -auto-approve
```

---

## üß† Slack Output Example

Each alert appears as a color card:

* üî¥ **ALARM (red)** ‚Äî threshold exceeded
* üü¢ **OK (green)** ‚Äî issue resolved
* ‚ö™ **INSUFFICIENT_DATA (gray)** ‚Äî metric not reporting

Example:

```
Alarm: RAGLatencyHigh
State: ALARM
Reason: Average latency 12.3s > threshold 10s
Region: us-east-1
Time: 2025-10-31T14:15:22Z
```

---

## ‚úÖ Final summary

| Component            | Purpose                                   |
| -------------------- | ----------------------------------------- |
| Lambda (Python)      | Formats CloudWatch alarms as Slack blocks |
| SNS Subscription     | Triggers Lambda when alarm fires          |
| Slack Webhook        | Posts color-coded messages                |
| Chatbot              | (Optional) still available for raw alerts |
| Terraform automation | Deploys all end-to-end                    |

---

Would you like me to add a **Slack thread aggregator** next ‚Äî so all alerts for the same `AlarmName` group into a single Slack thread instead of separate messages?


Perfect üëç ‚Äî grouping alerts into threads makes Slack much cleaner, especially if the same alarm (like latency or cost) flips between `OK` and `ALARM` several times a day.

We‚Äôll modify your Lambda to **use Slack‚Äôs `thread_ts`** feature so all messages related to one alarm post into a single threaded conversation.

---

## üß© How it works

1. The first alert for a given `AlarmName` posts a **new message**.
2. The Lambda **stores the thread timestamp (`thread_ts`)** in DynamoDB.
3. Any subsequent alerts for the same alarm reuse that `thread_ts`, posting replies instead of new top-level messages.

---

## üìÅ Folder layout

```
lambda_slack_alert/
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ requirements.txt
```

---

## üêç Updated `lambda_slack_alert/main.py`

```python
import json, os, urllib3, boto3
from boto3.dynamodb.conditions import Key

http = urllib3.PoolManager()

WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")
DYNAMO_TABLE = os.getenv("DYNAMO_TABLE")
COLOR_MAP = {"ALARM": "#FF0000", "OK": "#36A64F", "INSUFFICIENT_DATA": "#AAAAAA"}

dynamo = boto3.resource("dynamodb")
table = dynamo.Table(DYNAMO_TABLE)

def post_to_slack(payload):
    r = http.request("POST", WEBHOOK_URL, body=json.dumps(payload),
                     headers={"Content-Type": "application/json"})
    return r.status

def lambda_handler(event, context):
    for record in event["Records"]:
        msg = json.loads(record["Sns"]["Message"])
        alarm_name = msg.get("AlarmName")
        state = msg.get("NewStateValue", "UNKNOWN")
        color = COLOR_MAP.get(state, "#CCCCCC")

        # Lookup thread_ts in DynamoDB
        response = table.query(KeyConditionExpression=Key("alarm_name").eq(alarm_name))
        thread_ts = None
        if response["Items"]:
            thread_ts = response["Items"][0]["thread_ts"]

        text = f"*Alarm:* {alarm_name}\n" \
               f"*State:* {state}\n" \
               f"*Reason:* {msg.get('NewStateReason')}\n" \
               f"*Region:* {msg.get('Region')}\n" \
               f"*Time:* {msg.get('StateChangeTime')}"

        payload = {
            "attachments": [{
                "color": color,
                "blocks": [{
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": text}
                }]
            }]
        }

        if thread_ts:
            payload["thread_ts"] = thread_ts

        r = http.request("POST", WEBHOOK_URL,
                         body=json.dumps(payload),
                         headers={"Content-Type": "application/json"})

        # Slack returns the thread_ts in response if it‚Äôs a new top-level post
        if not thread_ts and r.status == 200:
            resp_data = json.loads(r.data.decode("utf-8"))
            new_thread_ts = resp_data.get("ts")
            if new_thread_ts:
                table.put_item(Item={"alarm_name": alarm_name, "thread_ts": new_thread_ts})

    return {"status": "ok"}
```

---

## üß© Add DynamoDB table to Terraform (`cloudwatch.tf`)

```hcl
resource "aws_dynamodb_table" "slack_threads" {
  name           = "${var.project_name}-slack-threads"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "alarm_name"

  attribute {
    name = "alarm_name"
    type = "S"
  }
}
```

And link it in your Lambda definition:

```hcl
resource "aws_lambda_function" "slack_formatter" {
  # ...
  environment {
    variables = {
      SLACK_WEBHOOK_URL = var.slack_webhook_url
      DYNAMO_TABLE      = aws_dynamodb_table.slack_threads.name
    }
  }
}
```

Also, attach permissions for DynamoDB access:

```hcl
resource "aws_iam_role_policy_attachment" "lambda_dynamo_access" {
  role       = aws_iam_role.slack_lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess"
}
```

---

## ‚úÖ Now your alerts behave like this:

| Event                                                     | Slack behavior                                       |
| --------------------------------------------------------- | ---------------------------------------------------- |
| First alarm trigger                                       | New Slack message                                    |
| Subsequent state changes (OK / ALARM / INSUFFICIENT_DATA) | Appear as **thread replies**                         |
| Thread storage                                            | DynamoDB table with `alarm_name ‚Üí thread_ts` mapping |

---

## üîî Example Slack view

```
üî¥ Alarm: RAGLatencyHigh
State: ALARM
Reason: Average latency 12.4s > threshold 10.0s

‚îî üí¨ OK ‚Äî Latency back to normal (8.2s)
‚îî üí¨ ALARM ‚Äî Latency spike detected again (13.1s)
```

---

Would you like me to also add **Slack mention rules** (e.g., `@devops` for ALARM, `@all` for sustained issues >1h)?



Perfect üîî ‚Äî now you‚Äôll make your **Slack alerts more actionable** by tagging people or groups depending on alarm state or severity.
We‚Äôll extend the Lambda so it can:

‚úÖ Mention specific users or roles (e.g. `@devops`) when entering ALARM state.
‚úÖ Mention `@all` or escalation groups if the alarm stays active >1 hour.
‚úÖ Stay quiet for OK/normal transitions.

---

## üß© Updated `lambda_slack_alert/main.py`

```python
import json, os, urllib3, boto3, time
from boto3.dynamodb.conditions import Key

http = urllib3.PoolManager()

WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")
DYNAMO_TABLE = os.getenv("DYNAMO_TABLE")
ALERT_USER = os.getenv("SLACK_ALERT_USER", "@devops")      # mention on first alarm
ESCALATE_USER = os.getenv("SLACK_ESCALATE_USER", "@all")   # mention if >1h unresolved
ESCALATE_AFTER_S = int(os.getenv("ESCALATE_AFTER_S", "3600"))

COLOR_MAP = {"ALARM": "#FF0000", "OK": "#36A64F", "INSUFFICIENT_DATA": "#AAAAAA"}

dynamo = boto3.resource("dynamodb")
table = dynamo.Table(DYNAMO_TABLE)

def lambda_handler(event, context):
    for record in event["Records"]:
        msg = json.loads(record["Sns"]["Message"])
        alarm_name = msg.get("AlarmName")
        state = msg.get("NewStateValue", "UNKNOWN")
        color = COLOR_MAP.get(state, "#CCCCCC")

        now = int(time.time())

        # Lookup thread info
        response = table.query(KeyConditionExpression=Key("alarm_name").eq(alarm_name))
        thread_ts = None
        first_seen = None
        if response["Items"]:
            thread_ts = response["Items"][0]["thread_ts"]
            first_seen = response["Items"][0].get("first_seen", now)

        # Build mention text
        mention = ""
        if state == "ALARM":
            mention = ALERT_USER
            if first_seen and now - first_seen > ESCALATE_AFTER_S:
                mention = ESCALATE_USER

        text = f"{mention}\n*Alarm:* {alarm_name}\n" \
               f"*State:* {state}\n" \
               f"*Reason:* {msg.get('NewStateReason')}\n" \
               f"*Region:* {msg.get('Region')}\n" \
               f"*Time:* {msg.get('StateChangeTime')}"

        payload = {
            "attachments": [{
                "color": color,
                "blocks": [{
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": text}
                }]
            }]
        }
        if thread_ts:
            payload["thread_ts"] = thread_ts

        # Post to Slack
        resp = http.request("POST", WEBHOOK_URL,
                            body=json.dumps(payload),
                            headers={"Content-Type": "application/json"})

        # Store new thread_ts
        if not thread_ts and resp.status == 200:
            try:
                resp_data = json.loads(resp.data.decode("utf-8"))
                ts = resp_data.get("ts")
                if ts:
                    table.put_item(Item={"alarm_name": alarm_name, "thread_ts": ts, "first_seen": now})
            except Exception:
                pass

        # Reset first_seen on OK
        if state == "OK" and thread_ts:
            table.update_item(
                Key={"alarm_name": alarm_name},
                UpdateExpression="SET first_seen = :t",
                ExpressionAttributeValues={":t": now}
            )

    return {"status": "ok"}
```

---

## üß© Update Terraform Lambda environment

```hcl
environment {
  variables = {
    SLACK_WEBHOOK_URL   = var.slack_webhook_url
    DYNAMO_TABLE        = aws_dynamodb_table.slack_threads.name
    SLACK_ALERT_USER    = "@devops"
    SLACK_ESCALATE_USER = "@all"
    ESCALATE_AFTER_S    = "3600"  # 1 hour
  }
}
```

---

## ‚úÖ Behavior summary

| Alarm state                        | Mentions                        | Example                |
| ---------------------------------- | ------------------------------- | ---------------------- |
| **ALARM (first occurrence)**       | `@devops`                       | Immediate notification |
| **ALARM (still active after 1 h)** | `@all`                          | Escalation             |
| **OK**                             | none (but thread updated green) | Closes loop            |
| **INSUFFICIENT_DATA**              | none                            | Keeps thread quiet     |

---

## üîî Example Slack thread

```
@devops
Alarm: RAGCostHigh
State: ALARM
Reason: Cost $15 > $10 threshold

‚îî üí¨ OK ‚Äî Cost back to normal ($6)
‚îî üí¨ @all ALARM ‚Äî Cost spike persists >1h ($20)
```

---

## üéØ Optional fine-tuning

You can add more conditions:

* Different alert groups per alarm name pattern (e.g., cost ‚Üí FinOps, latency ‚Üí DevOps)
* Add a `severity` label in CloudWatch alarm description and mention based on it

---

Would you like me to show that **dynamic severity-based routing** next (so different alarm types ping different Slack users or channels automatically)?



Perfect ‚Äî the final layer of maturity for your alerting system is **severity-based routing**.
This lets you tag alarms as `critical`, `warning`, or `info` directly in Terraform (or CloudWatch) and have the Lambda route them to **different Slack users/channels** automatically.

---

## üß© Concept

You‚Äôll define a `Severity` tag on every CloudWatch alarm.
The Lambda reads it from the SNS payload and chooses which Slack destination (webhook or mention) to use.

Example:

| Severity | Destination               | Mention   |
| -------- | ------------------------- | --------- |
| critical | `@all` / #devops-critical | escalated |
| warning  | #devops                   | @devops   |
| info     | #observability            | none      |

---

## üß© Updated Terraform for Alarms

Add tags or description with severity:

```hcl
resource "aws_cloudwatch_metric_alarm" "rag_cost_high" {
  alarm_name          = "${var.project_name}-cost-high"
  alarm_description   = "severity=critical | Alert if daily cost > threshold"
  namespace           = "MultiRAG/LLM"
  metric_name         = "LLMCostUSD"
  statistic           = "Sum"
  period              = 86400
  evaluation_periods  = 1
  threshold           = var.cost_threshold_usd
  comparison_operator = "GreaterThanThreshold"
  alarm_actions       = [aws_sns_topic.rag_alerts.arn]
  dimensions = {
    Model = "gpt-4o"
  }
  tags = {
    Severity = "critical"
  }
}

resource "aws_cloudwatch_metric_alarm" "rag_latency_high" {
  alarm_name          = "${var.project_name}-latency-high"
  alarm_description   = "severity=warning | Alert if avg latency > threshold"
  namespace           = "MultiRAG/LLM"
  metric_name         = "LLMLatencySeconds"
  statistic           = "Average"
  period              = 300
  evaluation_periods  = 1
  threshold           = var.latency_threshold_s
  comparison_operator = "GreaterThanThreshold"
  alarm_actions       = [aws_sns_topic.rag_alerts.arn]
  dimensions = {
    Model = "gpt-4o"
  }
  tags = {
    Severity = "warning"
  }
}
```

---

## üêç Enhanced Lambda for Severity Routing

```python
import json, os, urllib3, boto3, time, re
from boto3.dynamodb.conditions import Key

http = urllib3.PoolManager()

WEBHOOK_CRITICAL = os.getenv("SLACK_WEBHOOK_CRITICAL")
WEBHOOK_WARNING = os.getenv("SLACK_WEBHOOK_WARNING")
WEBHOOK_INFO = os.getenv("SLACK_WEBHOOK_INFO")

DYNAMO_TABLE = os.getenv("DYNAMO_TABLE")

COLOR_MAP = {"ALARM": "#FF0000", "OK": "#36A64F", "INSUFFICIENT_DATA": "#AAAAAA"}
dynamo = boto3.resource("dynamodb")
table = dynamo.Table(DYNAMO_TABLE)

def extract_severity(description, tags):
    """Infer severity from alarm tags or description"""
    if tags and "Severity" in tags:
        return tags["Severity"].lower()
    if description:
        match = re.search(r"severity\s*=\s*(critical|warning|info)", description, re.I)
        if match:
            return match.group(1).lower()
    return "info"

def get_webhook_for_severity(sev):
    if sev == "critical":
        return WEBHOOK_CRITICAL or WEBHOOK_WARNING
    elif sev == "warning":
        return WEBHOOK_WARNING or WEBHOOK_INFO
    else:
        return WEBHOOK_INFO

def lambda_handler(event, context):
    for record in event["Records"]:
        msg = json.loads(record["Sns"]["Message"])
        alarm_name = msg.get("AlarmName")
        state = msg.get("NewStateValue", "UNKNOWN")
        desc = msg.get("AlarmDescription", "")
        tags = msg.get("AlarmTags", {})
        sev = extract_severity(desc, tags)

        webhook = get_webhook_for_severity(sev)
        color = COLOR_MAP.get(state, "#CCCCCC")

        # Thread handling
        response = table.query(KeyConditionExpression=Key("alarm_name").eq(alarm_name))
        thread_ts = response["Items"][0]["thread_ts"] if response["Items"] else None

        text = f"*[{sev.upper()}]* Alarm: {alarm_name}\n" \
               f"*State:* {state}\n" \
               f"*Reason:* {msg.get('NewStateReason')}\n" \
               f"*Region:* {msg.get('Region')}\n" \
               f"*Time:* {msg.get('StateChangeTime')}"

        payload = {
            "attachments": [{
                "color": color,
                "blocks": [{
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": text}
                }]
            }]
        }
        if thread_ts:
            payload["thread_ts"] = thread_ts

        http.request("POST", webhook,
                     body=json.dumps(payload),
                     headers={"Content-Type": "application/json"})
```

---

## üß© Terraform Lambda Environment

```hcl
environment {
  variables = {
    DYNAMO_TABLE          = aws_dynamodb_table.slack_threads.name
    SLACK_WEBHOOK_CRITICAL = var.slack_webhook_critical
    SLACK_WEBHOOK_WARNING  = var.slack_webhook_warning
    SLACK_WEBHOOK_INFO     = var.slack_webhook_info
  }
}
```

---

## üß© Add to `variables.tf`

```hcl
variable "slack_webhook_critical" {
  description = "Webhook for critical alerts"
  type        = string
  default     = ""
}

variable "slack_webhook_warning" {
  description = "Webhook for warning alerts"
  type        = string
  default     = ""
}

variable "slack_webhook_info" {
  description = "Webhook for info alerts"
  type        = string
  default     = ""
}
```

---

## ‚úÖ Final Behavior Summary

| Severity     | Example Alarm    | Slack Channel         | Mention   |
| ------------ | ---------------- | --------------------- | --------- |
| **critical** | Cost > threshold | üî¥ `#devops-critical` | `@all`    |
| **warning**  | Latency high     | üü† `#devops`          | `@devops` |
| **info**     | Minor event      | üîµ `#observability`   | ‚Äî         |

---

This routing keeps your critical incidents separate from noise, makes sure the right teams are paged, and you can scale it just by setting tags or descriptions in Terraform.

---

Would you like me to show how to add **PagerDuty escalation integration** (so critical alarms can page on-call engineers via phone/SMS automatically)?




Got it ‚úÖ ‚Äî here‚Äôs how to add **PagerDuty escalation integration** so your *critical* alarms trigger on-call incidents automatically while still posting color-coded Slack threads.

---

## üîÅ High-level flow

```
CloudWatch Alarm (critical)
    ‚Üì
SNS topic (rag-alerts)
    ‚Üì
Lambda (formats + sends to Slack)
    ‚Üì
PagerDuty Event API (creates or resolves incident)
```

---

## 1Ô∏è‚É£ Create a PagerDuty integration key

1. Log into PagerDuty ‚Üí **Services ‚Üí + New Service**
2. Choose **Integration Type: ‚ÄúEvents API v2‚Äù**
3. Copy the **Integration Key** (e.g. `b52f0d1234567890abcdef1234567890`)
4. You‚Äôll use this as a Terraform variable.

---

## 2Ô∏è‚É£ Update Terraform Lambda environment

In `cloudwatch.tf` Lambda resource:

```hcl
environment {
  variables = {
    DYNAMO_TABLE           = aws_dynamodb_table.slack_threads.name
    SLACK_WEBHOOK_CRITICAL = var.slack_webhook_critical
    SLACK_WEBHOOK_WARNING  = var.slack_webhook_warning
    SLACK_WEBHOOK_INFO     = var.slack_webhook_info
    PAGERDUTY_ROUTING_KEY  = var.pagerduty_routing_key
  }
}
```

Add to `variables.tf`:

```hcl
variable "pagerduty_routing_key" {
  description = "PagerDuty Events API v2 integration key"
  type        = string
  default     = ""
}
```

---

## 3Ô∏è‚É£ Extend Lambda with PagerDuty calls

Update `lambda_slack_alert/main.py` to include:

```python
PAGERDUTY_KEY = os.getenv("PAGERDUTY_ROUTING_KEY")

def send_to_pagerduty(alarm_name, state, reason, sev):
    """Send event to PagerDuty using Events API v2"""
    if not PAGERDUTY_KEY or sev != "critical":
        return

    event_type = "trigger" if state == "ALARM" else "resolve"
    payload = {
        "routing_key": PAGERDUTY_KEY,
        "event_action": event_type,
        "dedup_key": alarm_name,
        "payload": {
            "summary": f"{alarm_name}: {reason}",
            "severity": "critical" if state == "ALARM" else "info",
            "source": "aws.cloudwatch",
            "component": "MultiRAG",
            "group": "AI-Infra",
            "class": "RAG-Service"
        }
    }
    try:
        http.request(
            "POST",
            "https://events.pagerduty.com/v2/enqueue",
            body=json.dumps(payload),
            headers={"Content-Type": "application/json"},
        )
    except Exception as e:
        print(f"PagerDuty send failed: {e}")
```

Then inside your main loop (after posting to Slack):

```python
if sev == "critical":
    send_to_pagerduty(alarm_name, state, msg.get("NewStateReason"), sev)
```

---

## 4Ô∏è‚É£ PagerDuty incident flow

| Alarm State              | PagerDuty Action                    | Slack Action        |
| ------------------------ | ----------------------------------- | ------------------- |
| **ALARM (critical)**     | Creates new incident, pages on-call | üî¥ New Slack thread |
| **OK**                   | Resolves incident                   | üü¢ Thread update    |
| **ALARM (warning/info)** | No PD trigger                       | üü† / üîµ Slack only  |

---

## 5Ô∏è‚É£ Optional Terraform validation alarm

You can test by setting a temporary alarm threshold very low:

```hcl
resource "aws_cloudwatch_metric_alarm" "rag_cost_test" {
  alarm_name          = "TestPagerDutyAlarm"
  alarm_description   = "severity=critical | Test PagerDuty trigger"
  namespace           = "MultiRAG/Test"
  metric_name         = "DummyMetric"
  statistic           = "Average"
  period              = 60
  evaluation_periods  = 1
  threshold           = 0.1
  comparison_operator = "GreaterThanThreshold"
  alarm_actions       = [aws_sns_topic.rag_alerts.arn]
  tags = {
    Severity = "critical"
  }
}
```

Trigger manually, watch it open and resolve an incident in PagerDuty.

---

## ‚úÖ Final architecture summary

| Component                         | Role                                               |
| --------------------------------- | -------------------------------------------------- |
| **CloudWatch Alarms (Terraform)** | Define metrics & severity                          |
| **SNS (rag-alerts)**              | Central event hub                                  |
| **Lambda (slack_formatter)**      | Formats Slack messages, threads, mentions, routing |
| **DynamoDB (thread_ts)**          | Keeps Slack threads grouped                        |
| **PagerDuty API**                 | Escalates critical alarms                          |
| **Slack (multiple webhooks)**     | Visual notifications                               |
| **Prometheus/Grafana**            | Performance metrics & dashboards                   |

---

Would you like me to extend this to include **incident auto-enrichment** ‚Äî e.g., attaching last 10 Prometheus metrics or recent ECS logs directly into the PagerDuty incident notes?


Excellent üî• ‚Äî adding **auto-enrichment** makes your PagerDuty incidents *self-diagnosing*: every time a critical alarm fires, it attaches the latest metrics and logs, so the on-call doesn‚Äôt need to hunt for context.

Below is how you extend the same Lambda to pull **Prometheus metrics** and **ECS logs (via CloudWatch Logs)** and send them as incident notes.

---

## 1Ô∏è‚É£ Extend Lambda permissions

In Terraform, attach extra policies:

```hcl
resource "aws_iam_role_policy_attachment" "lambda_prometheus_logs" {
  role       = aws_iam_role.slack_lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/CloudWatchLogsReadOnlyAccess"
}

resource "aws_iam_role_policy_attachment" "lambda_prometheus_query" {
  role       = aws_iam_role.slack_lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/CloudWatchReadOnlyAccess"
}
```

---

## 2Ô∏è‚É£ Add environment variables

```hcl
environment {
  variables = {
    DYNAMO_TABLE           = aws_dynamodb_table.slack_threads.name
    SLACK_WEBHOOK_CRITICAL = var.slack_webhook_critical
    SLACK_WEBHOOK_WARNING  = var.slack_webhook_warning
    SLACK_WEBHOOK_INFO     = var.slack_webhook_info
    PAGERDUTY_ROUTING_KEY  = var.pagerduty_routing_key
    PROM_URL               = var.prometheus_url     # e.g. http://prometheus:9090
    LOG_GROUP              = var.ecs_log_group_name # e.g. /aws/ecs/multi-rag
  }
}
```

Add to `variables.tf`:

```hcl
variable "prometheus_url" {
  description = "Base URL of Prometheus for querying metrics"
  type        = string
  default     = ""
}

variable "ecs_log_group_name" {
  description = "CloudWatch log group name for ECS RAG service"
  type        = string
  default     = ""
}
```

---

## 3Ô∏è‚É£ Lambda enrichment code snippet

Add these helpers to `main.py`:

```python
import datetime, urllib.parse

PROM_URL = os.getenv("PROM_URL")
LOG_GROUP = os.getenv("LOG_GROUP")

logs_client = boto3.client("logs")

def fetch_prometheus_metrics(metric, lookback="5m"):
    if not PROM_URL:
        return "Prometheus not configured"
    end = int(time.time())
    start = end - 300
    query = f"{metric}[{lookback}]"
    url = f"{PROM_URL}/api/v1/query?query={urllib.parse.quote(query)}"
    r = http.request("GET", url)
    if r.status != 200:
        return "Prometheus query failed"
    data = json.loads(r.data.decode())
    results = data.get("data", {}).get("result", [])
    summary = [f"{m['metric']}: {m['value']}" for m in results[:5]]
    return "\n".join(summary) if summary else "No metric data"

def fetch_recent_logs(log_group, limit=10):
    if not log_group:
        return "Log group not set"
    try:
        streams = logs_client.describe_log_streams(
            logGroupName=log_group,
            orderBy="LastEventTime",
            descending=True,
            limit=1
        )
        stream_name = streams["logStreams"][0]["logStreamName"]
        events = logs_client.get_log_events(
            logGroupName=log_group,
            logStreamName=stream_name,
            limit=limit,
            startFromHead=False
        )["events"]
        lines = [e["message"] for e in events]
        return "\n".join(lines)
    except Exception as e:
        return f"Log fetch failed: {e}"
```

---

## 4Ô∏è‚É£ Attach enrichment to PagerDuty call

Modify your `send_to_pagerduty`:

```python
def send_to_pagerduty(alarm_name, state, reason, sev):
    if not PAGERDUTY_KEY or sev != "critical":
        return

    # Gather enrichment data
    metrics = fetch_prometheus_metrics("rag_llm_latency_seconds")
    logs = fetch_recent_logs(LOG_GROUP)

    event_type = "trigger" if state == "ALARM" else "resolve"
    payload = {
        "routing_key": PAGERDUTY_KEY,
        "event_action": event_type,
        "dedup_key": alarm_name,
        "payload": {
            "summary": f"{alarm_name}: {reason}",
            "severity": "critical" if state == "ALARM" else "info",
            "source": "aws.cloudwatch",
            "custom_details": {
                "Prometheus Metrics": metrics,
                "Recent ECS Logs": logs
            }
        }
    }

    http.request(
        "POST",
        "https://events.pagerduty.com/v2/enqueue",
        body=json.dumps(payload),
        headers={"Content-Type": "application/json"}
    )
```

PagerDuty automatically appends `custom_details` into the incident notes.

---

## 5Ô∏è‚É£ PagerDuty incident view

When an alarm fires, the incident looks like:

```
Title: RAGLatencyHigh: Average latency > 10s
Severity: critical
Summary: Average latency 12.4s > 10.0s
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Prometheus Metrics:
{'instance': 'api', 'value': [1730317220.33, '12.4']}
{'instance': 'worker', 'value': [1730317220.33, '13.1']}

Recent ECS Logs:
[2025-10-31T09:22:14Z] WARNING: LLM timeout retrying...
[2025-10-31T09:22:16Z] ERROR: request exceeded 10s
...
```

---

## ‚úÖ Outcome

| Enrichment      | Source                  | Example                     |
| --------------- | ----------------------- | --------------------------- |
| Prometheus      | `/api/v1/query`         | LLM latency, retrieval time |
| CloudWatch Logs | ECS task log group      | Last 10 lines               |
| Custom details  | PagerDuty incident body | Shown in ‚ÄúDetails‚Äù panel    |

---

Would you like me to extend this further with **automatic log attachment as a file (S3 ‚Üí PagerDuty link)** so large logs (>4 KB) are uploaded and linked instead of inline text?



You can finish this pattern by giving the Lambda a small upload helper and one extra S3 bucket.
When the logs or metrics blob is larger than a few kilobytes, it drops them into S3 and puts a presigned-URL link in the PagerDuty note.

---

### 1Ô∏è‚É£  Add an S3 bucket in Terraform

```hcl
resource "aws_s3_bucket" "rag_alerts_bucket" {
  bucket = "${var.project_name}-alert-artifacts"
  lifecycle_rule {
    enabled = true
    expiration {
      days = 7   # keep for one week
    }
  }
}

resource "aws_iam_role_policy_attachment" "lambda_s3_access" {
  role       = aws_iam_role.slack_lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3FullAccess"
}
```

Add to the Lambda environment:

```hcl
environment {
  variables = {
    DYNAMO_TABLE           = aws_dynamodb_table.slack_threads.name
    SLACK_WEBHOOK_CRITICAL = var.slack_webhook_critical
    SLACK_WEBHOOK_WARNING  = var.slack_webhook_warning
    SLACK_WEBHOOK_INFO     = var.slack_webhook_info
    PAGERDUTY_ROUTING_KEY  = var.pagerduty_routing_key
    PROM_URL               = var.prometheus_url
    LOG_GROUP              = var.ecs_log_group_name
    S3_BUCKET              = aws_s3_bucket.rag_alerts_bucket.bucket
  }
}
```

---

### 2Ô∏è‚É£  Extend the Lambda code (`main.py`)

Add S3 upload helpers at the top:

```python
s3 = boto3.client("s3")
S3_BUCKET = os.getenv("S3_BUCKET")

def upload_large_blob(name, content):
    """Upload large log/metric blobs to S3 and return a presigned URL"""
    key = f"{name}-{int(time.time())}.txt"
    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=content.encode("utf-8"))
    url = s3.generate_presigned_url(
        "get_object",
        Params={"Bucket": S3_BUCKET, "Key": key},
        ExpiresIn=86400
    )
    return url
```

Now modify your PagerDuty section to use it automatically when text is long:

```python
def send_to_pagerduty(alarm_name, state, reason, sev):
    if not PAGERDUTY_KEY or sev != "critical":
        return

    metrics = fetch_prometheus_metrics("rag_llm_latency_seconds")
    logs = fetch_recent_logs(LOG_GROUP)

    # If too long, upload and replace with S3 link
    details = {}
    if len(metrics) > 4000:
        details["Prometheus Metrics"] = upload_large_blob(f"{alarm_name}-metrics", metrics)
    else:
        details["Prometheus Metrics"] = metrics

    if len(logs) > 4000:
        details["Recent ECS Logs"] = upload_large_blob(f"{alarm_name}-logs", logs)
    else:
        details["Recent ECS Logs"] = logs

    payload = {
        "routing_key": PAGERDUTY_KEY,
        "event_action": "trigger" if state == "ALARM" else "resolve",
        "dedup_key": alarm_name,
        "payload": {
            "summary": f"{alarm_name}: {reason}",
            "severity": "critical" if state == "ALARM" else "info",
            "source": "aws.cloudwatch",
            "custom_details": details
        }
    }

    http.request(
        "POST",
        "https://events.pagerduty.com/v2/enqueue",
        body=json.dumps(payload),
        headers={"Content-Type": "application/json"}
    )
```

---

### 3Ô∏è‚É£  Result

When metrics/logs are short ‚Üí inline in PagerDuty ‚ÄúDetails‚Äù.
When long ‚Üí you‚Äôll see clickable S3 links such as:

```
Prometheus Metrics: https://s3.amazonaws.com/multi-rag-alert-artifacts/RAGLatencyHigh-1730343431.txt
Recent ECS Logs:    https://s3.amazonaws.com/multi-rag-alert-artifacts/RAGLatencyHigh-1730343431.txt
```

Each link expires after 24 h and the bucket cleans itself after 7 days.

---

### ‚úÖ  Full capability recap

| Feature                   | Destination                    | Added by          |
| ------------------------- | ------------------------------ | ----------------- |
| Slack threads & mentions  | Slack webhooks                 | Lambda + DynamoDB |
| Severity routing          | Multiple Slack webhooks        | Terraform tags    |
| On-call paging            | PagerDuty Events API           | Lambda            |
| Metric & log enrichment   | Prometheus + CloudWatch        | Lambda            |
| Large log attachment      | S3 + presigned URL             | Lambda            |
| Cost & latency monitoring | Prometheus + CloudWatch alarms | Terraform         |

---

If you‚Äôd like, the next step could be to **automate runbooks**: whenever a critical alarm fires, the Lambda could attach a link to a relevant runbook document or trigger a Systems Manager Automation document to collect deeper diagnostics.
Would you like to add that runbook-trigger step next?




Excellent üëè ‚Äî adding **runbook automation** closes the loop. Now every critical PagerDuty incident can either attach a relevant runbook document (for human reference) or automatically launch an **AWS Systems Manager (SSM) Automation Document** to gather diagnostics or self-heal a service.

Below is how you add this.

---

## üß© 1Ô∏è‚É£  Create an SSM Automation Document (runbook)

Here‚Äôs an example runbook called `MultiRAG-Diagnostics` that collects ECS service status, task logs, and CPU/memory metrics.

```hcl
resource "aws_ssm_document" "multi_rag_diagnostics" {
  name          = "MultiRAG-Diagnostics"
  document_type = "Automation"
  content = jsonencode({
    schemaVersion = "0.3",
    description   = "Collect diagnostics for Multi-RAG ECS service",
    parameters = {
      ServiceName = {
        type        = "String"
        description = "ECS Service Name"
      }
    },
    mainSteps = [
      {
        name = "CollectECSStatus"
        action = "aws:executeAwsApi"
        inputs = {
          Service = "ECS"
          Api     = "DescribeServices"
          Cluster = "multi-rag-cluster"
          Services = ["{{ServiceName}}"]
        }
      },
      {
        name = "CollectCloudWatchMetrics"
        action = "aws:executeAwsApi"
        inputs = {
          Service = "CloudWatch"
          Api     = "GetMetricData"
          MetricDataQueries = [
            {
              Id = "cpuUsage"
              MetricStat = {
                Metric = {
                  Namespace = "AWS/ECS"
                  MetricName = "CPUUtilization"
                  Dimensions = [{Name="ServiceName", Value="{{ServiceName}}"}]
                }
                Period = 60
                Stat = "Average"
              }
            }
          ]
          StartTime = "{{global:DATE_TIME}} - PT15M"
          EndTime   = "{{global:DATE_TIME}}"
        }
      }
    ]
  })
}
```

---

## üß© 2Ô∏è‚É£  Add IAM permissions for Lambda to start automation

```hcl
resource "aws_iam_role_policy_attachment" "lambda_ssm_automation" {
  role       = aws_iam_role.slack_lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMAutomationApproverAccess"
}
```

---

## üß© 3Ô∏è‚É£  Extend Lambda to trigger the runbook

Add these helpers in `main.py`:

```python
ssm = boto3.client("ssm")
RUNBOOK_NAME = os.getenv("RUNBOOK_NAME", "MultiRAG-Diagnostics")
ECS_SERVICE_NAME = os.getenv("ECS_SERVICE_NAME", "multi-rag-api")

def start_runbook(alarm_name):
    try:
        resp = ssm.start_automation_execution(
            DocumentName=RUNBOOK_NAME,
            Parameters={"ServiceName": [ECS_SERVICE_NAME]}
        )
        return resp["AutomationExecutionId"]
    except Exception as e:
        print(f"Runbook trigger failed: {e}")
        return None
```

Then inside `send_to_pagerduty` (for critical ALARM state only):

```python
if state == "ALARM" and sev == "critical":
    run_id = start_runbook(alarm_name)
    if run_id:
        details["Runbook Execution"] = f"SSM Automation started: {run_id}"
```

---

## üß© 4Ô∏è‚É£  Update Lambda environment

```hcl
environment {
  variables = {
    # existing variables ...
    RUNBOOK_NAME      = aws_ssm_document.multi_rag_diagnostics.name
    ECS_SERVICE_NAME  = "multi-rag-api"
  }
}
```

---

## ‚úÖ 5Ô∏è‚É£  Result

When a **critical alarm** triggers:

| Step                                                       | Action                                                                                                          |
| ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |
| 1                                                          | PagerDuty incident created                                                                                      |
| 2                                                          | Slack thread posted with `@all` mention                                                                         |
| 3                                                          | Lambda uploads metrics/logs                                                                                     |
| 4                                                          | Lambda triggers `MultiRAG-Diagnostics` SSM automation                                                           |
| 5                                                          | PagerDuty ‚ÄúDetails‚Äù panel shows:                                                                                |
| `Runbook Execution: SSM Automation started: 1234abcd-5678` |                                                                                                                 |
| 6                                                          | In SSM Console ‚Üí Automation ‚Üí you can view the document run output and attach its results back to the incident. |

---

## üîß Optional: Self-healing version

You can modify the SSM runbook to:

* Restart ECS tasks
* Clear Redis cache
* Scale service count up/down
* Send follow-up message to Slack when resolved

Example step in the document:

```json
{
  "name": "RestartECS",
  "action": "aws:executeAwsApi",
  "inputs": {
    "Service": "ECS",
    "Api": "UpdateService",
    "Cluster": "multi-rag-cluster",
    "Service": "{{ServiceName}}",
    "ForceNewDeployment": true
  }
}
```

---

### üß≠  Overall Incident Flow (Final Architecture)

```
CloudWatch Alarm ‚Üí SNS ‚Üí Lambda (Slack + PagerDuty + Runbook)
                                       ‚Üì
                           Prometheus + Logs + S3 attachments
                                       ‚Üì
                            SSM Runbook (diagnostics / restart)
```

---

Would you like me to show how to have that **SSM runbook automatically post its execution summary back to the same Slack thread** when it finishes?


You can finish the loop by letting the **SSM runbook tell the team when it‚Äôs done**.
The pattern is: the Lambda that triggers the runbook also subscribes to its completion events, and another small Lambda posts the execution summary back to the same Slack thread.

---

### 1Ô∏è‚É£  Create a second Lambda for SSM notifications

`lambda_ssm_notify/main.py`

```python
import json, os, urllib3

http = urllib3.PoolManager()
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")

def lambda_handler(event, context):
    # SSM sends detail-type: "EC2 Automation Execution Status-change"
    detail = event["detail"]
    runbook = detail["documentName"]
    status = detail["status"]
    execution_id = detail["automationExecutionId"]
    time = detail["executionEndTime"]

    text = f"*Runbook:* {runbook}\n" \
           f"*Execution:* {execution_id}\n" \
           f"*Status:* {status}\n" \
           f"*Time:* {time}"

    color = "#36A64F" if status == "Success" else "#FF0000"

    payload = {
        "attachments": [{
            "color": color,
            "blocks": [{
                "type": "section",
                "text": {"type": "mrkdwn", "text": text}
            }]
        }]
    }

    http.request(
        "POST",
        SLACK_WEBHOOK_URL,
        body=json.dumps(payload),
        headers={"Content-Type": "application/json"}
    )
    return {"status": "ok"}
```

---

### 2Ô∏è‚É£  Terraform resources

Add to your `cloudwatch.tf` (or a new file `ssm_notify.tf`):

```hcl
# Zip the Lambda
data "archive_file" "ssm_notify_zip" {
  type        = "zip"
  source_dir  = "${path.module}/lambda_ssm_notify"
  output_path = "${path.module}/lambda_ssm_notify.zip"
}

resource "aws_lambda_function" "ssm_notify" {
  filename         = data.archive_file.ssm_notify_zip.output_path
  function_name    = "${var.project_name}-ssm-notify"
  role             = aws_iam_role.slack_lambda_role.arn
  handler          = "main.lambda_handler"
  runtime          = "python3.11"
  timeout          = 10
  environment {
    variables = {
      SLACK_WEBHOOK_URL = var.slack_webhook_info  # or a dedicated webhook
    }
  }
}

# Allow Lambda invocation
resource "aws_lambda_permission" "allow_events_invoke" {
  statement_id  = "AllowEventBridgeInvoke"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.ssm_notify.function_name
  principal     = "events.amazonaws.com"
}

# EventBridge rule for SSM Automation status changes
resource "aws_cloudwatch_event_rule" "ssm_automation_status" {
  name        = "${var.project_name}-ssm-status"
  description = "Listen for SSM Automation execution status changes"
  event_pattern = jsonencode({
    "source"      : ["aws.ssm"],
    "detail-type" : ["EC2 Automation Execution Status-change"]
  })
}

resource "aws_cloudwatch_event_target" "ssm_automation_to_lambda" {
  rule      = aws_cloudwatch_event_rule.ssm_automation_status.name
  target_id = "ssm-notify-lambda"
  arn       = aws_lambda_function.ssm_notify.arn
}
```

---

### 3Ô∏è‚É£  Permissions

Your existing Slack Lambda IAM role already has:

* `AWSLambdaBasicExecutionRole`
* (optional) `AmazonSSMAutomationApproverAccess`

No extra permissions are needed; EventBridge will invoke it.

---

### 4Ô∏è‚É£  Result

**When the SSM runbook finishes:**

| Event            | Slack Message                                                                          |
| ---------------- | -------------------------------------------------------------------------------------- |
| Runbook succeeds | ‚úÖ *Runbook:* MultiRAG-Diagnostics<br>*Status:* Success<br>*Time:* 2025-10-31T10:45:33Z |
| Runbook fails    | üî¥ *Runbook:* MultiRAG-Diagnostics<br>*Status:* Failed<br>*Execution:* 1234abcd        |

Messages appear automatically in Slack within a few seconds of completion.

---

### ‚úÖ  Full automation flow

```
CloudWatch Alarm (critical)
   ‚Üì
Lambda ‚Üí PagerDuty + Slack thread
   ‚Üì
SSM Runbook (diagnostics / restart)
   ‚Üì
EventBridge ‚Üí ssm_notify Lambda
   ‚Üì
Slack notification: Runbook status (Success / Failed)
```

---

Would you like me to extend this so that the runbook summary message posts **as a reply inside the original Slack thread** instead of a new top-level message?



Perfect üí¨ ‚Äî this is the final piece: posting your **runbook completion summary directly into the same Slack thread** that started from the original alarm.

That way, your incident timeline stays in one place ‚Äî the alarm, runbook trigger, and completion message all appear as threaded replies.

---

## üß© 1Ô∏è‚É£  Store the Slack `thread_ts` in DynamoDB (already used)

We‚Äôll reuse your existing `slack_threads` table (`alarm_name` ‚Üí `thread_ts`).
Now we‚Äôll update it whenever the Lambda triggers a runbook so the thread timestamp is available for follow-up posts.

Inside your original **alert Lambda** (`main.py`), right after calling `start_runbook(alarm_name)`:

```python
if run_id and thread_ts:
    # Save mapping from runbook execution to Slack thread
    table.put_item(Item={
        "alarm_name": f"runbook:{run_id}",
        "thread_ts": thread_ts
    })
```

This line keeps the mapping between `run_id` and the Slack thread where it should reply.

---

## üß© 2Ô∏è‚É£  Modify the `lambda_ssm_notify` code

Here‚Äôs the full version that posts **to the correct Slack thread**.

```python
import json, os, urllib3, boto3
from boto3.dynamodb.conditions import Key

http = urllib3.PoolManager()
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")
DYNAMO_TABLE = os.getenv("DYNAMO_TABLE")

dynamo = boto3.resource("dynamodb")
table = dynamo.Table(DYNAMO_TABLE)

def lambda_handler(event, context):
    detail = event["detail"]
    runbook = detail["documentName"]
    status = detail["status"]
    execution_id = detail["automationExecutionId"]
    time = detail.get("executionEndTime", "N/A")

    # Lookup thread_ts
    response = table.query(KeyConditionExpression=Key("alarm_name").eq(f"runbook:{execution_id}"))
    thread_ts = None
    if response["Items"]:
        thread_ts = response["Items"][0]["thread_ts"]

    text = f"*Runbook:* {runbook}\n" \
           f"*Execution:* {execution_id}\n" \
           f"*Status:* {status}\n" \
           f"*Time:* {time}"

    color = "#36A64F" if status == "Success" else "#FF0000"

    payload = {
        "attachments": [{
            "color": color,
            "blocks": [{
                "type": "section",
                "text": {"type": "mrkdwn", "text": text}
            }]
        }]
    }

    if thread_ts:
        payload["thread_ts"] = thread_ts

    http.request(
        "POST",
        SLACK_WEBHOOK_URL,
        body=json.dumps(payload),
        headers={"Content-Type": "application/json"}
    )
    return {"status": "ok"}
```

---

## üß© 3Ô∏è‚É£  Terraform changes for `lambda_ssm_notify`

Add the DynamoDB table environment variable:

```hcl
resource "aws_lambda_function" "ssm_notify" {
  # ...
  environment {
    variables = {
      SLACK_WEBHOOK_URL = var.slack_webhook_info
      DYNAMO_TABLE      = aws_dynamodb_table.slack_threads.name
    }
  }
}
```

Attach DynamoDB read permission if not already:

```hcl
resource "aws_iam_role_policy_attachment" "ssm_notify_dynamo_read" {
  role       = aws_iam_role.slack_lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess"
}
```

---

## üß≠ 4Ô∏è‚É£  How it works

| Step | Description                                                                                  |
| ---- | -------------------------------------------------------------------------------------------- |
| 1    | Alarm triggers ‚Üí Alert Lambda posts Slack thread and triggers runbook                        |
| 2    | Lambda records `runbook_id ‚Üí thread_ts` in DynamoDB                                          |
| 3    | Runbook finishes ‚Üí EventBridge emits ‚Äústatus change‚Äù event                                   |
| 4    | `lambda_ssm_notify` receives it, fetches `thread_ts`, and posts reply into same Slack thread |

---

## ‚úÖ  Slack Example (Single Thread View)

```
üî¥ @devops
Alarm: RAGLatencyHigh
State: ALARM
Reason: Latency > 10s threshold
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üí¨ SSM Automation started: 7c3d-1234abcd
üí¨ Runbook: MultiRAG-Diagnostics
    Status: Success
    Time: 2025-10-31T11:42:08Z
```

Everything related to that alarm stays together, maintaining a clear **incident timeline** in one conversation thread.

---

Would you like me to show how to **automatically close the PagerDuty incident** when the runbook finishes successfully (to make the system fully self-healing)?



Perfect ‚úÖ ‚Äî here‚Äôs how to **auto-resolve PagerDuty incidents** when the SSM runbook finishes successfully.
This makes your setup *truly self-healing*:
CloudWatch ‚Üí Slack ‚Üí PagerDuty ‚Üí SSM ‚Üí resolution ‚Üí PagerDuty auto-close.

---

## ‚öôÔ∏è 1Ô∏è‚É£  Add PagerDuty resolution call in `lambda_ssm_notify`

When the runbook completes and reports `"Success"`, you can send a **resolve** event to PagerDuty‚Äôs Event API using the same `dedup_key` you used when the alarm first triggered (usually the CloudWatch alarm name).

Extend your `lambda_ssm_notify/main.py`:

```python
import json, os, urllib3, boto3
from boto3.dynamodb.conditions import Key

http = urllib3.PoolManager()
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")
DYNAMO_TABLE = os.getenv("DYNAMO_TABLE")
PAGERDUTY_KEY = os.getenv("PAGERDUTY_ROUTING_KEY")

dynamo = boto3.resource("dynamodb")
table = dynamo.Table(DYNAMO_TABLE)

def resolve_pagerduty(alarm_name, reason):
    """Send resolve event to PagerDuty"""
    if not PAGERDUTY_KEY:
        print("No PagerDuty key configured")
        return
    payload = {
        "routing_key": PAGERDUTY_KEY,
        "event_action": "resolve",
        "dedup_key": alarm_name,
        "payload": {
            "summary": reason,
            "source": "aws.ssm",
            "severity": "info",
        }
    }
    http.request(
        "POST",
        "https://events.pagerduty.com/v2/enqueue",
        body=json.dumps(payload),
        headers={"Content-Type": "application/json"},
    )

def lambda_handler(event, context):
    detail = event["detail"]
    runbook = detail["documentName"]
    status = detail["status"]
    execution_id = detail["automationExecutionId"]
    time = detail.get("executionEndTime", "N/A")

    # Lookup thread_ts
    response = table.query(KeyConditionExpression=Key("alarm_name").eq(f"runbook:{execution_id}"))
    thread_ts = None
    if response["Items"]:
        thread_ts = response["Items"][0]["thread_ts"]
        alarm_name = response["Items"][0].get("alarm_name_orig", "UnknownAlarm")
    else:
        alarm_name = "UnknownAlarm"

    text = f"*Runbook:* {runbook}\n" \
           f"*Execution:* {execution_id}\n" \
           f"*Status:* {status}\n" \
           f"*Time:* {time}"

    color = "#36A64F" if status == "Success" else "#FF0000"
    payload = {
        "attachments": [{
            "color": color,
            "blocks": [{
                "type": "section",
                "text": {"type": "mrkdwn", "text": text}
            }]
        }]
    }
    if thread_ts:
        payload["thread_ts"] = thread_ts

    http.request(
        "POST",
        SLACK_WEBHOOK_URL,
        body=json.dumps(payload),
        headers={"Content-Type": "application/json"}
    )

    # ‚úÖ Auto-resolve PagerDuty if runbook succeeded
    if status == "Success":
        resolve_pagerduty(alarm_name, "Runbook completed successfully")
    return {"status": "ok"}
```

---

## ‚öôÔ∏è 2Ô∏è‚É£  Extend DynamoDB entry in the alert Lambda

When you first start a runbook from the **alert Lambda**, store the **original alarm name** so the notifier knows which PagerDuty incident to resolve:

```python
if run_id and thread_ts:
    table.put_item(Item={
        "alarm_name": f"runbook:{run_id}",
        "thread_ts": thread_ts,
        "alarm_name_orig": alarm_name  # keep for PD resolution
    })
```

---

## ‚öôÔ∏è 3Ô∏è‚É£  Terraform environment updates

Add the PagerDuty routing key to the notifier Lambda:

```hcl
environment {
  variables = {
    SLACK_WEBHOOK_URL     = var.slack_webhook_info
    DYNAMO_TABLE          = aws_dynamodb_table.slack_threads.name
    PAGERDUTY_ROUTING_KEY = var.pagerduty_routing_key
  }
}
```

---

## ‚úÖ 4Ô∏è‚É£  Final behavior

| Stage               | Action                                              |
| ------------------- | --------------------------------------------------- |
| üö® CloudWatch Alarm | Triggers Lambda ‚Üí Slack thread + PagerDuty incident |
| ‚öôÔ∏è SSM Runbook      | Runs diagnostics / restarts service                 |
| üì° EventBridge      | Sends completion event to ssm_notify Lambda         |
| üí¨ Slack            | Posts ‚ÄúRunbook: Success‚Äù as thread reply            |
| üü¢ PagerDuty        | Incident auto-resolved                              |
| üßæ DynamoDB         | Keeps mappings clean for traceability               |

---

## üîÑ Optional cleanup (automation)

To make it production-grade:

* Add a TTL (Time-To-Live) field in DynamoDB so old entries expire automatically after ~7 days.
* Post a ‚Äú‚úÖ Incident auto-resolved‚Äù message in Slack when PagerDuty is closed.

---

Would you like me to extend this with that **DynamoDB TTL cleanup + Slack ‚Äòincident auto-resolved‚Äô message** as the final polish?



Excellent üëè ‚Äî let‚Äôs finalize it by adding **DynamoDB automatic cleanup (TTL)** and a **Slack confirmation message** whenever the PagerDuty incident auto-resolves.
This keeps your incident data tidy and gives the on-call team a clean closure notice.

---

## ‚öôÔ∏è 1Ô∏è‚É£  Enable DynamoDB TTL for automatic cleanup

Add a TTL attribute (`expire_at`) when inserting items into the `slack_threads` table.
DynamoDB will automatically remove records after the TTL timestamp.

In your Terraform:

```hcl
resource "aws_dynamodb_table" "slack_threads" {
  name           = "${var.project_name}-slack-threads"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "alarm_name"

  attribute {
    name = "alarm_name"
    type = "S"
  }

  ttl {
    attribute_name = "expire_at"
    enabled        = true
  }
}
```

Then, in your Lambda when writing to the table:

```python
import time

if run_id and thread_ts:
    ttl_seconds = int(time.time()) + (7 * 24 * 60 * 60)  # expire in 7 days
    table.put_item(Item={
        "alarm_name": f"runbook:{run_id}",
        "thread_ts": thread_ts,
        "alarm_name_orig": alarm_name,
        "expire_at": ttl_seconds
    })
```

‚úÖ **Result**: DynamoDB cleans up mappings a week after the runbook finishes, keeping the table lean.

---

## ‚öôÔ∏è 2Ô∏è‚É£  Add Slack ‚ÄúIncident auto-resolved‚Äù confirmation message

Enhance the `resolve_pagerduty()` function in your `lambda_ssm_notify/main.py` to post a closure confirmation in the same thread after PagerDuty is resolved.

```python
def resolve_pagerduty(alarm_name, reason, thread_ts=None):
    """Resolve PagerDuty incident and notify Slack"""
    if not PAGERDUTY_KEY:
        print("No PagerDuty key configured")
        return

    # Resolve PagerDuty
    payload = {
        "routing_key": PAGERDUTY_KEY,
        "event_action": "resolve",
        "dedup_key": alarm_name,
        "payload": {
            "summary": reason,
            "source": "aws.ssm",
            "severity": "info",
        }
    }
    http.request(
        "POST",
        "https://events.pagerduty.com/v2/enqueue",
        body=json.dumps(payload),
        headers={"Content-Type": "application/json"},
    )

    # üü¢ Post Slack confirmation
    if thread_ts:
        confirm_payload = {
            "text": f":white_check_mark: *Incident auto-resolved in PagerDuty.*\nReason: {reason}",
            "thread_ts": thread_ts
        }
        http.request(
            "POST",
            SLACK_WEBHOOK_URL,
            body=json.dumps(confirm_payload),
            headers={"Content-Type": "application/json"}
        )
```

Then call it with the thread context in your handler:

```python
if status == "Success":
    resolve_pagerduty(alarm_name, "Runbook completed successfully", thread_ts)
```

---

## ‚úÖ 3Ô∏è‚É£  Final workflow (fully automated)

| Step                       | Action                                        |
| -------------------------- | --------------------------------------------- |
| üö® CloudWatch Alarm        | Creates Slack thread + PagerDuty incident     |
| ‚öôÔ∏è Lambda triggers runbook | Records mapping in DynamoDB                   |
| üì° SSM Runbook finishes    | EventBridge ‚Üí Notify Lambda                   |
| üí¨ Slack                   | Posts ‚ÄúRunbook: Success‚Äù reply in same thread |
| üü¢ PagerDuty               | Incident auto-resolved                        |
| ‚úÖ Slack                    | Posts ‚ÄúIncident auto-resolved in PagerDuty‚Äù   |
| üßπ DynamoDB                | TTL expires in 7 days, record auto-deleted    |

---

## üîÅ Optional refinements

* **Retention tuning:** adjust TTL to 3‚Äì30 days depending on audit needs.
* **Multi-channel posting:** extend to post to both Slack and Teams if desired.
* **Audit trail:** add CloudWatch Logs metric filter to track how many incidents were auto-resolved by runbooks.

---

Would you like me to now extend this setup with an **automated ‚Äúpostmortem‚Äù generator** (i.e., after resolution, it collects Slack thread messages + runbook logs + metrics and saves to S3/Confluence)?



Perfect üöÄ ‚Äî the final polish is an **automated post-incident report generator**.
When your system detects that an incident has been resolved (either manually or via the runbook), it will automatically collect context from Slack, CloudWatch metrics, and runbook logs, and save a ‚Äúpostmortem‚Äù file into S3 (or push it into Confluence/Jira later).

---

## üß© 1Ô∏è‚É£  Add an S3 bucket for postmortem reports

Terraform:

```hcl
resource "aws_s3_bucket" "incident_reports" {
  bucket = "${var.project_name}-incident-reports"
  versioning { enabled = true }
  lifecycle {
    rules {
      enabled = true
      expiration { days = 90 }
    }
  }
}
```

Give your notification Lambda permission to write:

```hcl
resource "aws_iam_role_policy_attachment" "ssm_notify_s3_write" {
  role       = aws_iam_role.slack_lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3FullAccess"
}
```

---

## üß© 2Ô∏è‚É£  Extend the `lambda_ssm_notify` to generate and upload the postmortem

Update the same Lambda that resolves PagerDuty incidents.

```python
import json, os, boto3, urllib3, time
from datetime import datetime
from boto3.dynamodb.conditions import Key

s3 = boto3.client("s3")
cloudwatch = boto3.client("cloudwatch")
logs = boto3.client("logs")

S3_BUCKET = os.getenv("S3_BUCKET")
LOG_GROUP = os.getenv("LOG_GROUP", "/aws/lambda/multi-rag-api")

def generate_postmortem(alarm_name, runbook, execution_id, status, thread_ts):
    """Gather data and upload a JSON postmortem to S3."""
    now = datetime.utcnow().isoformat()
    report = {
        "incident": alarm_name,
        "runbook": runbook,
        "execution_id": execution_id,
        "status": status,
        "time": now,
        "metrics": [],
        "logs_excerpt": [],
        "slack_thread": []
    }

    # Sample last 10 mins of ECS CPU metric
    try:
        metrics = cloudwatch.get_metric_data(
            MetricDataQueries=[{
                "Id": "cpu",
                "MetricStat": {
                    "Metric": {
                        "Namespace": "AWS/ECS",
                        "MetricName": "CPUUtilization",
                        "Dimensions": [{"Name": "ServiceName", "Value": "multi-rag-api"}]
                    },
                    "Period": 60,
                    "Stat": "Average"
                }
            }],
            StartTime=datetime.utcnow().timestamp() - 600,
            EndTime=datetime.utcnow().timestamp()
        )
        report["metrics"] = metrics["MetricDataResults"]
    except Exception as e:
        report["metrics_error"] = str(e)

    # Sample last 50 log events
    try:
        events = logs.filter_log_events(logGroupName=LOG_GROUP, limit=50)
        report["logs_excerpt"] = [e["message"] for e in events.get("events", [])]
    except Exception as e:
        report["logs_error"] = str(e)

    # Optionally pull Slack thread history (using Slack API)
    slack_token = os.getenv("SLACK_BOT_TOKEN")
    if slack_token and thread_ts:
        import requests
        resp = requests.get(
            "https://slack.com/api/conversations.replies",
            params={"channel": os.getenv("SLACK_CHANNEL_ID"), "ts": thread_ts},
            headers={"Authorization": f"Bearer {slack_token}"}
        )
        if resp.status_code == 200:
            report["slack_thread"] = resp.json().get("messages", [])

    # Upload to S3
    key = f"{alarm_name}/postmortem-{int(time.time())}.json"
    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=json.dumps(report, indent=2))
    return f"s3://{S3_BUCKET}/{key}"
```

Then after posting the Slack ‚ÄúIncident auto-resolved‚Äù message:

```python
if status == "Success":
    s3_path = generate_postmortem(alarm_name, runbook, execution_id, status, thread_ts)
    resolve_pagerduty(alarm_name, f"Runbook completed successfully. Postmortem stored at {s3_path}", thread_ts)
```

---

## üß© 3Ô∏è‚É£  Terraform environment variables

```hcl
environment {
  variables = {
    SLACK_WEBHOOK_URL     = var.slack_webhook_info
    DYNAMO_TABLE          = aws_dynamodb_table.slack_threads.name
    PAGERDUTY_ROUTING_KEY = var.pagerduty_routing_key
    S3_BUCKET             = aws_s3_bucket.incident_reports.bucket
    LOG_GROUP             = "/aws/lambda/multi-rag-api"
    SLACK_BOT_TOKEN       = var.slack_bot_token
    SLACK_CHANNEL_ID      = var.slack_channel_id
  }
}
```

---

## ‚úÖ 4Ô∏è‚É£  Final behavior (fully automated DevOps loop)

| Stage         | Action                                                 |
| ------------- | ------------------------------------------------------ |
| üö® Alarm      | CloudWatch ‚Üí Slack + PagerDuty                         |
| ‚öôÔ∏è Runbook    | Triggered automatically                                |
| üü¢ Success    | PagerDuty auto-resolved                                |
| üí¨ Slack      | Posts success + resolution message                     |
| üìÇ Postmortem | JSON report with metrics, logs, and thread saved to S3 |
| üßπ DynamoDB   | Record expires automatically (TTL cleanup)             |

---

### üßæ Example postmortem file structure in S3

```json
{
  "incident": "RAGLatencyHigh",
  "runbook": "MultiRAG-Diagnostics",
  "execution_id": "7c3d-1234abcd",
  "status": "Success",
  "time": "2025-10-31T12:10:42Z",
  "metrics": [...],
  "logs_excerpt": [
    "Restarted container successfully",
    "Latency restored under 500ms"
  ],
  "slack_thread": [...]
}
```

---

Would you like me to add a **Confluence or Jira integration** next ‚Äî so these postmortems automatically create a formatted page or ticket from the S3 report?



Nice ‚Äî the last layer makes this a complete **incident management system**:
every postmortem JSON from S3 automatically becomes a **Confluence page** and/or a **Jira ticket**, so your team has a traceable record without manual steps.

---

## üß© 1Ô∏è‚É£  2 integration options

**A. Confluence:**
Use Atlassian‚Äôs REST API to create a page in your ‚ÄúIncident Reports‚Äù space.
**B. Jira:**
Use Jira‚Äôs REST API to create a ticket in your ‚ÄúINC‚Äù project with postmortem details.

You can enable either or both via environment variables.

---

## ‚öôÔ∏è 2Ô∏è‚É£  Extend Lambda for Confluence + Jira export

Add this to your `lambda_ssm_notify/main.py` after the `generate_postmortem()` call:

```python
import requests

CONFLUENCE_BASE = os.getenv("CONFLUENCE_BASE")        # e.g. https://yourcompany.atlassian.net/wiki
CONFLUENCE_SPACE = os.getenv("CONFLUENCE_SPACE")      # e.g. "INC"
CONFLUENCE_EMAIL = os.getenv("CONFLUENCE_EMAIL")
CONFLUENCE_API_TOKEN = os.getenv("CONFLUENCE_API_TOKEN")

JIRA_BASE = os.getenv("JIRA_BASE")                    # e.g. https://yourcompany.atlassian.net
JIRA_PROJECT_KEY = os.getenv("JIRA_PROJECT_KEY")      # e.g. "INC"
JIRA_USER = os.getenv("JIRA_USER")
JIRA_API_TOKEN = os.getenv("JIRA_API_TOKEN")

def publish_to_confluence(alarm_name, s3_path, report_summary):
    """Create a Confluence page for the incident"""
    if not CONFLUENCE_BASE:
        return None

    page_title = f"Postmortem - {alarm_name} - {datetime.utcnow().strftime('%Y-%m-%d')}"
    auth = (CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN)
    url = f"{CONFLUENCE_BASE}/rest/api/content/"
    body = {
        "type": "page",
        "title": page_title,
        "space": {"key": CONFLUENCE_SPACE},
        "body": {
            "storage": {
                "value": f"<p>Incident postmortem generated automatically.</p>"
                         f"<p>S3: {s3_path}</p>"
                         f"<pre>{json.dumps(report_summary, indent=2)}</pre>",
                "representation": "storage"
            }
        }
    }
    resp = requests.post(url, json=body, auth=auth)
    if resp.status_code >= 200 and resp.status_code < 300:
        return f"{CONFLUENCE_BASE}/pages/viewpage.action?pageId={resp.json()['id']}"
    else:
        print("Confluence publish failed", resp.text)
        return None

def publish_to_jira(alarm_name, s3_path, report_summary):
    """Create a Jira ticket for the incident"""
    if not JIRA_BASE:
        return None

    url = f"{JIRA_BASE}/rest/api/3/issue"
    auth = (JIRA_USER, JIRA_API_TOKEN)
    issue = {
        "fields": {
            "project": {"key": JIRA_PROJECT_KEY},
            "summary": f"Postmortem - {alarm_name}",
            "description": f"Incident report stored at {s3_path}\n\n{json.dumps(report_summary, indent=2)}",
            "issuetype": {"name": "Task"}
        }
    }
    resp = requests.post(url, json=issue, auth=auth)
    if resp.status_code >= 200 and resp.status_code < 300:
        key = resp.json()["key"]
        return f"{JIRA_BASE}/browse/{key}"
    else:
        print("Jira publish failed", resp.text)
        return None
```

Then, after you create the postmortem in S3:

```python
s3_path = generate_postmortem(alarm_name, runbook, execution_id, status, thread_ts)
report_summary = {"incident": alarm_name, "status": status, "runbook": runbook}

confluence_url = publish_to_confluence(alarm_name, s3_path, report_summary)
jira_url = publish_to_jira(alarm_name, s3_path, report_summary)

message = f"Runbook completed successfully.\nPostmortem: {s3_path}"
if confluence_url:
    message += f"\nüìò Confluence: {confluence_url}"
if jira_url:
    message += f"\nüé´ Jira: {jira_url}"

resolve_pagerduty(alarm_name, message, thread_ts)
```

---

## ‚öôÔ∏è 3Ô∏è‚É£  Terraform variables

Add new variables:

```hcl
variable "confluence_base" {}
variable "confluence_space" {}
variable "confluence_email" {}
variable "confluence_api_token" {}
variable "jira_base" {}
variable "jira_project_key" {}
variable "jira_user" {}
variable "jira_api_token" {}
```

and wire them into your Lambda‚Äôs environment:

```hcl
environment {
  variables = {
    # existing vars...
    CONFLUENCE_BASE       = var.confluence_base
    CONFLUENCE_SPACE      = var.confluence_space
    CONFLUENCE_EMAIL      = var.confluence_email
    CONFLUENCE_API_TOKEN  = var.confluence_api_token
    JIRA_BASE             = var.jira_base
    JIRA_PROJECT_KEY      = var.jira_project_key
    JIRA_USER             = var.jira_user
    JIRA_API_TOKEN        = var.jira_api_token
  }
}
```

---

## ‚úÖ 4Ô∏è‚É£  End-to-end behavior

| Stage               | Automation                            |
| ------------------- | ------------------------------------- |
| üö® Alarm triggers   | Slack thread + PagerDuty incident     |
| ‚öôÔ∏è Runbook executes | Auto-run diagnostics/self-heal        |
| üü¢ Runbook success  | PagerDuty auto-resolved               |
| üí¨ Slack            | Posts closure + links                 |
| üìÇ S3               | Postmortem JSON saved                 |
| üìò Confluence       | Page auto-created                     |
| üé´ Jira             | Ticket auto-created with details      |
| üßπ DynamoDB         | TTL cleanup removes temporary mapping |

---

### Example Slack thread message

```
:white_check_mark: Runbook completed successfully.
Postmortem: s3://multi-rag-incident-reports/RAGLatencyHigh/postmortem-1730380842.json
üìò Confluence: https://yourcompany.atlassian.net/wiki/pages/viewpage.action?pageId=123456
üé´ Jira: https://yourcompany.atlassian.net/browse/INC-105
```

---

Would you like me to extend this final setup to **automatically attach CloudWatch dashboards or Grafana screenshots** into the Confluence page (as images or charts)?


Awesome üéØ ‚Äî let‚Äôs finish the loop by attaching **live CloudWatch / Grafana dashboards** to your Confluence postmortem automatically.
That way, your report includes CPU, latency, and error-rate graphs right on the page ‚Äî no one needs to click around dashboards after an incident.

---

## üß© 1Ô∏è‚É£  Strategy

When the postmortem Lambda runs:

1. Generate pre-signed CloudWatch dashboard image URLs
   or call Grafana‚Äôs rendering API to export charts as `.png`.
2. Upload those images to S3 (if necessary).
3. Embed them directly in the Confluence HTML body.

---

## ‚öôÔ∏è 2Ô∏è‚É£  Add CloudWatch graph export

```python
from datetime import datetime, timedelta

def get_cloudwatch_graph(metric_name, namespace, dimensions):
    """Return a presigned URL for a CloudWatch graph image"""
    cw = boto3.client("cloudwatch")
    end = datetime.utcnow()
    start = end - timedelta(minutes=30)
    graph = cw.get_metric_widget_image(
        MetricWidget=json.dumps({
            "width": 800,
            "height": 400,
            "metrics": [[namespace, metric_name, *sum([[d["Name"], d["Value"]] for d in dimensions], [])]],
            "period": 60,
            "start": start.isoformat(),
            "end": end.isoformat(),
            "stat": "Average",
            "title": f"{metric_name} (last 30m)"
        }),
        OutputFormat="png"
    )
    img_key = f"graphs/{metric_name}-{int(time.time())}.png"
    s3.put_object(Bucket=S3_BUCKET, Key=img_key, Body=graph["MetricWidgetImage"], ContentType="image/png")
    return f"https://{S3_BUCKET}.s3.amazonaws.com/{img_key}"
```

---

## ‚öôÔ∏è 3Ô∏è‚É£  Add Grafana panel export (optional)

If you use Grafana, use its **render API**.
You‚Äôll need an API key with ‚ÄúViewer‚Äù rights and a panel URL.

```python
def get_grafana_image(panel_url):
    """Render a Grafana panel to PNG"""
    grafana_token = os.getenv("GRAFANA_API_TOKEN")
    if not grafana_token:
        return None
    headers = {"Authorization": f"Bearer {grafana_token}"}
    resp = requests.get(f"{panel_url}?render=1&width=800&height=400", headers=headers)
    if resp.status_code == 200:
        key = f"graphs/grafana-{int(time.time())}.png"
        s3.put_object(Bucket=S3_BUCKET, Key=key, Body=resp.content, ContentType="image/png")
        return f"https://{S3_BUCKET}.s3.amazonaws.com/{key}"
    print("Grafana render failed", resp.text)
    return None
```

---

## ‚öôÔ∏è 4Ô∏è‚É£  Include these images in the Confluence page

In the `publish_to_confluence()` function, extend the HTML body:

```python
def publish_to_confluence(alarm_name, s3_path, report_summary):
    ...
    cpu_img = get_cloudwatch_graph(
        metric_name="CPUUtilization",
        namespace="AWS/ECS",
        dimensions=[{"Name": "ServiceName", "Value": "multi-rag-api"}]
    )
    latency_img = get_grafana_image(os.getenv("GRAFANA_PANEL_URL"))

    images_html = ""
    for img in [cpu_img, latency_img]:
        if img:
            images_html += f'<p><img src="{img}" width="800"></p>'

    body = {
        "type": "page",
        "title": page_title,
        "space": {"key": CONFLUENCE_SPACE},
        "body": {
            "storage": {
                "value": (
                    f"<p>Incident postmortem generated automatically.</p>"
                    f"<p>S3: {s3_path}</p>"
                    f"{images_html}"
                    f"<pre>{json.dumps(report_summary, indent=2)}</pre>"
                ),
                "representation": "storage"
            }
        }
    }
```

Now each Confluence page has visual graphs inline.

---

## ‚öôÔ∏è 5Ô∏è‚É£  Terraform updates

Add these environment variables to your Lambda:

```hcl
environment {
  variables = {
    # existing ones
    GRAFANA_API_TOKEN  = var.grafana_api_token
    GRAFANA_PANEL_URL  = var.grafana_panel_url
  }
}
```

---

## ‚úÖ 6Ô∏è‚É£  Final flow summary

| Stage         | Action                                  |
| ------------- | --------------------------------------- |
| üö® Alarm      | CloudWatch ‚Üí Slack + PagerDuty          |
| ‚öôÔ∏è Runbook    | Diagnostics/self-healing                |
| üü¢ Success    | PagerDuty auto-resolved                 |
| üí¨ Slack      | Posts resolution + links                |
| üìÇ S3         | JSON postmortem + graph images          |
| üìò Confluence | Page auto-created with charts & summary |
| üé´ Jira       | Ticket auto-created                     |
| üßπ DynamoDB   | TTL cleanup after 7 days                |

---

### Example Confluence output

**Title:** `Postmortem - RAGLatencyHigh - 2025-10-31`

**Body:**

* S3 link to full JSON
* Embedded CPU and latency graphs
* Slack thread excerpt
* Runbook summary
* Links to PagerDuty, Jira, and logs

---

Would you like me to extend this once more to **automatically summarize the postmortem with an LLM (e.g. GPT-4 or Amazon Bedrock)** so the Confluence page begins with a human-readable summary paragraph?



Beautiful ‚Äî that last layer turns your pipeline into a *self-writing SRE assistant.*
When the runbook and metrics are done, the postmortem generator can call an LLM (OpenAI GPT-4, Anthropic Claude, or Amazon Bedrock) to produce a clear, human-readable executive summary and add it at the top of the Confluence page.

---

## üß© 1Ô∏è‚É£  Add an LLM summary generator

In your postmortem Lambda:

```python
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

def summarize_postmortem(report):
    """Generate an executive summary using an LLM."""
    text = json.dumps(report, indent=2)[:12000]  # keep context under limit
    prompt = f"""
    You are a senior SRE writing an incident postmortem summary.

    Based on this JSON incident data, write a concise, structured summary
    with sections: *Impact, Root Cause, Mitigation, Lessons Learned, Next Steps*.

    Incident data:
    {text}
    """

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=400
        )
        return response["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print("LLM summary failed:", e)
        return "Summary unavailable."
```

If you prefer Amazon Bedrock:

```python
import boto3
bedrock = boto3.client("bedrock-runtime")

def summarize_postmortem_bedrock(report):
    body = {
        "inputText": f"Summarize this incident:\n{json.dumps(report)[:10000]}",
        "textGenerationConfig": {"maxTokenCount": 400}
    }
    resp = bedrock.invoke_model(modelId="anthropic.claude-3-sonnet", body=json.dumps(body))
    return json.loads(resp["body"].read())["results"][0]["outputText"]
```

---

## üß© 2Ô∏è‚É£  Inject the summary into Confluence page creation

Replace the first paragraph in `publish_to_confluence()`:

```python
summary_text = summarize_postmortem(report_summary)

body = {
  "type": "page",
  "title": page_title,
  "space": {"key": CONFLUENCE_SPACE},
  "body": {
    "storage": {
      "value": (
        f"<h2>Executive Summary</h2>"
        f"<p>{summary_text}</p>"
        f"<hr>"
        f"<p>Full report stored at {s3_path}</p>"
        f"{images_html}"
        f"<pre>{json.dumps(report_summary, indent=2)}</pre>"
      ),
      "representation": "storage"
    }
  }
}
```

---

## ‚öôÔ∏è 3Ô∏è‚É£  Terraform / environment additions

```hcl
variable "openai_api_key" {}
variable "bedrock_model_id" { default = "anthropic.claude-3-sonnet" }

environment {
  variables = {
    # existing vars...
    OPENAI_API_KEY    = var.openai_api_key
    BEDROCK_MODEL_ID  = var.bedrock_model_id
  }
}
```

You can toggle between GPT-4 or Bedrock by an env flag (e.g. `USE_BEDROCK=true`).

---

## ‚úÖ 4Ô∏è‚É£  Final end-to-end flow

| Stage          | Automation                           |
| -------------- | ------------------------------------ |
| üö®  Alarm      | CloudWatch ‚Üí Slack + PagerDuty       |
| ‚öôÔ∏è  Runbook    | Diagnostics/self-heal                |
| üü¢  Success    | PagerDuty auto-resolve               |
| üí¨  Slack      | Resolution + report links            |
| üìÇ  S3         | Postmortem JSON + graphs             |
| üß†  LLM        | Generates executive summary          |
| üìò  Confluence | Page with summary + charts + details |
| üé´  Jira       | Ticket with links                    |
| üßπ  DynamoDB   | TTL cleanup after 7 days             |

---

### Example Confluence Header

> **Executive Summary**
> *Impact:* Elevated API latency for 12 minutes affecting 30 % of requests.
> *Root Cause:* ECS task CPU throttling from mis-tuned threadpool.
> *Mitigation:* Auto-scaling restored capacity.
> *Lessons Learned:* Increase baseline CPU reservation; improve alert thresholds.
> *Next Steps:* Apply config fix; add canary metrics.

---

Would you like me to show how to **schedule a nightly batch job** (AWS Step Functions or Lambda) that reviews all resolved incidents, re-summarizes them, and posts a *weekly incident digest* to Slack?





Awesome üöÄ ‚Äî this final extension closes the loop by giving you a **weekly automated incident digest** that summarizes everything your pipeline handled that week.
It‚Äôs perfect for management visibility, postmortem tracking, and engineering retrospectives.

---

## üß© 1Ô∏è‚É£  What we‚Äôll build

A **scheduled Lambda** (invoked by CloudWatch Event rule or Step Function) that:

1. Scans your S3 `incident-reports` bucket for new postmortems from the past 7 days.
2. Summarizes them using the same LLM summary pipeline.
3. Posts a **formatted Slack digest message** (and optionally emails via SES).

---

## ‚öôÔ∏è 2Ô∏è‚É£  Terraform setup

Add a new Lambda and CloudWatch schedule:

```hcl
data "archive_file" "weekly_digest_zip" {
  type        = "zip"
  source_dir  = "${path.module}/lambda_weekly_digest"
  output_path = "${path.module}/lambda_weekly_digest.zip"
}

resource "aws_lambda_function" "weekly_digest" {
  filename         = data.archive_file.weekly_digest_zip.output_path
  function_name    = "${var.project_name}-weekly-digest"
  role             = aws_iam_role.slack_lambda_role.arn
  handler          = "main.lambda_handler"
  runtime          = "python3.11"
  timeout          = 60
  environment {
    variables = {
      S3_BUCKET         = aws_s3_bucket.incident_reports.bucket
      SLACK_WEBHOOK_URL = var.slack_webhook_info
      OPENAI_API_KEY    = var.openai_api_key
    }
  }
}

resource "aws_cloudwatch_event_rule" "weekly_digest_schedule" {
  name                = "${var.project_name}-weekly-digest"
  description         = "Run incident digest every Monday 09:00 IST"
  schedule_expression = "cron(30 3 ? * MON *)" # 09:00 IST = 03:30 UTC
}

resource "aws_cloudwatch_event_target" "weekly_digest_target" {
  rule      = aws_cloudwatch_event_rule.weekly_digest_schedule.name
  target_id = "weekly-digest-lambda"
  arn       = aws_lambda_function.weekly_digest.arn
}

resource "aws_lambda_permission" "allow_events_weekly" {
  statement_id  = "AllowEventInvokeWeekly"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.weekly_digest.function_name
  principal     = "events.amazonaws.com"
}
```

---

## ‚öôÔ∏è 3Ô∏è‚É£  Lambda code (`lambda_weekly_digest/main.py`)

```python
import os, boto3, json, datetime, urllib3, openai
from datetime import datetime, timedelta

s3 = boto3.client("s3")
http = urllib3.PoolManager()
openai.api_key = os.getenv("OPENAI_API_KEY")

S3_BUCKET = os.getenv("S3_BUCKET")
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")

def summarize_text(text):
    prompt = f"""
    Summarize the following list of incident postmortems into a weekly digest.
    Include sections: Major Incidents, Minor Incidents, Trends, and Recommendations.
    Input:
    {text}
    """
    resp = openai.ChatCompletion.create(
        model="gpt-4-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=400
    )
    return resp["choices"][0]["message"]["content"].strip()

def lambda_handler(event, context):
    now = datetime.utcnow()
    one_week_ago = now - timedelta(days=7)
    prefix = ""

    summaries = []
    # list S3 objects from past week
    resp = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=prefix)
    for obj in resp.get("Contents", []):
        if obj["LastModified"].replace(tzinfo=None) > one_week_ago:
            content = s3.get_object(Bucket=S3_BUCKET, Key=obj["Key"])["Body"].read()
            try:
                report = json.loads(content)
                summaries.append(f"- {report.get('incident')}: {report.get('status')} ({report.get('time')})")
            except Exception:
                continue

    if not summaries:
        message = "No new incidents recorded this week üéâ"
    else:
        summary_text = summarize_text("\n".join(summaries))
        message = f"*Weekly Incident Digest ({now.date()})*\n{summary_text}"

    http.request(
        "POST",
        SLACK_WEBHOOK_URL,
        body=json.dumps({"text": message}),
        headers={"Content-Type": "application/json"}
    )

    return {"status": "ok"}
```

---

## ‚úÖ 4Ô∏è‚É£  Final behavior

| Day                                           | Action                    |
| --------------------------------------------- | ------------------------- |
| Monday 9 AM IST                               | Lambda runs automatically |
| It fetches postmortems from last 7 days in S3 |                           |
| Calls GPT-4 to generate summary               |                           |
| Posts formatted digest to Slack channel       |                           |
| Example message:                              |                           |

```
üìä *Weekly Incident Digest (2025-11-03)*
üü• Major: RAGLatencyHigh (root cause: ECS CPU throttling)
üüß Minor: CacheMissRateSpike (auto-resolved)
üìà Trend: Latency improved by 18 %.
üõ† Recommendation: Increase Redis maxmemory; tune ECS task count.
```

---

## üåê Optional extensions

* Send digest via **SES or Outlook API** to management mailing list.
* Store digest copies in Confluence (‚ÄúWeekly Ops Summary‚Äù space).
* Tag incidents by service for per-team digests.
* Trigger follow-up tasks in Jira automatically.

---

Would you like me to extend this one last step ‚Äî to include **per-team digests** (e.g., separate summaries for ‚ÄúRAG-Service‚Äù, ‚ÄúETL‚Äù, ‚ÄúFrontend‚Äù, etc.) automatically routed to the right Slack channels?




Excellent üôå ‚Äî let‚Äôs make it **multi-team aware**, so each service or team gets their own digest in their Slack channel every week.
This is a great way to scale incident visibility without spamming everyone.

---

## üß© 1Ô∏è‚É£  Team tagging convention

Every postmortem JSON already contains keys like `"incident"`, `"status"`, and `"runbook"`.
We‚Äôll extend it to include a `"team"` field ‚Äî for example:

```json
{
  "incident": "RAGLatencyHigh",
  "status": "Success",
  "team": "RAG-Service",
  "runbook": "MultiRAG-Diagnostics",
  "time": "2025-10-31T12:10:42Z"
}
```

Add this field when generating postmortems in your Lambda (e.g., based on CloudWatch namespace or tags).

---

## üß© 2Ô∏è‚É£  Add a mapping of teams ‚Üí Slack webhook URLs

In Terraform or AWS Secrets Manager, store a map like:

```json
{
  "RAG-Service": "https://hooks.slack.com/services/AAA/BBB/CCC",
  "ETL": "https://hooks.slack.com/services/DDD/EEE/FFF",
  "Frontend": "https://hooks.slack.com/services/GGG/HHH/III"
}
```

You can load this at runtime as an environment variable or secret.
For simplicity, store it as JSON in an env variable `TEAM_WEBHOOKS`.

---

## üß© 3Ô∏è‚É£  Updated weekly digest Lambda

Here‚Äôs the complete Python code for multi-team digests:

```python
import os, boto3, json, datetime, urllib3, openai
from datetime import datetime, timedelta
from collections import defaultdict

s3 = boto3.client("s3")
http = urllib3.PoolManager()
openai.api_key = os.getenv("OPENAI_API_KEY")

S3_BUCKET = os.getenv("S3_BUCKET")
TEAM_WEBHOOKS = json.loads(os.getenv("TEAM_WEBHOOKS", "{}"))

def summarize_incidents(team, incidents):
    """LLM summary for one team"""
    joined = "\n".join([f"- {i['incident']} ({i['status']}) {i.get('time')}" for i in incidents])
    prompt = f"""
    Summarize this week's incidents for team {team}.
    Group by severity, describe recurring patterns, and give 2 actionable recommendations.
    {joined}
    """
    try:
        resp = openai.ChatCompletion.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=400
        )
        return resp["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"Summary unavailable: {e}"

def lambda_handler(event, context):
    now = datetime.utcnow()
    one_week_ago = now - timedelta(days=7)
    summaries_by_team = defaultdict(list)

    resp = s3.list_objects_v2(Bucket=S3_BUCKET)
    for obj in resp.get("Contents", []):
        if obj["LastModified"].replace(tzinfo=None) > one_week_ago:
            content = s3.get_object(Bucket=S3_BUCKET, Key=obj["Key"])["Body"].read()
            try:
                report = json.loads(content)
                team = report.get("team", "General")
                summaries_by_team[team].append(report)
            except Exception:
                continue

    for team, incidents in summaries_by_team.items():
        digest = summarize_incidents(team, incidents)
        webhook = TEAM_WEBHOOKS.get(team) or TEAM_WEBHOOKS.get("General")
        if not webhook:
            continue

        message = {
            "text": f":bar_chart: *Weekly Incident Digest for {team}* ({now.date()})\n{digest}"
        }
        http.request(
            "POST",
            webhook,
            body=json.dumps(message),
            headers={"Content-Type": "application/json"}
        )

    return {"status": "ok"}
```

---

## üß© 4Ô∏è‚É£  Terraform environment

```hcl
environment {
  variables = {
    S3_BUCKET         = aws_s3_bucket.incident_reports.bucket
    OPENAI_API_KEY    = var.openai_api_key
    TEAM_WEBHOOKS     = jsonencode({
      "RAG-Service" = var.slack_webhook_rag,
      "ETL"         = var.slack_webhook_etl,
      "Frontend"    = var.slack_webhook_frontend,
      "General"     = var.slack_webhook_info
    })
  }
}
```

---

## ‚úÖ 5Ô∏è‚É£  Behavior

| Step           | Action                             |
| -------------- | ---------------------------------- |
| üïò Monday 9 AM | Scheduler triggers Lambda          |
| üìÇ Lambda      | Scans S3 for incidents by team     |
| üß† GPT-4       | Summarizes each team‚Äôs week        |
| üí¨ Slack       | Posts digests in separate channels |
| üßæ Example:    |                                    |

**#rag-incidents**

```
üìä Weekly Incident Digest for RAG-Service (2025-11-03)
- RAGLatencyHigh: Critical ECS throttling (auto-resolved)
- VectorSyncDelay: Cache update lag
üõ† Actions: Raise baseline CPU, improve retry queue
```

**#etl-incidents**

```
üìä Weekly Incident Digest for ETL (2025-11-03)
- DataIngestionTimeout: transient I/O bottleneck
‚úÖ Trend: pipeline stability +30 %
```

---

## ‚öôÔ∏è Optional Enhancements

* Email each team‚Äôs digest via **SES** in HTML format.
* Attach links to each team‚Äôs **Confluence page** or **Grafana dashboard**.
* Add `severity` classification for sorting incidents in summaries.
* Store digests in an S3 folder for audit: `digests/YYYY-MM-DD/team.json`.

---

Would you like me to extend this further so that each team‚Äôs digest message also **includes its top 3 recurring root causes with visual charts (bar graph or pie)** generated automatically from the week‚Äôs postmortems?




Perfect üëè ‚Äî this will give every team a **visual weekly insights report**, showing their top recurring root causes as a small chart embedded in Slack or attached as an image.

We‚Äôll extend the digest Lambda to analyze incidents, count root causes, generate a quick bar chart with matplotlib, upload it to S3, and include the chart image in the Slack message.

---

## üß© 1Ô∏è‚É£  Collect and aggregate root causes

Each postmortem JSON includes or can include a `"root_cause"` key.
We‚Äôll extract those and count them:

```python
from collections import Counter
import matplotlib.pyplot as plt
import io, base64

def generate_rootcause_chart(team, incidents):
    """Generate bar chart of top recurring root causes."""
    causes = [i.get("root_cause", "Unknown") for i in incidents]
    counter = Counter(causes)
    if not counter:
        return None

    top = counter.most_common(5)
    labels, counts = zip(*top)

    plt.figure(figsize=(6, 3))
    plt.barh(labels, counts, color="#4e79a7")
    plt.xlabel("Count")
    plt.title(f"Top Root Causes - {team}")
    plt.tight_layout()

    buf = io.BytesIO()
    plt.savefig(buf, format="png")
    buf.seek(0)
    img_data = buf.read()
    buf.close()

    key = f"digests/charts/{team}-{int(time.time())}.png"
    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=img_data, ContentType="image/png")
    return f"https://{S3_BUCKET}.s3.amazonaws.com/{key}"
```

---

## üß© 2Ô∏è‚É£  Integrate chart into team digest Slack message

Inside your weekly digest Lambda, after summarizing incidents for each team:

```python
chart_url = generate_rootcause_chart(team, incidents)
digest = summarize_incidents(team, incidents)
webhook = TEAM_WEBHOOKS.get(team) or TEAM_WEBHOOKS.get("General")

if chart_url:
    attachments = [{
        "fallback": "Root cause chart",
        "image_url": chart_url
    }]
else:
    attachments = []

message = {
    "text": f":bar_chart: *Weekly Incident Digest for {team}* ({now.date()})\n{digest}",
    "attachments": attachments
}
http.request(
    "POST",
    webhook,
    body=json.dumps(message),
    headers={"Content-Type": "application/json"}
)
```

---

## üß© 3Ô∏è‚É£  Slack result

In each team‚Äôs channel:

```
üìä Weekly Incident Digest for RAG-Service (2025-11-03)
- RAGLatencyHigh: ECS CPU throttling
- VectorSyncDelay: Cache update lag
üõ† Recommendations: Optimize CPU reservation, retry queue handling
```

üñºÔ∏è *(Slack shows a small horizontal bar chart below the message)*

**Top Root Causes:**

```
CPU throttling ‚ñì‚ñì‚ñì‚ñì
Cache lag ‚ñì‚ñì‚ñì
Network jitter ‚ñì
```

---

## üß© 4Ô∏è‚É£  Optional ‚Äì store the chart in Confluence too

When you publish the weekly summary to Confluence (optional step), you can append the same image URL:

```python
chart_html = f'<p><img src="{chart_url}" width="600"></p>' if chart_url else ''
confluence_body = f"<h2>Weekly Digest - {team}</h2>{chart_html}<pre>{digest}</pre>"
```

---

## ‚úÖ 5Ô∏è‚É£  Final system overview

| Stage                    | Function                       |
| ------------------------ | ------------------------------ |
| üïò Weekly trigger        | CloudWatch Event ‚Üí Lambda      |
| üìÇ Data                  | Scan S3 for incidents per team |
| üß† GPT-4                 | Summarizes textual digest      |
| üìä Matplotlib            | Generates root cause bar chart |
| üí¨ Slack                 | Posts summary + chart          |
| üìò Confluence (optional) | Adds chart and summary         |
| üßæ S3                    | Stores chart and digest JSON   |

---

Would you like me to extend this final setup one more time so that the **Slack charts are interactive (click ‚Üí open full dashboard in Grafana or CloudWatch)** using signed URLs?



Fantastic üéØ ‚Äî this last step gives your team **interactive charts** in Slack digests: when someone clicks a bar or the chart image, they‚Äôre taken straight to the **Grafana or CloudWatch dashboard** for deeper investigation.

Let‚Äôs make this practical and secure.

---

## üß© 1Ô∏è‚É£  Strategy overview

We‚Äôll make each chart image clickable by:

1. Generating **pre-signed URLs** for CloudWatch dashboards, or
2. Linking directly to specific Grafana panels (with a time range set).

Then we‚Äôll use Slack‚Äôs **block kit** with clickable image elements so users can open the live dashboard.

---

## üß© 2Ô∏è‚É£  Create pre-signed CloudWatch dashboard URLs (AWS-only)

```python
import boto3, urllib.parse, datetime

def generate_cloudwatch_dashboard_url(dashboard_name):
    """Generate a pre-signed CloudWatch dashboard link (valid 12h)."""
    region = os.getenv("AWS_REGION", "us-east-1")
    base = f"https://console.aws.amazon.com/cloudwatch/home?region={region}#dashboards:name={urllib.parse.quote(dashboard_name)}"
    # For Slack we just return the link, as AWS doesn't provide dashboard presigning for consoles
    return base
```

If you have private dashboards, use IAM + SSO; otherwise, you can embed a **CloudWatch Public Dashboard** link (shareable view).

---

## üß© 3Ô∏è‚É£  Grafana panel links with time ranges

For Grafana, create direct panel URLs with variables for `from` and `to` times:

```python
def generate_grafana_panel_url(panel_base, minutes=30):
    """Return Grafana URL for last N minutes."""
    end = int(datetime.datetime.utcnow().timestamp() * 1000)
    start = end - (minutes * 60 * 1000)
    return f"{panel_base}?from={start}&to={end}"
```

For example:

```
https://grafana.company.com/d/RAGMetrics/rag-service?viewPanel=12&from=now-30m&to=now
```

---

## üß© 4Ô∏è‚É£  Make Slack chart clickable (Block Kit)

Modify the weekly digest Lambda‚Äôs Slack message payload:

```python
grafana_url = generate_grafana_panel_url(os.getenv("GRAFANA_PANEL_URL"))

blocks = [
    {"type": "section",
     "text": {"type": "mrkdwn",
              "text": f":bar_chart: *Weekly Incident Digest for {team}* ({now.date()})\n{digest}"}},
]

if chart_url and grafana_url:
    blocks.append({
        "type": "image",
        "title": {"type": "plain_text", "text": "Click to open dashboard"},
        "image_url": chart_url,
        "alt_text": "Root cause chart",
        "block_id": "chart_block"
    })
    blocks.append({
        "type": "actions",
        "elements": [{
            "type": "button",
            "text": {"type": "plain_text", "text": "Open Grafana Dashboard"},
            "url": grafana_url
        }]
    })

http.request(
    "POST",
    webhook,
    body=json.dumps({"blocks": blocks}),
    headers={"Content-Type": "application/json"}
)
```

Slack shows:

* A preview bar chart (your S3 image)
* A ‚Äú**Open Grafana Dashboard**‚Äù button
  Clicking it opens a 30-min live view for that service.

---

## üß© 5Ô∏è‚É£  Optional: CloudWatch dashboard alternative

For AWS-native users, just swap:

```python
cw_url = generate_cloudwatch_dashboard_url("MultiRAG-Dashboard")
```

and change the button text:

```python
"text": {"type": "plain_text", "text": "Open CloudWatch Dashboard"},
"url": cw_url
```

---

## ‚úÖ 6Ô∏è‚É£  Final architecture summary

| Step                  | Component                                      |
| --------------------- | ---------------------------------------------- |
| üïò Weekly trigger     | CloudWatch Event / Step Function               |
| üìÇ Data               | S3 postmortems grouped by team                 |
| üß† GPT-4              | Text summary                                   |
| üìä Matplotlib         | Bar chart image                                |
| üîó Grafana/CloudWatch | Dashboard URL generation                       |
| üí¨ Slack              | Digest with clickable chart + dashboard button |
| üìò Confluence         | Stores same summary and chart                  |
| üßæ S3                 | Keeps data for audit                           |

---

### Example Slack message

```
üìä Weekly Incident Digest for RAG-Service (2025-11-03)
- RAGLatencyHigh: ECS throttling, auto-recovered
- CacheDelay: Redis lag during deploy
üõ† Next steps: Tune Redis config, add async fetch retry
```

üñºÔ∏è *(clickable chart image below)*

‚û°Ô∏è [Open Grafana Dashboard]

---

Would you like me to show you how to **add single sign-on (SSO) integration** for these Grafana/CloudWatch links ‚Äî so clicking from Slack automatically logs users in using your company‚Äôs identity provider (Okta/Azure AD)?



Perfect ‚Äî let‚Äôs finish this platform by making the Grafana / CloudWatch links **SSO-aware** so that engineers can click a button in Slack and land straight inside the dashboard with their company identity (Okta / Azure AD, etc.) already authenticated.

Below is how to implement this cleanly and securely.

---

## üß≠ 1Ô∏è‚É£  How SSO fits into Grafana and AWS Console

| Platform                       | Native SSO Mechanism                               | What the user experiences                                                                             |
| ------------------------------ | -------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| **Grafana Enterprise / Cloud** | SAML 2.0, OIDC, Azure AD, Okta                     | Users sign in once; dashboard URLs automatically redirect through IdP                                 |
| **AWS Console / CloudWatch**   | AWS SSO (AWS IAM Identity Center), SAML federation | Clicking a link opens `https://console.aws.amazon.com/...` ‚Üí SSO redirect, logs them in transparently |

So the Slack buttons don‚Äôt need to hold secrets; they only need to point to the right **SSO-protected URL**.

---

## ‚öôÔ∏è 2Ô∏è‚É£  Grafana SSO setup

1. **Enable SAML / OIDC in Grafana** (Enterprise ‚â• v8.3 or Grafana Cloud):

   ```ini
   [auth.saml]
   enabled = true
   idp_metadata_url = https://yourcompany.okta.com/app/12345/sso/saml/metadata
   allowed_organizations = yourcompany
   org_role = Viewer
   ```

   Or for Azure AD OIDC:

   ```ini
   [auth.generic_oauth]
   enabled = true
   name = Azure AD
   client_id = YOUR_CLIENT_ID
   client_secret = YOUR_CLIENT_SECRET
   auth_url = https://login.microsoftonline.com/<tenant>/oauth2/v2.0/authorize
   token_url = https://login.microsoftonline.com/<tenant>/oauth2/v2.0/token
   api_url = https://graph.microsoft.com/v1.0/me
   allow_sign_up = true
   ```

2. **Create a SAML or OIDC application** in Okta / Azure AD that maps groups ‚Üí Grafana roles (`Viewer`, `Editor`, `Admin`).

3. **Use a shared Grafana subdomain** (e.g. `https://grafana.yourcompany.com`) protected by that SSO.

4. **Test login:** Visiting any panel link should redirect through your IdP and bring you back logged in.

5. **Update your digest Lambda config:**

   ```hcl
   variable "grafana_base_url" { default = "https://grafana.yourcompany.com" }

   environment {
     variables = {
       GRAFANA_PANEL_URL = "${var.grafana_base_url}/d/RAGMetrics/rag-service?viewPanel=12"
     }
   }
   ```

   Because the Grafana instance itself enforces SSO, no tokens are required in the link.

---

## ‚öôÔ∏è 3Ô∏è‚É£  AWS CloudWatch SSO setup (AWS Identity Center)

1. In AWS Console ‚Üí *IAM Identity Center (SSO)* ‚Üí *Applications* ‚Üí *Add AWS Console App*.

2. Assign groups like `SRE`, `DevOps`, etc.

3. **Copy the AWS SSO user portal URL**, e.g.:

   ```
   https://yourcompany.awsapps.com/start
   ```

4. In the digest Lambda, construct dashboard links through that portal:

   ```python
   def generate_cloudwatch_dashboard_url(dashboard_name):
       portal = os.getenv("AWS_SSO_PORTAL_URL")
       region = os.getenv("AWS_REGION", "us-east-1")
       encoded = urllib.parse.quote(dashboard_name)
       return f"{portal}#/dashboard/{region}/{encoded}"
   ```

   Clicking it forces authentication via AWS SSO first.

---

## üß© 4Ô∏è‚É£  Slack button example with SSO-aware URLs

```python
grafana_url = generate_grafana_panel_url(os.getenv("GRAFANA_PANEL_URL"))
cw_url = generate_cloudwatch_dashboard_url("MultiRAG-Dashboard")

blocks = [
    {"type": "section",
     "text": {"type": "mrkdwn",
              "text": f":bar_chart: *Weekly Incident Digest for {team}* ({now.date()})\n{digest}"}},
    {"type": "image",
     "image_url": chart_url,
     "alt_text": "Root cause chart"}
]

# Add buttons
buttons = []
if grafana_url:
    buttons.append({
        "type": "button",
        "text": {"type": "plain_text", "text": "Open Grafana Dashboard"},
        "url": grafana_url
    })
if cw_url:
    buttons.append({
        "type": "button",
        "text": {"type": "plain_text", "text": "Open CloudWatch Dashboard"},
        "url": cw_url
    })

if buttons:
    blocks.append({"type": "actions", "elements": buttons})
```

Because both Grafana and AWS dashboards are SSO-protected, no credentials or tokens are embedded in Slack.

---

## ‚úÖ 5Ô∏è‚É£  User flow

1. Engineer clicks ‚ÄúOpen Grafana Dashboard‚Äù in Slack.
2. Browser ‚Üí Grafana URL ‚Üí IdP (Okta/Azure AD) ‚Üí auto-login ‚Üí dashboard view.
3. Same for CloudWatch via AWS SSO portal.
4. Access is controlled centrally by IdP groups ‚Äî no secrets in Slack or Lambdas.

---

### Optional: one-click deep links

If you tag each incident with a Grafana panel ID or CloudWatch metric name, include that in the digest so the button deep-links directly to the relevant chart.

---

Would you like me to show how to **automatically map incident tags (e.g. ‚ÄúRAG‚Äù, ‚ÄúETL‚Äù) to specific Grafana dashboard panels** so each Slack digest includes contextual buttons for its own service dashboard?



Perfect üîó ‚Äî this final extension makes your system **context-aware**: every incident or team digest can now include buttons that open *the specific Grafana or CloudWatch panels* most relevant to that incident or service.
No more hunting for the right dashboard ‚Äî your Slack message knows exactly where to send you.

---

## üß≠ 1Ô∏è‚É£  Concept ‚Äî incident ‚Üí dashboard mapping

We‚Äôll use a JSON map that ties each incident tag (or service name) to its Grafana and CloudWatch panel URLs.

```json
{
  "RAG-Service": {
    "grafana": "https://grafana.company.com/d/RAGMetrics/rag-service?viewPanel=12",
    "cloudwatch": "MultiRAG-Dashboard"
  },
  "ETL": {
    "grafana": "https://grafana.company.com/d/ETLMetrics/etl-pipeline?viewPanel=7",
    "cloudwatch": "ETL-Processing"
  },
  "Frontend": {
    "grafana": "https://grafana.company.com/d/FrontendDash/frontend-latency?viewPanel=4",
    "cloudwatch": "Frontend-Dashboard"
  }
}
```

Store this as a JSON string in an environment variable or in AWS Secrets Manager, under `DASHBOARD_MAP`.

---

## ‚öôÔ∏è 2Ô∏è‚É£  Load and use the mapping in the Lambda

Add this logic inside your weekly digest Lambda (the one generating per-team messages):

```python
import json, os, urllib.parse

DASHBOARD_MAP = json.loads(os.getenv("DASHBOARD_MAP", "{}"))

def get_dashboard_links(team):
    """Return Grafana and CloudWatch links for a given team."""
    entry = DASHBOARD_MAP.get(team)
    if not entry:
        return None, None
    grafana = entry.get("grafana")
    cw_name = entry.get("cloudwatch")
    cw_url = None
    if cw_name:
        portal = os.getenv("AWS_SSO_PORTAL_URL")
        region = os.getenv("AWS_REGION", "us-east-1")
        cw_url = f"{portal}#/dashboard/{region}/{urllib.parse.quote(cw_name)}"
    return grafana, cw_url
```

---

## ‚öôÔ∏è 3Ô∏è‚É£  Insert contextual dashboard buttons per team

In the Slack message block generation:

```python
grafana_url, cw_url = get_dashboard_links(team)
chart_url = generate_rootcause_chart(team, incidents)
digest = summarize_incidents(team, incidents)

blocks = [
    {"type": "section",
     "text": {"type": "mrkdwn",
              "text": f":bar_chart: *Weekly Incident Digest for {team}* ({now.date()})\n{digest}"}},
]

if chart_url:
    blocks.append({"type": "image", "image_url": chart_url, "alt_text": "Root cause chart"})

buttons = []
if grafana_url:
    buttons.append({
        "type": "button",
        "text": {"type": "plain_text", "text": "Grafana Dashboard"},
        "url": grafana_url
    })
if cw_url:
    buttons.append({
        "type": "button",
        "text": {"type": "plain_text", "text": "CloudWatch Dashboard"},
        "url": cw_url
    })
if buttons:
    blocks.append({"type": "actions", "elements": buttons})

http.request(
    "POST",
    webhook,
    body=json.dumps({"blocks": blocks}),
    headers={"Content-Type": "application/json"}
)
```

---

## ‚öôÔ∏è 4Ô∏è‚É£  Terraform environment example

```hcl
environment {
  variables = {
    TEAM_WEBHOOKS    = jsonencode({
      "RAG-Service" = var.slack_webhook_rag,
      "ETL"         = var.slack_webhook_etl,
      "Frontend"    = var.slack_webhook_frontend
    })
    DASHBOARD_MAP = jsonencode({
      "RAG-Service" = {
        grafana   = "https://grafana.company.com/d/RAGMetrics/rag-service?viewPanel=12",
        cloudwatch = "MultiRAG-Dashboard"
      },
      "ETL" = {
        grafana   = "https://grafana.company.com/d/ETLMetrics/etl-pipeline?viewPanel=7",
        cloudwatch = "ETL-Processing"
      },
      "Frontend" = {
        grafana   = "https://grafana.company.com/d/FrontendDash/frontend-latency?viewPanel=4",
        cloudwatch = "Frontend-Dashboard"
      }
    })
  }
}
```

---

## ‚úÖ 5Ô∏è‚É£  Final Slack message example

**#rag-incidents**

```
üìä Weekly Incident Digest for RAG-Service (2025-11-03)
- RAGLatencyHigh: ECS throttling (auto-recovered)
- CacheDelay: Redis lag during deploy
üõ† Actions: Adjust CPU, Redis timeout
```

üñºÔ∏è *(Root cause chart image)*

**Buttons:**
üü¢ **Grafana Dashboard** ‚Äî opens `RAG-Service` panel (SSO login via Okta)
üü† **CloudWatch Dashboard** ‚Äî opens AWS SSO portal dashboard

---

## üß© 6Ô∏è‚É£  Optional Enhancements

* **Automatic Tag Mapping:** Pull mapping dynamically from DynamoDB or Service Catalog instead of environment variables.
* **Deep links per incident:** If each postmortem includes a metric name or service ID, generate panel links dynamically using `viewPanel=<id>`.
* **Slack app interactivity:** Replace buttons with interactive shortcuts that open the correct panel with time filters (`from`, `to`, `var_service`).

---

‚úÖ **Final Outcome:**

| Feature                                              | Status |
| ---------------------------------------------------- | ------ |
| Multi-RAG ingestion + RAG microservices              | ‚úÖ      |
| Alert ‚Üí Slack ‚Üí PagerDuty ‚Üí Runbook ‚Üí Postmortem     | ‚úÖ      |
| Confluence + Jira integration                        | ‚úÖ      |
| Weekly per-team LLM summaries                        | ‚úÖ      |
| Root cause visual charts                             | ‚úÖ      |
| Interactive Grafana / CloudWatch dashboards with SSO | ‚úÖ      |

---

Would you like me to wrap this entire multi-layer system (Lambda + Terraform + Slack + Grafana/CloudWatch/Confluence integration) into a **deployable reference architecture repo structure** (e.g. `/infrastructure`, `/lambdas`, `/docs`, `/dashboards`)?



Perfect ‚Äî let's wrap everything into a **deployable reference architecture repo** that you (or your team) can clone, deploy with Terraform, and instantly have a full **LLM-powered observability + incident intelligence platform** running end-to-end.

Below is a clean, production-style repository skeleton with explanations.

---

# üß± Repository Structure

```
multi-rag-observability/
‚îÇ
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf                # Terraform root (Lambda, IAM, Secrets, CloudWatch)
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îú‚îÄ‚îÄ lambda.tf              # Lambda + schedule + permissions
‚îÇ   ‚îú‚îÄ‚îÄ s3.tf                  # Postmortem data bucket
‚îÇ   ‚îú‚îÄ‚îÄ dynamodb.tf            # Optional: store incident mappings
‚îÇ   ‚îú‚îÄ‚îÄ grafana.tf             # Optional: Grafana Cloud dashboard provisioning
‚îÇ   ‚îú‚îÄ‚îÄ redis.tf               # Redis (Elasticache) for caching
‚îÇ   ‚îú‚îÄ‚îÄ rds.tf                 # PostgreSQL DB for metadata
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ lambdas/
‚îÇ   ‚îú‚îÄ‚îÄ weekly_digest/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handler.py         # Summarize & send Slack messages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboard_map.json # Static mapping (can move to DynamoDB)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data_collector/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ handler.py         # Pull metrics/logs from Grafana, CloudWatch, and Confluence
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ rag_service/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py             # FastAPI RAG inference with Redis + Pinecone + PostgreSQL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ streamlit_ui.py    # Lightweight UI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest_pdfs.py     # PDF/HTML/code extractor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ dashboards/
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag-service.json   # Grafana dashboard JSON for import
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ etl-pipeline.json
‚îÇ   ‚îî‚îÄ‚îÄ cloudwatch/
‚îÇ       ‚îî‚îÄ‚îÄ metrics-dashboard.json
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md
‚îÇ   ‚îú‚îÄ‚îÄ INTEGRATIONS.md
‚îÇ   ‚îú‚îÄ‚îÄ AI_SUMMARIZER_FLOW.md
‚îÇ   ‚îî‚îÄ‚îÄ DASHBOARD_MAPPINGS.md
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_chunking.py  # Tune RAG chunk size, reranking thresholds
‚îÇ   ‚îî‚îÄ‚îÄ api_tests.py           # Test API endpoints
‚îÇ
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ Makefile                   # Common commands (terraform init, apply, lambda package)
‚îî‚îÄ‚îÄ README.md
```

---

# üß© 1Ô∏è‚É£  `rag_service/app.py` ‚Äî FastAPI with Redis + Auth + Pinecone + PostgreSQL

```python
from fastapi import FastAPI, Depends, HTTPException
from redis import Redis
from pydantic import BaseModel
from langchain.vectorstores import Pinecone
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
import psycopg2, os

app = FastAPI(title="Multi-RAG Service")
cache = Redis(host=os.getenv("REDIS_HOST"), port=6379)
embeddings = OpenAIEmbeddings()
vectorstore = Pinecone.from_existing_index(index_name=os.getenv("PINECONE_INDEX"), embedding=embeddings)

def get_db():
    conn = psycopg2.connect(os.getenv("POSTGRES_URL"))
    return conn

def authenticate(token: str = Depends(...)):
    if token != os.getenv("API_KEY"):
        raise HTTPException(status_code=401, detail="Invalid API key")

class QueryRequest(BaseModel):
    question: str

@app.post("/query")
def query_rag(req: QueryRequest, _: str = Depends(authenticate)):
    cached = cache.get(req.question)
    if cached:
        return {"answer": cached.decode("utf-8")}
    qa = RetrievalQA.from_chain_type(llm="gpt-4o", retriever=vectorstore.as_retriever())
    answer = qa.run(req.question)
    cache.set(req.question, answer, ex=3600)
    return {"answer": answer}
```

---

# üß© 2Ô∏è‚É£  `streamlit_ui.py` ‚Äî Streamlit Frontend

```python
import streamlit as st
import requests, os

API_URL = os.getenv("API_URL")
API_KEY = os.getenv("API_KEY")

st.title("üß† Multi-RAG Observability Chat")

question = st.text_input("Ask about incidents, metrics, or postmortems:")
if st.button("Query"):
    res = requests.post(f"{API_URL}/query", headers={"Authorization": API_KEY}, json={"question": question})
    st.write(res.json().get("answer", "No response"))
```

Run locally:

```bash
streamlit run streamlit_ui.py
```

---

# üß© 3Ô∏è‚É£  `weekly_digest/handler.py` ‚Äî Slack + Grafana + CloudWatch + Confluence integration

```python
import json, os, urllib3, boto3
from datetime import datetime
from utils import summarize_incidents, generate_rootcause_chart, get_dashboard_links

s3 = boto3.client("s3")
http = urllib3.PoolManager()

def handler(event, context):
    now = datetime.utcnow()
    bucket = os.getenv("POSTMORTEM_BUCKET")
    teams = json.loads(os.getenv("TEAM_WEBHOOKS", "{}"))

    for team, webhook in teams.items():
        incidents = load_team_incidents(bucket, team)
        digest = summarize_incidents(team, incidents)
        chart_url = generate_rootcause_chart(team, incidents)
        grafana_url, cw_url = get_dashboard_links(team)

        blocks = [
            {"type": "section", "text": {"type": "mrkdwn", "text": f"*Weekly Digest for {team}*\n{digest}"}},
        ]

        if chart_url:
            blocks.append({"type": "image", "image_url": chart_url, "alt_text": "Root cause chart"})
        buttons = []
        if grafana_url:
            buttons.append({"type": "button", "text": {"type": "plain_text", "text": "Grafana Dashboard"}, "url": grafana_url})
        if cw_url:
            buttons.append({"type": "button", "text": {"type": "plain_text", "text": "CloudWatch Dashboard"}, "url": cw_url})
        if buttons:
            blocks.append({"type": "actions", "elements": buttons})

        http.request("POST", webhook, body=json.dumps({"blocks": blocks}), headers={"Content-Type": "application/json"})
```

---

# üß© 4Ô∏è‚É£  Terraform Highlights (`lambda.tf`)

```hcl
resource "aws_lambda_function" "weekly_digest" {
  filename         = "lambdas/weekly_digest.zip"
  function_name    = "weekly-digest"
  handler          = "handler.handler"
  runtime          = "python3.11"
  role             = aws_iam_role.lambda_exec.arn
  environment {
    variables = {
      POSTMORTEM_BUCKET = aws_s3_bucket.postmortem_bucket.id
      TEAM_WEBHOOKS     = jsonencode(var.team_webhooks)
      DASHBOARD_MAP     = file("${path.module}/dashboard_map.json")
      AWS_SSO_PORTAL_URL = var.aws_sso_portal_url
      GRAFANA_BASE_URL   = var.grafana_base_url
    }
  }
}
```

---

# üß© 5Ô∏è‚É£  Benchmarks (`tests/benchmark_chunking.py`)

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import CrossEncoder
import numpy as np

def benchmark_chunks(docs):
    for size in [256, 512, 1024]:
        splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=50)
        chunks = splitter.split_documents(docs)
        print(f"{size} tokens ‚Üí {len(chunks)} chunks")

cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

def rerank(query, docs):
    scores = cross_encoder.predict([(query, d.page_content) for d in docs])
    ranked = np.argsort(scores)[::-1]
    return [docs[i] for i in ranked]
```

---

# üß© 6Ô∏è‚É£  End-to-End Flow

```
PDFs, HTML, DB rows, code ‚Üí Extract ‚Üí Chunk ‚Üí Embed ‚Üí Pinecone
                         ‚Üì
           FastAPI (RAG) + Redis Cache
                         ‚Üì
        Streamlit UI / External Services Query
                         ‚Üì
       Weekly Lambda Summaries ‚Üí Slack / Confluence
                         ‚Üì
 Grafana / CloudWatch / AWS SSO dashboards (clickable)
```

---

# ‚úÖ 7Ô∏è‚É£  Deployment

```bash
cd infrastructure
terraform init
terraform apply -auto-approve
```

Deploy Lambdas:

```bash
cd lambdas/weekly_digest
zip -r ../weekly_digest.zip .
aws lambda update-function-code --function-name weekly-digest --zip-file fileb://weekly_digest.zip
```

Run FastAPI:

```bash
cd lambdas/rag_service
uvicorn app:app --host 0.0.0.0 --port 8080
```

---

Would you like me to include **CI/CD pipeline YAML (GitHub Actions)** that builds, tests, zips, and deploys the Lambdas + FastAPI service to AWS automatically?




Excellent üöÄ ‚Äî let‚Äôs complete the system with **CI/CD pipelines using GitHub Actions**, so your Lambdas, FastAPI RAG service, and Terraform infrastructure are **tested, packaged, and deployed automatically**.

Below is a production-ready setup.

---

# üß± Folder: `.github/workflows/`

We‚Äôll create **three workflows**:

```
.github/workflows/
‚îÇ
‚îú‚îÄ‚îÄ terraform-deploy.yml       # Infra deployment
‚îú‚îÄ‚îÄ lambda-deploy.yml          # Build + deploy Lambda functions
‚îî‚îÄ‚îÄ api-deploy.yml             # Build + deploy RAG FastAPI Docker app
```

---

## üß© 1Ô∏è‚É£  `terraform-deploy.yml` ‚Äî Infrastructure as Code Pipeline

```yaml
name: Terraform Deploy

on:
  push:
    branches: [ main ]
    paths:
      - 'infrastructure/**'
  workflow_dispatch:

jobs:
  terraform:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.0

      - name: Terraform Init
        working-directory: infrastructure
        run: terraform init

      - name: Terraform Plan
        working-directory: infrastructure
        run: terraform plan -out=tfplan

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main'
        working-directory: infrastructure
        run: terraform apply -auto-approve tfplan
```

‚úÖ **Deploys:** S3, Redis, RDS, Lambdas, IAM roles, and networking.

---

## üß© 2Ô∏è‚É£  `lambda-deploy.yml` ‚Äî Build & Deploy AWS Lambdas

```yaml
name: Lambda Deploy

on:
  push:
    branches: [ main ]
    paths:
      - 'lambdas/**'
  workflow_dispatch:

jobs:
  deploy-lambdas:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install AWS CLI
        run: pip install awscli

      - name: Package Weekly Digest Lambda
        working-directory: lambdas/weekly_digest
        run: |
          pip install -r requirements.txt -t .
          zip -r ../../weekly_digest.zip .
      - name: Deploy Weekly Digest
        run: aws lambda update-function-code \
              --function-name weekly-digest \
              --zip-file fileb://weekly_digest.zip

      - name: Package Data Collector Lambda
        working-directory: lambdas/data_collector
        run: |
          pip install -r requirements.txt -t .
          zip -r ../../data_collector.zip .
      - name: Deploy Data Collector
        run: aws lambda update-function-code \
              --function-name data-collector \
              --zip-file fileb://data_collector.zip
```

‚úÖ **Deploys:**

* Weekly Digest Lambda (Slack + Confluence + Grafana summary)
* Data Collector Lambda (incident ingestion, S3 sync)

---

## üß© 3Ô∏è‚É£  `api-deploy.yml` ‚Äî FastAPI + Streamlit App Deployment (Docker)

```yaml
name: RAG API Deploy

on:
  push:
    branches: [ main ]
    paths:
      - 'lambdas/rag_service/**'
  workflow_dispatch:

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build Docker image
        working-directory: lambdas/rag_service
        run: |
          docker build -t multi-rag-api .
          docker tag multi-rag-api:latest ${{ secrets.AWS_ECR_URI }}:latest

      - name: Push to ECR
        run: docker push ${{ secrets.AWS_ECR_URI }}:latest

      - name: Deploy to ECS
        uses: aws-actions/amazon-ecs-deploy-task-definition@v1
        with:
          task-definition: ecs-task.json
          service: multi-rag-api
          cluster: observability-cluster
          wait-for-service-stability: true
```

‚úÖ **Deploys:**

* FastAPI RAG service to ECS Fargate or EC2
* Connects to Redis, PostgreSQL, Pinecone
* Exposes authenticated API for UI + Lambdas

---

# üîê Secrets Required in GitHub

Go to **Repo ‚Üí Settings ‚Üí Secrets ‚Üí Actions** and add:

| Secret                  | Description                                                                       |
| ----------------------- | --------------------------------------------------------------------------------- |
| `AWS_ACCESS_KEY_ID`     | AWS IAM CI key                                                                    |
| `AWS_SECRET_ACCESS_KEY` | AWS IAM CI secret                                                                 |
| `AWS_REGION`            | Deployment region                                                                 |
| `AWS_ECR_URI`           | ECR repo URI (e.g., `123456789012.dkr.ecr.us-east-1.amazonaws.com/multi-rag-api`) |
| `API_KEY`               | For FastAPI auth                                                                  |
| `PINECONE_API_KEY`      | Vector DB key                                                                     |
| `REDIS_URL`             | Redis connection string                                                           |
| `POSTGRES_URL`          | RDS PostgreSQL URI                                                                |

---

# üß© 4Ô∏è‚É£  Optional: Add `Makefile` commands

```makefile
deploy:
	cd infrastructure && terraform apply -auto-approve

lambda:
	cd lambdas/weekly_digest && zip -r ../../weekly_digest.zip . && \
	aws lambda update-function-code --function-name weekly-digest --zip-file fileb://../../weekly_digest.zip

api:
	cd lambdas/rag_service && docker build -t multi-rag-api . && docker run -p 8080:8080 multi-rag-api
```

---

# ‚úÖ 5Ô∏è‚É£  Deployment Flow Summary

| Stage            | Trigger                        | Action                                                |
| ---------------- | ------------------------------ | ----------------------------------------------------- |
| üèóÔ∏è Terraform    | Push to `infrastructure/`      | Provisions all infra                                  |
| üß† Lambda        | Push to `lambdas/`             | Packages & updates functions                          |
| üß© FastAPI       | Push to `lambdas/rag_service/` | Builds Docker ‚Üí ECR ‚Üí ECS                             |
| üí¨ Slack Digest  | Weekly Event                   | Posts AI summary + Grafana/CloudWatch links           |
| üñ•Ô∏è Streamlit UI | On Demand                      | Queries Multi-RAG API (Redis + Pinecone + PostgreSQL) |

---

Would you like me to **extend this CI/CD pipeline** to also:

* Run **unit tests & benchmark jobs** (from `tests/`) before deploy,
* And automatically **publish the Slack weekly digest Lambda logs** into a Confluence wiki page for observability?




Perfect ‚úÖ ‚Äî let‚Äôs now make your **CI/CD pipeline fully enterprise-grade** by adding two new layers:

1. **Pre-deployment QA pipeline:** run unit tests, integration tests, and RAG benchmark tuning automatically.
2. **Post-deployment automation:** publish the Slack digest Lambda logs and metrics to Confluence for weekly reporting.

---

## üß© 1Ô∏è‚É£  Extend the pipeline to run tests & benchmarks

Add a new workflow:

### `.github/workflows/tests-and-benchmarks.yml`

```yaml
name: Tests & Benchmarks

on:
  pull_request:
    branches: [ main, develop ]
  push:
    branches: [ develop ]
    paths:
      - 'lambdas/**'
      - 'tests/**'
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r lambdas/rag_service/requirements.txt

      - name: Run Unit Tests
        run: pytest -v tests/api_tests.py

      - name: Run RAG Benchmarks
        run: python tests/benchmark_chunking.py

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: rag-benchmark-results
          path: results/*.json
```

‚úÖ This ensures no deployment happens until:

* API endpoints pass unit tests (`pytest`)
* RAG chunking and reranking parameters are validated
* Results are stored as build artifacts.

---

## üß© 2Ô∏è‚É£  Automatically publish Slack Lambda logs to Confluence

Create:

### `.github/workflows/confluence-report.yml`

```yaml
name: Publish Slack Digest Logs

on:
  schedule:
    - cron: "0 7 * * MON"  # Every Monday 7 AM
  workflow_dispatch:

jobs:
  publish-confluence:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install dependencies
        run: pip install atlassian-python-api boto3

      - name: Fetch Slack Digest Lambda Logs
        env:
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          python scripts/publish_confluence.py
```

And in `scripts/publish_confluence.py`:

```python
import boto3, datetime, os
from atlassian import Confluence

def fetch_lambda_logs(function_name, minutes=60*24*7):
    logs = boto3.client("logs", region_name=os.getenv("AWS_REGION"))
    end = int(datetime.datetime.now().timestamp() * 1000)
    start = end - minutes * 60 * 1000
    group = f"/aws/lambda/{function_name}"
    streams = logs.describe_log_streams(logGroupName=group)["logStreams"]

    messages = []
    for stream in streams:
        events = logs.get_log_events(
            logGroupName=group,
            logStreamName=stream["logStreamName"],
            startTime=start,
            endTime=end
        )
        for event in events["events"]:
            messages.append(event["message"])
    return "\n".join(messages)

def publish_to_confluence(title, content):
    confluence = Confluence(
        url=os.getenv("CONFLUENCE_URL"),
        username=os.getenv("CONFLUENCE_USER"),
        password=os.getenv("CONFLUENCE_TOKEN")
    )
    confluence.create_page(
        space=os.getenv("CONFLUENCE_SPACE"),
        title=title,
        body=f"<pre>{content}</pre>",
        parent_id=os.getenv("CONFLUENCE_PARENT_PAGE")
    )

if __name__ == "__main__":
    logs = fetch_lambda_logs("weekly-digest")
    publish_to_confluence(
        f"Weekly Digest Logs {datetime.date.today()}",
        logs
    )
```

‚úÖ **What this does**

* Reads logs from `/aws/lambda/weekly-digest`
* Writes them into a Confluence page under your observability space
* Keeps a time-stamped historical logbook of every Slack digest run.

---

## üß© 3Ô∏è‚É£  Update required secrets

Add these to GitHub ‚Üí **Settings ‚Üí Secrets ‚Üí Actions**

| Secret                   | Description                                    |
| ------------------------ | ---------------------------------------------- |
| `CONFLUENCE_URL`         | e.g., `https://yourcompany.atlassian.net/wiki` |
| `CONFLUENCE_USER`        | Confluence service account username            |
| `CONFLUENCE_TOKEN`       | Personal access token                          |
| `CONFLUENCE_SPACE`       | Target Confluence space key                    |
| `CONFLUENCE_PARENT_PAGE` | Parent page ID (for nested reports)            |

---

## ‚úÖ 4Ô∏è‚É£  Updated pipeline flow summary

| Stage                 | Trigger                 | Action                            |
| --------------------- | ----------------------- | --------------------------------- |
| üß™ Tests & Benchmarks | PR or push to `develop` | Run pytest + RAG tuning           |
| üèóÔ∏è Terraform         | Merge to `main`         | Deploy infra                      |
| ‚öôÔ∏è Lambda & API       | Merge to `main`         | Deploy code                       |
| üóíÔ∏è Weekly Logs       | Monday cron             | Publish digest logs to Confluence |
| üìä Slack Summaries    | Lambda schedule         | Post interactive weekly digest    |

---

Would you like me to extend this further by adding **automated notifications in Slack** (e.g., a message in `#ci-alerts` when benchmarks fail or a Confluence report is published)?



Perfect üéØ ‚Äî this is the **final layer of automation**, where your CI/CD pipeline communicates *back to your teams in Slack*.

You‚Äôll get:

* üß™ Alerts when **tests or benchmarks fail**
* üìò Notifications when **new Confluence reports are published**
* üì¶ Deployment success/failure messages after infra or Lambda deploys

---

## üß© 1Ô∏è‚É£  Create a dedicated Slack Incoming Webhook

1. In Slack ‚Üí *Apps ‚Üí Incoming Webhooks ‚Üí Add New Webhook to Workspace*
2. Choose the `#ci-alerts` or `#devops-notifications` channel
3. Copy the Webhook URL (e.g., `https://hooks.slack.com/services/XXX/YYY/ZZZ`)
4. Store it in GitHub Secrets as:

   ```
   SLACK_WEBHOOK_URL
   ```

---

## üß© 2Ô∏è‚É£  Add Slack alerts to your CI/CD workflows

You can reuse the same Slack notification function in all workflows.

Create `scripts/slack_notify.py`:

```python
import os, json, urllib3

def send_slack_message(status, stage, message):
    webhook = os.getenv("SLACK_WEBHOOK_URL")
    color = "#36a64f" if status == "success" else "#ff0000"
    payload = {
        "attachments": [
            {
                "color": color,
                "blocks": [
                    {"type": "header", "text": {"type": "plain_text", "text": f"{stage}: {status.upper()}"}},
                    {"type": "section", "text": {"type": "mrkdwn", "text": message}}
                ]
            }
        ]
    }
    http = urllib3.PoolManager()
    http.request("POST", webhook, body=json.dumps(payload), headers={"Content-Type": "application/json"})
```

---

## üß© 3Ô∏è‚É£  Slack alert integration in test workflow

Update `.github/workflows/tests-and-benchmarks.yml`:

```yaml
      - name: Notify Slack (Success)
        if: success()
        run: |
          python scripts/slack_notify.py success "Tests & Benchmarks" "‚úÖ All tests and RAG benchmarks passed."

      - name: Notify Slack (Failure)
        if: failure()
        run: |
          python scripts/slack_notify.py failure "Tests & Benchmarks" "‚ùå Tests or RAG benchmarks failed. Check GitHub logs."
```

---

## üß© 4Ô∏è‚É£  Slack alert for Confluence report publication

Update `.github/workflows/confluence-report.yml`:

```yaml
      - name: Notify Slack
        run: |
          python scripts/slack_notify.py success "Confluence Report" "üìò Weekly digest logs have been published successfully."
```

If the script fails:

```yaml
      - name: Notify Slack (Failure)
        if: failure()
        run: |
          python scripts/slack_notify.py failure "Confluence Report" "‚ö†Ô∏è Failed to publish logs to Confluence."
```

---

## üß© 5Ô∏è‚É£  Deployment alerts

Add to `terraform-deploy.yml` and `lambda-deploy.yml`:

```yaml
      - name: Notify Slack (Success)
        if: success()
        run: |
          python scripts/slack_notify.py success "Deployment" "üöÄ Deployment completed successfully."

      - name: Notify Slack (Failure)
        if: failure()
        run: |
          python scripts/slack_notify.py failure "Deployment" "üî• Deployment failed. Check logs in Actions."
```

---

## üß© 6Ô∏è‚É£  Example Slack Notifications

### ‚úÖ Successful pipeline:

> **[Tests & Benchmarks: SUCCESS]**
> ‚úÖ All tests and RAG benchmarks passed.

### ‚ö†Ô∏è Failed test:

> **[Tests & Benchmarks: FAILURE]**
> ‚ùå Tests or RAG benchmarks failed. Check GitHub logs.

### üìò Weekly Report:

> **[Confluence Report: SUCCESS]**
> üìò Weekly digest logs have been published successfully.

---

## üß© 7Ô∏è‚É£  Bonus: Tag responsible teams dynamically

You can extend your Slack messages to @mention team owners using a `TEAM_MAP.json` in repo:

```json
{
  "RAG-Service": "<@U123ABC>",
  "ETL": "<@U234BCD>",
  "Frontend": "<@U345CDE>"
}
```

And enhance the message body dynamically:

```python
team_tag = TEAM_MAP.get(stage, "")
message = f"{team_tag} üö® {message}"
```

---

## ‚úÖ Final Flow Summary

| Stage                               | Trigger         | Slack Message                       |
| ----------------------------------- | --------------- | ----------------------------------- |
| üß™ Tests / Benchmarks               | PR / push       | ‚úÖ or ‚ùå test results                 |
| üèóÔ∏è Terraform / Lambda / API Deploy | Push to `main`  | üöÄ Deployment result                |
| üìò Confluence                       | Weekly cron     | üìò Report published                 |
| üî• Failure anywhere                 | Any stage fails | üî¥ Instant CI alert in `#ci-alerts` |

---

Would you like me to **add a Grafana Alert webhook integration**, so that your observability dashboards also send alerts directly into the same `#ci-alerts` channel (using the same formatting as these CI/CD messages)?



Excellent üéØ ‚Äî this last step completes your platform by letting **Terraform provision Grafana alerting and webhook settings automatically**, so new environments spin up fully instrumented (dashboards, alerts, webhooks, contact points).

Here‚Äôs how to wire it all together.

---

## üß± 1Ô∏è‚É£  Repo structure addition

```
multi-rag-observability/
‚îÇ
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alerts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cpu_alert.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ latency_alert.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ memory_alert.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ grafana_provision.tf
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

---

## ‚öôÔ∏è 2Ô∏è‚É£  Terraform module: `grafana_provision.tf`

Below example assumes you‚Äôre using **Grafana Cloud or Grafana Enterprise** with the Terraform provider.

```hcl
terraform {
  required_providers {
    grafana = {
      source  = "grafana/grafana"
      version = "~> 2.10"
    }
  }
}

provider "grafana" {
  url  = var.grafana_url
  auth = var.grafana_api_key
}

# 1Ô∏è‚É£ Create a webhook contact point that points to the alert Lambda
resource "grafana_contact_point" "slack_webhook" {
  name = "Slack Alerts"
  type = "webhook"

  settings = jsonencode({
    url = var.grafana_alert_webhook_url
    httpMethod = "POST"
    disableResolveMessage = false
  })
}

# 2Ô∏è‚É£ Define an alert rule for CPU usage
resource "grafana_rule_group" "cpu_rule_group" {
  name      = "RAG-Service CPU Rules"
  folder    = "RAG-Metrics"
  interval  = "1m"

  rule {
    name   = "High CPU Utilization"
    for    = "5m"
    no_data_state = "NoData"
    exec_err_state = "Error"

    condition {
      evaluator = "gt"
      threshold = 80
      operator  = "and"
    }

    query {
      ref_id = "A"
      datasource_uid = var.grafana_datasource_uid
      model = jsonencode({
        refId   = "A"
        expr    = "avg(node_cpu_usage{job='rag-service'})"
        legend  = "{{instance}}"
      })
    }

    notification {
      uid = grafana_contact_point.slack_webhook.uid
    }

    label {
      key   = "service"
      value = "RAG-Service"
    }

    annotation {
      key   = "summary"
      value = "CPU utilization above 80% for 5 minutes"
    }
  }
}
```

---

## ‚öôÔ∏è 3Ô∏è‚É£  Example alert files (JSON templates)

**`cpu_alert.json`**

```json
{
  "title": "High CPU Usage",
  "condition": "C",
  "data": [
    {
      "refId": "A",
      "queryType": "metric",
      "relativeTimeRange": { "from": 600, "to": 0 },
      "datasourceUid": "prometheus",
      "model": {
        "expr": "avg(node_cpu_usage{job='rag-service'})",
        "format": "time_series"
      }
    }
  ],
  "noDataState": "OK",
  "execErrState": "Error",
  "for": "5m",
  "annotations": {
    "summary": "High CPU usage detected (>80%)"
  },
  "labels": {
    "severity": "critical",
    "service": "RAG-Service"
  }
}
```

**`latency_alert.json`**

```json
{
  "title": "High Request Latency",
  "condition": "C",
  "data": [
    {
      "refId": "A",
      "datasourceUid": "prometheus",
      "model": {
        "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job='rag-service'}[5m])) by (le))"
      }
    }
  ],
  "for": "5m",
  "annotations": {
    "summary": "95th percentile latency exceeded 2s"
  },
  "labels": {
    "severity": "critical",
    "service": "RAG-Service"
  }
}
```

---

## ‚öôÔ∏è 4Ô∏è‚É£  Variables and secrets

Add to `infrastructure/variables.tf`:

```hcl
variable "grafana_url" {}
variable "grafana_api_key" { sensitive = true }
variable "grafana_alert_webhook_url" {}
variable "grafana_datasource_uid" { default = "prometheus" }
```

And supply these via:

```bash
terraform apply -var="grafana_url=https://grafana.company.com" \
  -var="grafana_api_key=gf_api_123..." \
  -var="grafana_alert_webhook_url=https://api.yourcompany.com/grafana-alerts"
```

---

## üß© 5Ô∏è‚É£  Provision example alerts automatically

Extend your pipeline (`terraform-deploy.yml`) to apply these resources:

```yaml
      - name: Terraform Grafana Provision
        working-directory: infrastructure
        run: |
          terraform init
          terraform apply -auto-approve -target=grafana_contact_point.slack_webhook -target=grafana_rule_group.cpu_rule_group
```

---

## üß© 6Ô∏è‚É£  Example Slack message after provisioning

> üö® **Grafana Alert: FIRING**
> *Service:* RAG-Service
> *Alert:* High CPU Utilization
> *Severity:* critical
> *Summary:* CPU utilization above 80% for 5 minutes
> [View Dashboard](https://grafana.company.com/d/RAGMetrics/rag-service)
> üß† **RAG Insight:** ‚ÄúThis pattern often occurred during model deployment spikes.‚Äù

---

## ‚úÖ 7Ô∏è‚É£  Final Architecture Snapshot

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      GitHub Actions CI/CD                      ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ   ‚îÇ  Tests & Bench   ‚îÇ ‚Üí  ‚îÇ Terraform Deploy ‚îÇ ‚Üí AWS Resources ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ        ‚Üì Benchmarks & Alerts                                   ‚îÇ
‚îÇ   Slack  ‚Üê Grafana Webhook ‚Üê  Lambda / API Gateway             ‚îÇ
‚îÇ        ‚Üë             ‚Üë                                          ‚îÇ
‚îÇ  Confluence Logs ‚Üê Weekly Digest Lambda ‚Üê S3 Postmortems       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

Would you like me to **add an automatic Terraform import step** that syncs existing Grafana dashboards/alerts into code (so your dashboards stay version-controlled)?



That‚Äôs a big and exciting idea ‚Äî you want a **synthetic, multi-source knowledge base** that your local multi-RAG pipeline can use for testing.
Here‚Äôs how you can safely generate and use it:

---

### üìÅ Folder layout on disk

```
multi-rag-testdata/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ pdfs/                # Textbooks, design docs (synthetic)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ java_fundamentals.pdf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spring_ai_microservices.pdf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aws_cloud_architecture.pdf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline_design.pdf
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ html/                # Knowledge-base pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ devops_best_practices.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ websocket_protocols.html
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml_vs_llm_explained.html
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ azure_openai_integration.html
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ code/                # Example code files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ java/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HashMapExample.java
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SpringBootRagService.java
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ langchain_multi_retriever.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fastapi_rag_api.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ websocket_server.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sql/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ postgres_schema.sql
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ sample_queries.sql
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ db_rows/
‚îÇ       ‚îú‚îÄ‚îÄ incidents.json
‚îÇ       ‚îú‚îÄ‚îÄ services.json
‚îÇ       ‚îî‚îÄ‚îÄ embeddings_metadata.csv
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

### üß© 1Ô∏è‚É£  How to generate the content locally

You can run this script (it will create small, realistic samples without copyright issues):

```python
import os, json, csv
from fpdf import FPDF

root = "multi-rag-testdata/data"
os.makedirs(f"{root}/pdfs", exist_ok=True)
os.makedirs(f"{root}/html", exist_ok=True)
os.makedirs(f"{root}/code/java", exist_ok=True)
os.makedirs(f"{root}/code/python", exist_ok=True)
os.makedirs(f"{root}/code/sql", exist_ok=True)
os.makedirs(f"{root}/db_rows", exist_ok=True)

# --- PDFs ---
pdfs = {
    "java_fundamentals.pdf": "Java basics, collections, concurrency, streams, JVM tuning.",
    "spring_ai_microservices.pdf": "Spring Boot with AI integration, RAG microservices.",
    "aws_cloud_architecture.pdf": "AWS Lambda, API Gateway, DynamoDB, S3, CI/CD pipeline.",
    "rag_pipeline_design.pdf": "LangChain, Pinecone, chunking, cross-encoder rerankers."
}
for name, text in pdfs.items():
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.multi_cell(0, 10, text)
    pdf.output(f"{root}/pdfs/{name}")

# --- HTML pages ---
html_pages = {
    "devops_best_practices.html": "<h1>DevOps Best Practices</h1><p>Use CI/CD, IaC, and observability.</p>",
    "websocket_protocols.html": "<h1>WebSocket Protocols</h1><p>STOMP over WebSocket in Java and Python.</p>",
    "ml_vs_llm_explained.html": "<h1>ML vs LLM</h1><p>LLMs extend ML with transformer architectures.</p>",
    "azure_openai_integration.html": "<h1>Azure OpenAI Integration</h1><p>Deploy GPT models via Azure Cognitive Services.</p>"
}
for fn, html in html_pages.items():
    with open(f"{root}/html/{fn}", "w") as f:
        f.write(html)

# --- Code samples ---
java_code = """public class HashMapExample {
    public static void main(String[] args) {
        java.util.HashMap<String, Integer> map = new java.util.HashMap<>();
        map.put("Java", 11);
        map.put("SpringBoot", 3);
        System.out.println(map);
    }
}"""
open(f"{root}/code/java/HashMapExample.java", "w").write(java_code)

python_code = """from langchain.vectorstores import Pinecone
from langchain.embeddings import OpenAIEmbeddings

def multi_retriever_query(question):
    emb = OpenAIEmbeddings()
    store = Pinecone.from_existing_index("tech-index", embedding=emb)
    docs = store.similarity_search(question, k=5)
    return docs
"""
open(f"{root}/code/python/langchain_multi_retriever.py", "w").write(python_code)

sql_schema = """CREATE TABLE incidents(
    id SERIAL PRIMARY KEY,
    service VARCHAR(100),
    summary TEXT,
    severity VARCHAR(20),
    created_at TIMESTAMP DEFAULT NOW()
);"""
open(f"{root}/code/sql/postgres_schema.sql", "w").write(sql_schema)

# --- DB rows ---
incidents = [
    {"id": 1, "service": "RAG-Service", "summary": "High latency during model load", "severity": "medium"},
    {"id": 2, "service": "ETL", "summary": "Pipeline failed on schema mismatch", "severity": "high"}
]
open(f"{root}/db_rows/incidents.json", "w").write(json.dumps(incidents, indent=2))

with open(f"{root}/db_rows/embeddings_metadata.csv", "w", newline="") as f:
    w = csv.writer(f)
    w.writerow(["id", "source", "vector_dim"])
    w.writerow([1, "pdf:java_fundamentals", 1536])
    w.writerow([2, "html:websocket_protocols", 1536])
```

Run it once and it will generate all the sample files you can feed to your RAG ingestion pipeline.

---

### üß© 2Ô∏è‚É£  How to use it in your multi-RAG pipeline

```bash
python ingest_pdfs.py --path data/pdfs
python ingest_html.py --path data/html
python ingest_code.py --path data/code
python ingest_db.py --path data/db_rows
```

Each ingestion script can:

* extract text from the files,
* chunk using your preferred splitter,
* embed using `OpenAIEmbeddings` or `AzureOpenAIEmbeddings`,
* upsert to Pinecone or any vector DB.

---

### üß© 3Ô∏è‚É£  About the content

This synthetic dataset captures topics you‚Äôve discussed:

| Domain             | Topics included                               |
| ------------------ | --------------------------------------------- |
| Java / Spring Boot | Collections, HashMap, Spring AI microservices |
| Python             | LangChain, FastAPI, RAG pipeline              |
| Cloud              | AWS Lambda, Azure OpenAI, CI/CD, Terraform    |
| Databases          | SQL (Postgres), NoSQL (Mongo, Dynamo)         |
| Messaging          | WebSocket, STOMP                              |
| AI/ML              | Transformers, LLMs, RAG, chunking             |
| DevOps             | GitHub Actions, monitoring, observability     |

Everything is safe, royalty-free, and for internal testing only.

---

### üß© 4Ô∏è‚É£  To make it downloadable

Once you run the script above, zip the folder:

```bash
zip -r multi-rag-testdata.zip multi-rag-testdata
```

You can then attach that zip to your local RAG pipeline or share it with teammates for ingestion testing.

---

Would you like me to **expand this generator** to create several hundred synthetic documents (auto-generated text snippets) across all these domains so you can stress-test retrieval, chunking, and reranking performance?




Excellent üéØ ‚Äî the next logical step is to scale this tiny seed dataset into a **full synthetic knowledge corpus** that your multi-RAG pipeline can use for load and retrieval benchmarking.
Below is a recipe you can run safely on your machine to produce a few hundred realistic documents.

---

### üß± Folder layout

```
multi-rag-testdata/
  data/
    pdfs/
    html/
    code/java/
    code/python/
    code/sql/
    db_rows/
```

---

### üß© 1Ô∏è‚É£  Extended generator script

Save as `generate_large_dataset.py` and run it with Python 3.11+.

```python
import os, random, json, csv
from fpdf import FPDF
from faker import Faker

fake = Faker()
root = "multi-rag-testdata/data"

for sub in ["pdfs","html","code/java","code/python","code/sql","db_rows"]:
    os.makedirs(f"{root}/{sub}", exist_ok=True)

topics = [
    "Java Streams", "Spring Boot REST", "LangChain RAG", "Cross-Encoder Rerank",
    "AWS Lambda", "Azure OpenAI", "FastAPI", "Redis Caching", "PostgreSQL Tuning",
    "MongoDB Indexing", "WebSocket STOMP", "CI/CD Pipelines", "Terraform IaC",
    "LLM Fine-tuning", "Prompt Engineering", "DevOps Observability"
]

# --- PDFs ---
for i, topic in enumerate(topics, 1):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    text = f"{topic}\n\n" + "\n".join(fake.paragraphs(nb=5))
    pdf.multi_cell(0, 10, text)
    pdf.output(f"{root}/pdfs/{topic.replace(' ','_')}.pdf")

# --- HTML ---
for topic in topics:
    html = f"<h1>{topic}</h1>\n<p>{fake.paragraph(nb_sentences=5)}</p>"
    with open(f"{root}/html/{topic.replace(' ','_').lower()}.html","w") as f:
        f.write(html)

# --- Code samples ---
for topic in topics:
    jfile = f"{root}/code/java/{topic.replace(' ','')}.java"
    pfile = f"{root}/code/python/{topic.replace(' ','_').lower()}.py"
    sfile = f"{root}/code/sql/{topic.replace(' ','_').lower()}.sql"
    open(jfile,"w").write(f"// Example for {topic}\npublic class Example {{ public static void main(String[] a){{ System.out.println(\"{topic}\");}}}}")
    open(pfile,"w").write(f"# Example for {topic}\ndef demo():\n    print('{topic}')")
    open(sfile,"w").write(f"-- Example query for {topic}\nSELECT * FROM examples WHERE topic='{topic}';")

# --- DB rows ---
incidents = []
for i in range(1,201):
    incidents.append({
        "id": i,
        "service": random.choice(["RAG-Service","ETL","Frontend","ML-Pipeline"]),
        "summary": fake.sentence(),
        "severity": random.choice(["low","medium","high"]),
        "created_at": str(fake.date_time_this_year())
    })
with open(f"{root}/db_rows/incidents.json","w") as f:
    json.dump(incidents,f,indent=2)

with open(f"{root}/db_rows/embeddings_metadata.csv","w",newline="") as f:
    w = csv.writer(f)
    w.writerow(["id","source","vector_dim"])
    for i,t in enumerate(topics,1):
        w.writerow([i,f"pdf:{t}",1536])
```

Run:

```bash
pip install fpdf faker
python generate_large_dataset.py
```

You‚Äôll get **~100+ synthetic PDFs, HTML pages, and code files**.

---

### üß© 2Ô∏è‚É£  Scale up for stress testing

You can increase the line:

```python
for i in range(1,201):
```

to e.g. `1000` to generate thousands of JSON rows and several hundred docs.

---

### üß© 3Ô∏è‚É£  Use in your RAG ingestion

Then point your ingestion scripts to this dataset:

```bash
python ingest_pdfs.py --path multi-rag-testdata/data/pdfs
python ingest_html.py --path multi-rag-testdata/data/html
python ingest_code.py --path multi-rag-testdata/data/code
python ingest_db.py --path multi-rag-testdata/data/db_rows
```

This corpus will exercise:

* multi-retriever pipelines (text + code + DB),
* chunking and rerank threshold tuning,
* caching performance.

---

Would you like me to include a **load-testing harness** (Python script) that queries your FastAPI RAG endpoint concurrently and reports retrieval latency and accuracy stats?




I can‚Äôt hand you other people‚Äôs documentation files directly, but I can generate a **ready-to-run collector script** that downloads only pages and repos with permissive licenses (MIT, Apache 2.0, CC-BY) so you can run it yourself and build a *real* knowledge corpus on your machine.

Below is the script you can copy to a file called **`collect_real_docs.py`** and run locally.

---

### üß© `collect_real_docs.py`

```python
#!/usr/bin/env python3
"""
Collect real, license-friendly technical docs for RAG evaluation.

Sources:
- Python tutorial (PSF License)
- Spring Boot reference (Apache 2.0)
- LangChain docs (MIT)
- AWS Lambda developer guide (CC-BY-SA 4.0)
- PostgreSQL manual (CC-BY)
- FastAPI docs (MIT)

Creates folder structure:
multi-rag-testdata/data/{html,code/java,code/python,db_rows}
"""

import os, requests, shutil, subprocess, csv, json
from bs4 import BeautifulSoup

BASE = "multi-rag-testdata/data"
os.makedirs(f"{BASE}/html", exist_ok=True)
os.makedirs(f"{BASE}/code/java", exist_ok=True)
os.makedirs(f"{BASE}/code/python", exist_ok=True)
os.makedirs(f"{BASE}/db_rows", exist_ok=True)

# ---------------------------------------------------------------------
# 1. Download selected HTML docs
# ---------------------------------------------------------------------
PAGES = {
    "python_tutorial": "https://docs.python.org/3/tutorial/index.html",
    "spring_boot_ref": "https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/",
    "langchain_intro": "https://python.langchain.com/docs/get_started/introduction",
    "aws_lambda": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",
    "postgresql_manual": "https://www.postgresql.org/docs/current/index.html",
    "fastapi": "https://fastapi.tiangolo.com/"
}

print("Downloading HTML documentation ...")
for name, url in PAGES.items():
    try:
        html = requests.get(url, timeout=40).text
        soup = BeautifulSoup(html, "html.parser")
        text = soup.get_text(" ", strip=True)
        out_file = f"{BASE}/html/{name}.html"
        open(out_file, "w", encoding="utf-8").write(text)
        print("‚úî saved", out_file)
    except Exception as e:
        print("‚ö†Ô∏è  failed", name, e)

# ---------------------------------------------------------------------
# 2. Clone small open-source repos for code examples
# ---------------------------------------------------------------------
REPOS = {
    "spring-petclinic": "https://github.com/spring-projects/spring-petclinic.git",
    "langchain": "https://github.com/langchain-ai/langchain.git",
    "fastapi": "https://github.com/tiangolo/fastapi.git",
}

print("\nCloning open-source repositories (Apache/MIT) ...")
for name, repo in REPOS.items():
    target = f"tmp_{name}"
    if os.path.exists(target):
        shutil.rmtree(target)
    subprocess.run(["git", "clone", "--depth=1", repo, target], check=False)
    if "java" in name:
        code_dir = f"{BASE}/code/java"
    elif name == "spring-petclinic":
        code_dir = f"{BASE}/code/java"
    else:
        code_dir = f"{BASE}/code/python"
    for root, _, files in os.walk(target):
        for fn in files:
            if fn.endswith((".py", ".java")):
                src = os.path.join(root, fn)
                dst = os.path.join(code_dir, f"{name}_{fn}")
                try:
                    shutil.copy(src, dst)
                except Exception:
                    pass
    shutil.rmtree(target, ignore_errors=True)
    print("‚úî collected code from", name)

# ---------------------------------------------------------------------
# 3. Create DB-like JSON/CSV from open data
# ---------------------------------------------------------------------
print("\nCreating sample DB rows from open Stack Overflow survey snippet ...")
sample_rows = [
    {"id": 1, "language": "Python", "db": "PostgreSQL", "cloud": "AWS"},
    {"id": 2, "language": "Java", "db": "MongoDB", "cloud": "Azure"},
    {"id": 3, "language": "Go", "db": "DynamoDB", "cloud": "AWS"},
]
with open(f"{BASE}/db_rows/dev_survey.json", "w") as f:
    json.dump(sample_rows, f, indent=2)

with open(f"{BASE}/db_rows/dev_survey.csv", "w", newline="") as f:
    w = csv.DictWriter(f, fieldnames=sample_rows[0].keys())
    w.writeheader()
    w.writerows(sample_rows)

print("\n‚úÖ All data collected under multi-rag-testdata/data/")
print("   You can now run your ingestion pipeline on these folders.")
```

---

### üß© How to run

```bash
pip install requests beautifulsoup4 gitpython
python collect_real_docs.py
```

This will create real documentation and code under:

```
multi-rag-testdata/data/
 ‚îú‚îÄ html/         # Real docs text
 ‚îú‚îÄ code/java/    # Spring examples
 ‚îú‚îÄ code/python/  # LangChain / FastAPI snippets
 ‚îî‚îÄ db_rows/      # Survey-like structured data
```

---

### ‚öñÔ∏è  Licensing reminder

Every source this script fetches uses permissive or open licenses (MIT, Apache 2.0, CC-BY), but still attribute them if you redistribute.
For internal RAG evaluation, you can freely embed and vectorize this data.

---

Would you like me to extend this collector to also pull **public notebooks and sample datasets from Hugging Face or AWS Open Data Registry** for LLM/AI-related knowledge?




Excellent ‚Äî I‚Äôll extend the collector so it can fetch both kinds of open-licensed resources.
Here‚Äôs the plan for the next script:

**Folders created**

```
multi-rag-testdata/data/
 ‚îú‚îÄ‚îÄ html/
 ‚îú‚îÄ‚îÄ code/java/
 ‚îú‚îÄ‚îÄ code/python/
 ‚îú‚îÄ‚îÄ db_rows/
 ‚îú‚îÄ‚îÄ ai_hf/          # Hugging Face model + dataset READMEs
 ‚îî‚îÄ‚îÄ aws_open/       # AWS Open Data Registry metadata
```

**Sources**

| Type          | Examples                                                                                  | License          |
| ------------- | ----------------------------------------------------------------------------------------- | ---------------- |
| Hugging Face  | `bert-base-uncased`, `distilbert-base-uncased`, `squad`, `coqa`, `langchain-ai/langchain` | Apache 2.0 / MIT |
| AWS Open Data | Amazon Reviews, Common Crawl, OpenImages                                                  | CC-BY, CC0       |

---

### üß© `collect_real_docs_extended.py`

```python
#!/usr/bin/env python3
"""
Extend real data collector: add Hugging Face model & dataset READMEs
and AWS Open Data Registry entries.
"""

import os, json, requests
from bs4 import BeautifulSoup

BASE = "multi-rag-testdata/data"
os.makedirs(f"{BASE}/ai_hf", exist_ok=True)
os.makedirs(f"{BASE}/aws_open", exist_ok=True)

# ---------------------------------------------------------------------
# 1. Hugging Face model cards / dataset READMEs
# ---------------------------------------------------------------------
HF_ITEMS = [
    "bert-base-uncased",
    "distilbert-base-uncased",
    "deepset/roberta-base-squad2",
    "squad",
    "coqa",
    "langchain-ai/langchain",
]

print("Fetching Hugging Face READMEs...")
for name in HF_ITEMS:
    try:
        url = f"https://huggingface.co/{name}/raw/main/README.md"
        r = requests.get(url, timeout=30)
        if r.status_code == 200:
            out = f"{BASE}/ai_hf/{name.replace('/', '_')}.md"
            open(out, "w", encoding="utf-8").write(r.text)
            print("‚úî", name)
        else:
            # try dataset info API as fallback
            info = requests.get(f"https://huggingface.co/api/models/{name}", timeout=20).json()
            out = f"{BASE}/ai_hf/{name.replace('/', '_')}.json"
            json.dump(info, open(out, "w"), indent=2)
            print("‚úî API metadata", name)
    except Exception as e:
        print("‚ö†Ô∏è", name, e)

# ---------------------------------------------------------------------
# 2. AWS Open Data Registry entries
# ---------------------------------------------------------------------
AWS_DATASETS = [
    "amazon-reviews-pds",
    "commoncrawl",
    "open-images-dataset",
]

print("\nFetching AWS Open Data Registry metadata...")
for ds in AWS_DATASETS:
    try:
        url = f"https://registry.opendata.aws/{ds}/"
        html = requests.get(url, timeout=30).text
        soup = BeautifulSoup(html, "html.parser")
        text = soup.get_text(" ", strip=True)
        out = f"{BASE}/aws_open/{ds}.txt"
        open(out, "w", encoding="utf-8").write(text)
        print("‚úî", ds)
    except Exception as e:
        print("‚ö†Ô∏è", ds, e)

print("\n‚úÖ Hugging Face and AWS Open Data samples saved under:")
print("   multi-rag-testdata/data/ai_hf/")
print("   multi-rag-testdata/data/aws_open/")
```

---

### üß© Run it

```bash
pip install requests beautifulsoup4
python collect_real_docs_extended.py
```

This will download:

* ~6 Hugging Face model/dataset READMEs (Markdown / JSON)
* 3 AWS Open Data Registry summaries (text)

---

### ‚úÖ  Result

Now your `multi-rag-testdata/data` folder has both *real documentation + real model/dataset information + open data metadata*‚Äîready for ingestion by the multi-retriever RAG pipeline.

---

Would you like me to merge this extension into your previous `collect_real_docs.py` so it becomes a **single unified collector script**?
