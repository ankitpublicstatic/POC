To detect **which specific pod** is facing **high memory + high latency problems**, you must follow a **systematic, production-safe approach** used by SREs and cloud engineers.

Below is the exact real-world method used in Kubernetes clusters (GKE, EKS, AKS).

---

# ‚úÖ **STEP 1 ‚Äî Identify Which Pod Has Abnormal Resource Usage**

Run:

```
kubectl top pods -n <namespace>
```

Example output:

```
NAME               CPU(cores)   MEMORY(bytes)
order-7df4b89dd8   120m         400Mi
order-7df4b89dd8   110m         420Mi
order-7df4b89dd8   140m         3.2Gi   <-- PROBLEM
order-7df4b89dd8   90m          410Mi
```

üëâ The pod using **3.2 GB** is the unhealthy one.

### Indicators:

* Memory usage much higher than others
* CPU spiking abnormally
* Constantly growing memory
* Pod restarted (OOMKilled)

---

# ‚úÖ **STEP 2 ‚Äî Check Pod Events for OOMKilled**

```
kubectl describe pod <pod-name>
```

Look for:

```
Last State:     Terminated
Reason:         OOMKilled
```

If one pod repeatedly shows OOMKilled ‚Üí it is the faulty pod.

---

# ‚úÖ **STEP 3 ‚Äî Check Latency for Each Pod Individually**

If your app exposes actuator metrics:

```
/actuator/metrics/http.server.requests?tag=uri:/api/test
```

Or with Prometheus:

```
http_server_requests_seconds_max{pod="pod-name"}
```

Compare p95/p99 latency across pods:

```
kubectl exec <pod> -- curl localhost:8080/actuator/metrics/http.server.requests?tag=status:200
```

You will see which pod is slow.

---

# ‚úÖ **STEP 4 ‚Äî Check Logs Per Pod for Slow Responses**

```
kubectl logs <pod-name>
```

Look for:

* Slow SQL query logs
* Timeout errors
* High GC pause warnings
* Thread pool exhaustion logs

Example (bad):

```
WARN GC pause (Full GC) 2.5s
ERROR Timeout waiting for connection from pool
```

Only the bad pod will show repeated warnings.

---

# ‚úÖ **STEP 5 ‚Äî Check Request Count Per Pod**

If service is under load:

```
kubectl exec <pod> -- curl localhost:8080/actuator/metrics/http.server.requests?tag=outcome:SUCCESS
```

If one pod processes disproportionately more requests ‚Üí its queue grows ‚Üí memory rises ‚Üí latency rises.

---

# ‚úÖ **STEP 6 ‚Äî Check JVM Memory of Each Pod**

Using **jcmd**:

```
kubectl exec <pod> -- jcmd 1 GC.heap_info
```

Compare across all pods:

| Pod   | Old Gen Used | Young Gen | GC Pauses |
| ----- | ------------ | --------- | --------- |
| Pod-1 | 120 MB       | 60 MB     | OK        |
| Pod-2 | 150 MB       | 70 MB     | OK        |
| Pod-3 | 1.8 GB       | 100 MB    | ‚ùå BAD POD |
| Pod-4 | 140 MB       | 70 MB     | OK        |

---

# ‚úÖ **STEP 7 ‚Äî Compare GC Pause Times Per Pod**

Using:

```
kubectl exec <pod> -- jcmd 1 GC.run
```

Or check GC logs:

```
kubectl cp <pod>:/logs/gc.log gc-pod.log
```

Bad pod will show:

* long pauses (500ms‚Äì3s)
* high Full GC count
* old gen nearly full

---

# ‚úÖ **STEP 8 ‚Äî Check Thread Metrics Per Pod**

```
kubectl exec <pod> -- jstack 1 | grep BLOCKED -c
kubectl exec <pod> -- jstack 1 | grep WAITING -c
```

A bad pod usually has:

* many BLOCKED threads
* threads waiting for DB pool
* threads stuck on I/O

---

# ‚úÖ **STEP 9 ‚Äî Check HPA (Horizontal Pod Autoscaler) Pod Distribution**

```
kubectl get hpa
kubectl describe deploy <deployment>
```

If HPA sends more traffic to one pod (rare but possible):

* one pod overloaded
* memory spirals
* latency grows

---

# ‚úÖ **STEP 10 ‚Äî Identify Bad Pod Through Cluster Monitoring**

### **GCP**

Cloud Monitoring ‚Üí Kubernetes ‚Üí Workloads ‚Üí Pod-level metrics
Look for:

* Memory hotspot (red)
* High latency
* High GC pause time

### **AWS**

CloudWatch Container Insights ‚Üí EKS ‚Üí Pods

Bad pod will show:

* High memory graph
* High restart count
* High CPU stall

---

# üìå **HOW TO QUICKLY VISUALLY DETECT BAD PODS (PRODUCTION CHECKLIST)**

### Run:

```
kubectl get pods -o wide
kubectl top pods
kubectl describe pod
kubectl logs --tail=100 <pod>
```

### Look For:

| Type     | Symptom                        | Interpretation       |
| -------- | ------------------------------ | -------------------- |
| Memory   | One pod using 2‚Äì5x more memory | Memory leak          |
| CPU      | One pod using 1000m+           | GC thrashing         |
| Restarts | OOMKilled count                | Memory leak / bad GC |
| Logs     | Slow responses, GC warnings    | Thread/GC problem    |
| Latency  | High p99 latency               | Internal bottleneck  |
| Threads  | Many BLOCKED                   | DB pool bottleneck   |

---

# üèÜ **THE MOST ACCURATE WAY**

Use a **side-by-side comparison**:

```
kubectl exec <pod1> -- jcmd 1 VM.native_memory summary
kubectl exec <pod2> -- jcmd 1 VM.native_memory summary
kubectl exec <pod3> -- jcmd 1 VM.native_memory summary
```

The faulty pod will show:

* Excessive heap
* Excessive class memory
* Native memory leak
* Excessive thread usage

---

# üöÄ **FAST 20-SECOND METHOD Used by SREs**

Run:

```
kubectl top pods
kubectl describe pod <X>
kubectl logs <X> | grep -i warn
kubectl exec <X> -- jcmd 1 GC.heap_info
kubectl exec <X> -- jcmd 1 GC.class_histogram
```

Compare results.
The bad pod will **stand out immediately**.

---

# ‚≠ê If you want I can generate:

### ‚úîÔ∏è Kubernetes memory leak troubleshooting checklist

### ‚úîÔ∏è automated script to detect which pod is leaking

### ‚úîÔ∏è Grafana dashboard to detect bad pods

### ‚úîÔ∏è JVM telemetry dashboard (thread, GC, heap, latency)

### ‚úîÔ∏è production SOP (standard operating procedure) document

Which one should I prepare next?
