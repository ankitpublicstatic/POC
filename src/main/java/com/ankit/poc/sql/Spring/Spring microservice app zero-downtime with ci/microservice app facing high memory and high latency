Below is the **exact real-world, production-grade diagnosis flow** used by top SREs and principal engineers when **one microservice shows high memory + high latency**.

This is detailed and practical â€” no theory â€” *exact steps you will execute in production on AWS/GCP Kubernetes*.

---

# ğŸš¨ **Problem: One Microservice Shows High Memory + High Latency**

Symptoms:

* CPU normal or little high
* Memory continuously growing / spiking
* Latency increasing (p99, p95, p50)
* Throughput decreasing
* Error-rate increasing (timeouts, 5xx)

This indicates one or more of the following:

### **ğŸ”´ 1. Memory leak**

### **ğŸ”´ 2. GC pressure / long GC pauses**

### **ğŸ”´ 3. Thread starvation**

### **ğŸ”´ 4. DB latency slowing the service**

### **ğŸ”´ 5. Downstream dependency slowness**

### **ğŸ”´ 6. Temporary load spike**

Now letâ€™s walk through a **step-by-step Production Troubleshooting Flow** used in real enterprises.

---

# ğŸ§­ **STEP 1: Check Kubernetes Pod Metrics**

Use:

### **GKE**

```
kubectl top pod <pod-name>
kubectl top pod -n <namespace>
```

### **EKS**

```
kubectl top pods --containers
```

Look for:

* Memory usage steadily increasing? â†’ suspect memory leak.
* Near OOMKilled? â†’ GC tuning or memory leak.
* CPU normal but latency high â†’ GC pauses or thread starvation.

---

# ğŸ§­ **STEP 2: Check JVM Real-Time Metrics**

### Using `/actuator/metrics`

```
/actuator/metrics/jvm.memory.used
/actuator/metrics/jvm.gc.pause
/actuator/metrics/jvm.threads.live
/actuator/metrics/jvm.buffer.memory.used
```

### What to check:

| Symptom                               | Root Cause                                  |
| ------------------------------------- | ------------------------------------------- |
| High `jvm.gc.pause`                   | Long GC pause, too many old-gen collections |
| High `jvm.memory.used` in **old-gen** | Memory leak                                 |
| High threads                          | Thread pool exhaustion                      |
| High GC count                         | Too frequent minor GC (object churn)        |

---

# ğŸ§­ **STEP 3: Check GC Logs**

If you enabled GC logs in your container:

```
-XX:+PrintGC -Xlog:gc*:file=/logs/gc.log
```

Copy logs from pod:

```
kubectl cp <pod>:/logs/gc.log gc.log
```

Look for:

* Many **long pauses > 200ms** â†’ latency
* Many **Full GC (Old gen)** â†’ memory leak
* Young gen too small â†’ too frequent minor GC

---

# ğŸ§­ **STEP 4: Check JVM Heap Usage Inside Pod**

```
kubectl exec -it <pod> -- jcmd 1 GC.heap_info
kubectl exec -it <pod> -- jcmd 1 GC.class_stats
kubectl exec -it <pod> -- jcmd 1 VM.native_memory summary
```

### Key checks:

* Old Gen is full â†’ leak
* Class memory growing â†’ classloader leak
* Native memory high â†’ system library leak (Netty, SSL, etc.)

---

# ğŸ§­ **STEP 5: Capturing a Heap Dump (Only when Needed)**

In production, you do NOT heap dump during peak hours.

To capture:

```
kubectl exec <pod> -- jmap -dump:live,format=b,file=/dumps/heap.hprof 1
kubectl cp <pod>:/dumps/heap.hprof heap.hprof
```

Analyze with:

* Eclipse Memory Analyzer (MAT)
* VisualVM
* JProfiler

Look for:

* **Dominators** (objects retaining memory)
* **Leak suspects** (collections with millions of objects)
* **Unclosed resources**
* **HTTP clients not shutting connections**

---

# ğŸ§­ **STEP 6: Check Thread Dumps**

To check latency, thread starvation:

```
kubectl exec <pod> -- jstack 1 > thread-dump.txt
```

Look for:

* Blocked threads
* Locked monitors
* Threads in WAITING
* WebClient / RestTemplate stuck in I/O
* JDBC pool exhausted

---

# ğŸ§­ **STEP 7: Check Upstream Latency**

Using Google Cloud Trace / AWS X-Ray / Jaeger

Look for:

* DB calls taking long?
* Redis latency rising?
* Another service slow?
* Network issues?

If latency is due to downstream, memory may increase due to:

* request queue buildup
* thread pool saturation

---

# ğŸ§­ **STEP 8: Check Pod Scaling / HPA**

If traffic spike â†’ latency grows â†’ memory grows.

Check pod count:

```
kubectl get hpa
```

If HPA not scaling â†’ fix the scaling metric.

Always use:

* **CPU**
* **Memory**
* **RPS**
* **Latency**

---

# ğŸ§­ **STEP 9: Check Container Limits**

A VERY COMMON PROBLEM:

### **Limit too low â†’ OOMKilled**

### **Limit too high â†’ GC struggling**

Example:

```
requests:
  memory: 512Mi
limits:
  memory: 1Gi
```

If actual usage is > 1Gi â†’ app killed
If you set 4Gi RAM â†’ GC takes long time â†’ latency

**Tune the limits based on real memory footprint.**

---

# ğŸ§­ **STEP 10: Check DB & Connection Pool**

Verify:

### **HikariCP metrics**

```
/actuator/metrics/hikaricp.connections.active
/actuator/metrics/hikaricp.connections.idle
```

If:

* Active > max â†’ bottleneck
* Idle = 0 â†’ starvation
* Timeout error â†’ latency

Memory & latency grow due to thread/wait queue.

---

# ğŸ§­ **STEP 11: Check High Object Allocation**

Using:

```
kubectl exec <pod> -- jcmd 1 GC.class_histogram
```

Look for:

* Many small objects â†’ excessive object churn
* Large lists/maps â†’ memory leak
* Byte[] (images, pdfs, etc.) â†’ not freed
* Strings â†’ poor caching or regex
* CompletableFuture not clearing

---

# ğŸ§­ **STEP 12: Check CPU & GC Tuning**

Enable G1GC:

```
-XX:+UseG1GC
-XX:MaxGCPauseMillis=200
```

Common tuning:

* Increase young gen
* Reduce old gen
* Use MaxRAMPercentage correctly

---

# ğŸ§­ **STEP 13: Check Networking Issues**

High latency often due to:

* DNS lookup slow
* TLS handshake slow
* Outbound network delays
* Slow calls to external APIs

Use:

**GCP** â†’ VPC Flow Logs
**AWS** â†’ VPC Flow Logs / ENI metrics

---

# ğŸ§­ **STEP 14: Identify if Only One Pod Misbehaving**

Compare memory:

```
kubectl top pods
```

If:

* Pod-3 is taking 2 GB
* Pod-1,2,4 taking 500 MB

â†’ likely memory leak, not traffic.

Delete bad pod â†’ brings temporary stability:

```
kubectl delete pod <pod>
```

But root cause must be fixed.

---

# ğŸ§­ **STEP 15: Verify Logs**

Look for:

* â€œOutOfMemoryErrorâ€
* â€œGC Overhead Limit Exceededâ€
* â€œConnection pool timeoutâ€
* â€œThread blocked â€¦â€
* 429 / 408 / 504 errors

---

# ğŸ§  FINAL ROOT CAUSE DECISION TABLE

| Symptom                               | Root Cause                           |
| ------------------------------------- | ------------------------------------ |
| Memory slowly increasing              | Memory leak                          |
| Memory increasing only during traffic | Under-provisioned horizontal scaling |
| Latency increasing + GC pauses high   | GC tuning required                   |
| Latency increasing + threads blocked  | DB bottleneck / deadlock             |
| One pod consuming too much memory     | Local leak                           |
| All pods consuming memory             | Global leak / load spike             |
| High CPU, high latency                | Serialization/deserialization issue  |

---

# ğŸ§© **Tools You Must Use**

* **jcmd** â†’ real-time JVM stats
* **jmap** â†’ heap dump
* **jstack** â†’ thread dump
* **GC logs**
* **Cloud Ops / CloudWatch**
* **Prometheus + Grafana**
* **OpenTelemetry traces**
* **Hikari metrics**

---

# âœ”ï¸ Want Me to Create Next?

I can generate:

### ğŸ”¹ Full troubleshooting checklist (PDF)

### ğŸ”¹ Commands cheat sheet for GCP + AWS + Kubernetes

### ğŸ”¹ Template dashboards (Grafana + Cloud Monitoring)

### ğŸ”¹ Real production case studies

### ğŸ”¹ Automated JVM memory leak detector script

Tell me which one you want.
